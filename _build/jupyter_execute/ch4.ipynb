{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4.1 Keywords Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/long/Library/CloudStorage/OneDrive-Personal/Projects/Guided-Topic-Modeling\n",
      "['/Users/long/miniforge3/envs/gtm_py11/lib/python311.zip', '/Users/long/miniforge3/envs/gtm_py11/lib/python3.11', '/Users/long/miniforge3/envs/gtm_py11/lib/python3.11/lib-dynload', '', '/Users/long/miniforge3/envs/gtm_py11/lib/python3.11/site-packages', '/Users/long/Library/CloudStorage/OneDrive-Personal/Projects/Guided-Topic-Modeling']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/long/miniforge3/envs/gtm_py11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/long/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/long/Library/CloudStorage/OneDrive-Personal/Projects/Guided-Topic-Modeling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize GTM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Topic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coarse_grain                   word #1  ; angle: 0.763\n",
      "agricultural                   word #2  ; angle: 0.624\n",
      "oilseeds                       word #3  ; angle: 0.690\n",
      "oilseed                        word #4  ; angle: 0.596\n",
      "grains                         word #5  ; angle: 0.624\n",
      "grain                          word #6  ; angle: 0.582\n",
      "feed_grain                     word #7  ; angle: 0.622\n",
      "cotton                         word #8  ; angle: 0.623\n",
      "feed_grains                    word #9  ; angle: 0.629\n",
      "cereal                         word #10 ; angle: 0.639\n",
      "livestock                      word #11 ; angle: 0.660\n",
      "feedgrain                      word #12 ; angle: 0.664\n",
      "soybean                        word #13 ; angle: 0.667\n",
      "sugar                          word #14 ; angle: 0.666\n",
      "corn                           word #15 ; angle: 0.666\n",
      "soymeal                        word #16 ; angle: 0.678\n",
      "wheat                          word #17 ; angle: 0.679\n",
      "cereals                        word #18 ; angle: 0.686\n",
      "soy                            word #19 ; angle: 0.689\n",
      "canadian_canola                word #20 ; angle: 0.686\n",
      "crop                           word #21 ; angle: 0.689\n",
      "soyoil                         word #22 ; angle: 0.691\n",
      "vegetable_oil                  word #23 ; angle: 0.690\n",
      "refined_sugar                  word #24 ; angle: 0.694\n",
      "raw_cotton                     word #25 ; angle: 0.707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argentine_corn                 word #26 ; angle: 0.708\n",
      "cane_sugar                     word #27 ; angle: 0.713\n",
      "farming                        word #28 ; angle: 0.715\n",
      "soybeans                       word #29 ; angle: 0.716\n",
      "soybean_meal                   word #30 ; angle: 0.716\n",
      "maize                          word #31 ; angle: 0.717\n",
      "argentine_soy                  word #32 ; angle: 0.733\n",
      "agribusiness                   word #33 ; angle: 0.739\n",
      "rapeseed_meal                  word #34 ; angle: 0.740\n",
      "chickpeas                      word #35 ; angle: 0.741\n",
      "beet_sugar                     word #36 ; angle: 0.745\n",
      "canola                         word #37 ; angle: 0.752\n",
      "durum_wheat                    word #38 ; angle: 0.752\n",
      "rough_rice                     word #39 ; angle: 0.752\n",
      "carryover_stocks               word #40 ; angle: 0.752\n",
      "barley                         word #41 ; angle: 0.754\n",
      "rapeseed                       word #42 ; angle: 0.753\n",
      "sugarbeet                      word #43 ; angle: 0.756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horticultural                  word #44 ; angle: 0.759\n",
      "agricultural_commodities       word #45 ; angle: 0.764\n",
      "soybean_export                 word #46 ; angle: 0.766\n",
      "durum                          word #47 ; angle: 0.769\n",
      "edible_oil                     word #48 ; angle: 0.769\n",
      "cotton_growers                 word #49 ; angle: 0.771\n",
      "post-harvest                   word #50 ; angle: 0.772\n",
      "coffee                         word #51 ; angle: 0.772\n",
      "yellow_corn                    word #52 ; angle: 0.778\n",
      "agricultural_products          word #53 ; angle: 0.778\n",
      "bean                           word #54 ; angle: 0.780\n",
      "rice                           word #55 ; angle: 0.780\n",
      "sunflowerseed                  word #56 ; angle: 0.782\n",
      "pigmeat                        word #57 ; angle: 0.784\n",
      "drought-hit                    word #58 ; angle: 0.793\n",
      "sorghum                        word #59 ; angle: 0.799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cottonseed                     word #60 ; angle: 0.799\n",
      "malting_barley                 word #61 ; angle: 0.800\n",
      "arable                         word #62 ; angle: 0.802\n",
      "ddgs                           word #63 ; angle: 0.809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sugarcane                      word #64 ; angle: 0.810\n",
      "paddy_rice                     word #65 ; angle: 0.812\n",
      "coarse_grains                  word #66 ; angle: 0.814\n",
      "new-crop                       word #67 ; angle: 0.814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exportable                     word #68 ; angle: 0.816\n",
      "feed_wheat                     word #69 ; angle: 0.816\n",
      "citrus                         word #70 ; angle: 0.817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wheat_crops                    word #71 ; angle: 0.818\n",
      "soybean_crushing               word #72 ; angle: 0.819\n",
      "cotton_crop                    word #73 ; angle: 0.825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wheat-growing                  word #74 ; angle: 0.825\n",
      "beet                           word #75 ; angle: 0.825\n",
      "sugarcane_crop                 word #76 ; angle: 0.825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spring_wheat                   word #77 ; angle: 0.827\n",
      "broiler                        word #78 ; angle: 0.827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malting                        word #79 ; angle: 0.827\n",
      "wheat_growers                  word #80 ; angle: 0.828\n",
      "farmers                        word #81 ; angle: 0.828\n",
      "millers                        word #82 ; angle: 0.828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pork                           word #83 ; angle: 0.830\n",
      "rapeseed_harvest               word #84 ; angle: 0.830\n",
      "crushers                       word #85 ; angle: 0.830\n",
      "corn_soybean                   word #86 ; angle: 0.833\n",
      "crops                          word #87 ; angle: 0.833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "citrus_fruit                   word #88 ; angle: 0.835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cane-growing                   word #89 ; angle: 0.838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poultry                        word #90 ; angle: 0.838\n",
      "old-crop                       word #91 ; angle: 0.838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erratic_weather                word #92 ; angle: 0.838\n",
      "wheat_flour                    word #93 ; angle: 0.839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flour_millers                  word #94 ; angle: 0.839\n",
      "agronomic                      word #95 ; angle: 0.840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed                           word #96 ; angle: 0.841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wheat_crop                     word #97 ; angle: 0.842\n",
      "pre-harvest                    word #98 ; angle: 0.842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planted_acreage                word #99 ; angle: 0.842\n",
      "cotton_acreage                 word #100; angle: 0.843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sugar_beets                    word #101; angle: 0.844\n",
      "soybean_processors             word #102; angle: 0.844\n",
      "sugar_cane                     word #103; angle: 0.844\n",
      "rapeseed_crop                  word #104; angle: 0.845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meat                           word #105; angle: 0.848\n",
      "soybean_crop                   word #106; angle: 0.849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cotton-growing                 word #107; angle: 0.849\n",
      "pork_producers                 word #108; angle: 0.849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rapeseed_oil                   word #109; angle: 0.850\n",
      "corn-growing                   word #110; angle: 0.851\n",
      "animal_feed                    word #111; angle: 0.851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soybean_imports                word #112; angle: 0.852\n",
      "grower                         word #113; angle: 0.852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sugar_beet                     word #114; angle: 0.852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grain_crops                    word #115; angle: 0.853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sugar-producing                word #116; angle: 0.853\n",
      "agricultural_goods             word #117; angle: 0.853\n",
      "granary                        word #118; angle: 0.854\n",
      "farm_goods                     word #119; angle: 0.854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sunflower                      word #120; angle: 0.855\n",
      "forestry                       word #121; angle: 0.855\n",
      "growers                        word #122; angle: 0.856\n",
      "soy_crop                       word #123; angle: 0.856\n",
      "soy_corn                       word #124; angle: 0.856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedgrains                     word #125; angle: 0.857\n",
      "wool                           word #126; angle: 0.857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soymeal_exports                word #127; angle: 0.859\n",
      "agronomy                       word #128; angle: 0.860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fertilizer                     word #129; angle: 0.861\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 52\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent working directory: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(cwd))\n\u001b[1;32m     41\u001b[0m sys\u001b[38;5;241m.\u001b[39margv \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgtm.py\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--ps1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124magriculture\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Add '--ns1', '--ns2', '--nw1', '--nw2' and their values if needed\u001b[39;00m\n\u001b[1;32m     50\u001b[0m ]\n\u001b[0;32m---> 52\u001b[0m \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtm.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m sys\u001b[38;5;241m.\u001b[39margv \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgtm.py\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--ps1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124magriculture\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Add '--ns1', '--ns2', '--nw1', '--nw2' and their values if needed\u001b[39;00m\n\u001b[1;32m     63\u001b[0m ]\n\u001b[1;32m     65\u001b[0m exec(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgtm.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread())\n",
      "File \u001b[0;32m<string>:307\u001b[0m\n",
      "File \u001b[0;32m<string>:228\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(self, params, pos_seed, neg_seed)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/gtm_py11/lib/python3.11/site-packages/scipy/optimize/_minimize.py:703\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    701\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_powell(fun, x0, args, callback, bounds, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 703\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_cg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    705\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_bfgs(fun, x0, args, jac, callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/miniforge3/envs/gtm_py11/lib/python3.11/site-packages/scipy/optimize/_optimize.py:1809\u001b[0m, in \u001b[0;36m_minimize_cg\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(pk, gfk) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39msigma_3 \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(gfk, gfk)\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1808\u001b[0m     alpha_k, fc, gc, old_fval, old_old_fval, gfkp1 \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m-> 1809\u001b[0m              \u001b[43m_line_search_wolfe12\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmyfprime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgfk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_fval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1810\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mold_old_fval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1811\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mextra_condition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescent_condition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _LineSearchError:\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;66;03m# Line search failed to find a better solution.\u001b[39;00m\n\u001b[1;32m   1814\u001b[0m     warnflag \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/gtm_py11/lib/python3.11/site-packages/scipy/optimize/_optimize.py:1215\u001b[0m, in \u001b[0;36m_line_search_wolfe12\u001b[0;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;124;03mSame as line_search_wolfe1, but fall back to line_search_wolfe2 if\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;124;03msuitable step length is not found, and raise an exception if a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1210\u001b[0m \n\u001b[1;32m   1211\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m extra_condition \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextra_condition\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1215\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mline_search_wolfe1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfprime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgfk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mold_fval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_old_fval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m                         \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m extra_condition \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1220\u001b[0m     xp1 \u001b[38;5;241m=\u001b[39m xk \u001b[38;5;241m+\u001b[39m ret[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m pk\n",
      "File \u001b[0;32m~/miniforge3/envs/gtm_py11/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:84\u001b[0m, in \u001b[0;36mline_search_wolfe1\u001b[0;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, args, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(gval[\u001b[38;5;241m0\u001b[39m], pk)\n\u001b[1;32m     82\u001b[0m derphi0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(gfk, pk)\n\u001b[0;32m---> 84\u001b[0m stp, fval, old_fval \u001b[38;5;241m=\u001b[39m \u001b[43mscalar_search_wolfe1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mphi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mderphi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_fval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_old_fval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mderphi0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stp, fc[\u001b[38;5;241m0\u001b[39m], gc[\u001b[38;5;241m0\u001b[39m], fval, old_fval, gval[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/gtm_py11/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:160\u001b[0m, in \u001b[0;36mscalar_search_wolfe1\u001b[0;34m(phi, derphi, phi0, old_phi0, derphi0, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    159\u001b[0m     alpha1 \u001b[38;5;241m=\u001b[39m stp\n\u001b[0;32m--> 160\u001b[0m     phi1 \u001b[38;5;241m=\u001b[39m \u001b[43mphi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     derphi1 \u001b[38;5;241m=\u001b[39m derphi(stp)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/gtm_py11/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:75\u001b[0m, in \u001b[0;36mline_search_wolfe1.<locals>.phi\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mphi\u001b[39m(s):\n\u001b[1;32m     74\u001b[0m     fc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/gtm_py11/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:267\u001b[0m, in \u001b[0;36mScalarFunction.fun\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 267\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\n",
      "File \u001b[0;32m~/miniforge3/envs/gtm_py11/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/gtm_py11/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/gtm_py11/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m<string>:106\u001b[0m, in \u001b[0;36mfunc\u001b[0;34m(self, a, W_orth, I, X, C, weights, params)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/gtm_py11/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2172\u001b[0m, in \u001b[0;36m_sum_dispatcher\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;124;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[1;32m   2104\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2167\u001b[0m \n\u001b[1;32m   2168\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m'\u001b[39m, a_min, a_max, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 2172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2173\u001b[0m                     initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[1;32m   2177\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[1;32m   2178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2179\u001b[0m         initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#####################################################################################\n",
    "##############################   select gtm_py11 env   ##############################\n",
    "#####################################################################################\n",
    "#import libraries\n",
    "# Check the Python version\n",
    "import sys\n",
    "\n",
    "sys.version\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "working_directory = '../Guided-Topic-Modeling'\n",
    "working_directory = '/Users/long/Library/CloudStorage/OneDrive-Personal/Projects/Guided-Topic-Modeling'\n",
    "print(os.path.abspath(working_directory))\n",
    "sys.path.append(working_directory)\n",
    "print(sys.path)\n",
    "import glob\n",
    "from bertopic import BERTopic\n",
    "#from sklearn.datasets import fetch_20newsgroups\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import openpyxl\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# set up working directory\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "sys.argv = [\n",
    "    'gtm.py',\n",
    "    '--ps1', 'agriculture',\n",
    "    '--ps2', 'farm',\n",
    "    '--pw1', '1.0',\n",
    "    '--pw2', '0.000000000000000000000000001',\n",
    "    '--size', '1000',\n",
    "    '--gravity', '0.1'\n",
    "    # Add '--ns1', '--ns2', '--nw1', '--nw2' and their values if needed\n",
    "]\n",
    "\n",
    "exec(open(\"gtm.py\").read())\n",
    "\n",
    "sys.argv = [\n",
    "    'gtm.py',\n",
    "    '--ps1', 'agriculture',\n",
    "    '--ps2', 'farm',\n",
    "    '--pw1', '1.0',\n",
    "    '--pw2', '0.000000000000000000000000001',\n",
    "    '--size', '2000',\n",
    "    '--gravity', '0.1'\n",
    "    # Add '--ns1', '--ns2', '--nw1', '--nw2' and their values if needed\n",
    "]\n",
    "\n",
    "exec(open(\"gtm.py\").read())\n",
    "\n",
    "# Extracting values from sys.argv\n",
    "ps1_index = sys.argv.index('--ps1') + 1\n",
    "size_index = sys.argv.index('--size') + 1\n",
    "gravity_index = sys.argv.index('--gravity') + 1\n",
    "\n",
    "ps1_value = sys.argv[ps1_index]\n",
    "size_value = sys.argv[size_index]\n",
    "gravity_value = sys.argv[gravity_index]\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = 'output'  # Replace with the actual folder path\n",
    "print(os.path.abspath(folder_path))\n",
    "\n",
    "# Find the last CSV file\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "latest_csv_file = max(csv_files, key=os.path.getmtime)\n",
    "\n",
    "# Construct new file name\n",
    "new_file_name = f\"{ps1_value}_{size_value}_{gravity_value}.csv\"\n",
    "new_file_path = os.path.join(folder_path, new_file_name)\n",
    "\n",
    "# Rename the file\n",
    "os.rename(latest_csv_file, new_file_path)\n",
    "print(f\"File '{latest_csv_file}' has been renamed to '{new_file_path}'\")\n",
    "\n",
    "# Read the CSV file (if needed)\n",
    "topics_dict = pd.read_csv(new_file_path)\n",
    "topics_dict.rename(columns={'Unnamed: 0': 'keyword'}, inplace=True)\n",
    "\n",
    "folder_path_ssd = '/Volumes/SSD/AFRI/Data/Meachine Learning/gtm'\n",
    "new_file_path_ssd = os.path.join(folder_path_ssd, new_file_name)\n",
    "# Read the CSV file (if needed)\n",
    "topics_dict.to_csv(new_file_path_ssd)\n",
    "print(f\"File '{latest_csv_file}' has been renamed to '{new_file_path_ssd}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Topic Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.util import bigrams\n",
    "import codecs\n",
    "import ast  # Module for literal string evaluation\n",
    "\n",
    "#from word_forms.word_forms import get_word_forms\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "###\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import missingno as msno\n",
    "import os\n",
    "import wget\n",
    "import openpyxl\n",
    "import math\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "###\n",
    "\n",
    "# Set the working directory\n",
    "os.chdir('/Volumes/SSD/AFRI/Data/')\n",
    "\n",
    "# Verify the current working directory\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "######## functions\n",
    "\n",
    "def generate_uni_bigrams_individual(text):\n",
    "    # Convert non-string inputs to strings\n",
    "    text = str(text)\n",
    "\n",
    "    # Try decoding the text using ISO-8859-1 encoding\n",
    "    try:\n",
    "        text = text.decode('ISO-8859-1')\n",
    "    except AttributeError:\n",
    "        pass  # Skip if the text is already a string\n",
    "    except UnicodeDecodeError:\n",
    "        # If decoding with ISO-8859-1 fails, try UTF-8 with errors='replace'\n",
    "        text = text.decode('utf-8', errors='replace')\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove non-alphabetic tokens and lowercase the alphabetic tokens\n",
    "    unigrams = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "    # Generate bigrams from the cleaned tokens\n",
    "    bigram_tuples = list(bigrams(unigrams))\n",
    "\n",
    "    # Join the words in each bigram with an underscore\n",
    "    bigrams_formatted = ['_'.join(bigram) for bigram in bigram_tuples]\n",
    "\n",
    "    # Combine unigrams and bigrams into one list\n",
    "    combined_list = unigrams + bigrams_formatted\n",
    "\n",
    "    return combined_list\n",
    "\n",
    "\n",
    "def generate_uni_bigrams_folder(source_folder, destination_folder):\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    # Iterate over each file in the source folder\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.endswith(\".csv\") and not filename.startswith('._'):\n",
    "            print(filename)\n",
    "            # Read the CSV file into a DataFrame\n",
    "            source_filepath = os.path.join(source_folder, filename)\n",
    "            df = pd.read_csv(source_filepath)\n",
    "\n",
    "            # Drop the 'Unnamed: 0' column if it exists\n",
    "            if 'Unnamed: 0' in df.columns:\n",
    "                df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "            # Apply the function generate_uni_bigrams to create 'uni_bigrams' column\n",
    "            df['uni_bigrams'] = df['original_text'].apply(generate_uni_bigrams_individual)\n",
    "\n",
    "            # Define the new filename for the destination\n",
    "            csv_filename = filename.replace(\"_clean.csv\", \"_processed.csv\")\n",
    "            destination_filepath_csv = os.path.join(destination_folder, csv_filename)\n",
    "\n",
    "            json_filename = filename.replace(\"_clean.csv\", \"_processed.json\")\n",
    "            destination_filepath_json = os.path.join(destination_folder, json_filename)\n",
    "\n",
    "            pkl_filename = filename.replace(\"_clean.csv\", \"_processed.pkl\")\n",
    "            destination_filepath_pkl = os.path.join(destination_folder, pkl_filename)\n",
    "\n",
    "            # Save the modified DataFrame to a new CSV file in the destination folder\n",
    "            df.to_csv(destination_filepath_csv, index=False)\n",
    "            df.to_json(destination_filepath_json, orient='records')\n",
    "            df.to_pickle(destination_filepath_pkl)\n",
    "\n",
    "            print(f\"Processed file '{filename}' successfully saved\")\n",
    "\n",
    "\n",
    "def count_total_occurrences(text, keywords):\n",
    "    # Initialize a dictionary to store word counts\n",
    "    word_counts = {word: 0 for word in keywords}\n",
    "\n",
    "    for word in text:\n",
    "        if word.lower() in word_counts:  # Convert to lowercase for case-insensitive matching\n",
    "            word_counts[word.lower()] += 1\n",
    "\n",
    "    # Sum all the occurrences\n",
    "    total_occurrences = sum(word_counts.values())\n",
    "\n",
    "    return total_occurrences\n",
    "\n",
    "\n",
    "def count_individual_occurrences(text, keywords):\n",
    "    # Initialize a dictionary to store counts of keywords, starting at 0\n",
    "    word_counts = {word: 0 for word in keywords}\n",
    "\n",
    "    # Count occurrences of each keyword in the text\n",
    "    for word in text:\n",
    "        word_lower = word.lower()  # Convert word to lowercase to ensure case-insensitive matching\n",
    "        if word_lower in word_counts:\n",
    "            word_counts[word_lower] += 1\n",
    "\n",
    "    # Filter out keywords with zero occurrences\n",
    "    word_counts = {word: count for word, count in word_counts.items() if count > 0}\n",
    "\n",
    "    return word_counts\n",
    "\n",
    "\n",
    "def count_individual_score(text, keyword_weights):\n",
    "    # Initialize a dictionary to store counts of keywords, starting at 0\n",
    "    word_counts = {word: 0 for word in keywords}\n",
    "\n",
    "    # Count occurrences of each keyword in the text\n",
    "    for word in text:\n",
    "        word_lower = word.lower()  # Convert word to lowercase to ensure case-insensitive matching\n",
    "        if word_lower in word_counts:\n",
    "            word_counts[word_lower] += 1\n",
    "\n",
    "    # Filter out keywords with zero occurrences\n",
    "    word_counts = {word: count for word, count in word_counts.items() if count > 0}\n",
    "\n",
    "    return word_counts\n",
    "\n",
    "\n",
    "def calculate_keyword_scores(text, keywords, keyword_weights):\n",
    "    # Initialize a dictionary to store counts of keywords\n",
    "    word_counts = {word: 0 for word in keywords}\n",
    "\n",
    "    # Count occurrences of each keyword in the text\n",
    "    for word in text:\n",
    "        word_lower = word.lower()\n",
    "        if word_lower in word_counts:\n",
    "            word_counts[word_lower] += 1\n",
    "\n",
    "    # Calculate scores for each keyword based on its weight and occurrence count\n",
    "    keyword_scores = {word: count * keyword_weights[word] for word, count in word_counts.items() if count > 0}\n",
    "\n",
    "    # Sort the keywords by their scores in descending order\n",
    "    sorted_keyword_scores = sorted(keyword_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    # Convert the sorted list of tuples back into a dictionary\n",
    "    sorted_keyword_scores_dict = dict(sorted_keyword_scores)\n",
    "\n",
    "    return sorted_keyword_scores_dict\n",
    "\n",
    "\n",
    "def calculate_weighted_score(text, keyword_weights):\n",
    "    # Initialize the total score\n",
    "    total_score = 0\n",
    "    #print(text)\n",
    "    # Split the text into words\n",
    "    for word in text:\n",
    "        # If the word is in the list and has a weight, add its weighted score\n",
    "        # Check if the word is in the keyword_weights dictionary and add its weighted score\n",
    "        if word in keyword_weights:\n",
    "            total_score += keyword_weights[word]\n",
    "    return total_score\n",
    "\n",
    "\n",
    "def extract_top_five_items(dictionary):\n",
    "    # Convert the string representation of dictionary to a dictionary object\n",
    "    dictionary = ast.literal_eval(dictionary)\n",
    "    # Sort the dictionary by values in descending order and take the first five items\n",
    "    top_five_items = dict(sorted(dictionary.items(), key=lambda item: item[1], reverse=True)[:5])\n",
    "    return top_five_items\n",
    "\n",
    "\n",
    "def extract_top_five_items(dictionary):\n",
    "    sorted_items = sorted(dictionary.items(), key=lambda item: item[1], reverse=True)[:5]\n",
    "\n",
    "    # Convert sorted items back to a dictionary if needed\n",
    "    top_five_items = dict(sorted_items)\n",
    "    return top_five_items\n",
    "\n",
    "\n",
    "def count_words(text):\n",
    "    text = str(text)\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "\n",
    "\n",
    "def classification(source_folder, destination_folder, keywords, keyword_weights):\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    # Iterate over each file in the source folder\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.endswith(\".pkl\") and not filename.startswith('._'):\n",
    "            print(filename)\n",
    "            # Read the CSV file into a DataFrame\n",
    "            source_filepath = os.path.join(source_folder, filename)\n",
    "\n",
    "            with open(source_filepath, 'rb') as file:\n",
    "                df = pickle.load(file)\n",
    "\n",
    "                # Drop the 'Unnamed: 0' column if it exists\n",
    "                if 'Unnamed: 0' in df.columns:\n",
    "                    df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "                # Count occurrences of keywords in uni_bigrams\n",
    "                df['uni_bigrams_occurrences'] = df['uni_bigrams'].apply(lambda x: count_total_occurrences(x, keywords))\n",
    "\n",
    "                # Count occurrences of each keyword in the text\n",
    "                df['uni_bigrams_word_counts'] = df['uni_bigrams'].apply(\n",
    "                    lambda x: count_individual_occurrences(x, keywords))\n",
    "\n",
    "                # top 5 keywords with scores\n",
    "                df['top5_uni_bigrams_word_counts'] = df['uni_bigrams_word_counts'].apply(\n",
    "                    lambda x: extract_top_five_items(x))\n",
    "\n",
    "                # Calculate scores of each keyword in the text\n",
    "                df['uni_bigrams_word_scores'] = df['uni_bigrams'].apply(\n",
    "                    lambda x: calculate_keyword_scores(x, keywords, keyword_weights))\n",
    "\n",
    "                # Calculate total scores of keywords in the text\n",
    "                df['total_weighted_score'] = df['uni_bigrams'].apply(\n",
    "                    lambda x: calculate_weighted_score(x, keyword_weights))\n",
    "\n",
    "                # Count the words\n",
    "                df['original_text_word_count'] = df['original_text'].apply(count_words)\n",
    "\n",
    "                # Calculate total scores of keywords per word in the text\n",
    "                df['total_weighted_score_per_word'] = df.apply(\n",
    "                    lambda row: row['total_weighted_score'] / row['original_text_word_count'] if row[\n",
    "                                                                                                     'original_text_word_count'] > 0 else 0,\n",
    "                    axis=1)\n",
    "\n",
    "                # top 5 keywords with scores\n",
    "                df['top5_uni_bigrams_word_scores'] = df['uni_bigrams_word_scores'].apply(\n",
    "                    lambda x: extract_top_five_items(x))\n",
    "\n",
    "                #.apply(lambda x: extract_top_five_items(x) if isinstance(x, str) else {})\n",
    "\n",
    "                # Define the new filename for the destination\n",
    "                csv_filename = filename.replace(\"_processed.pkl\", \"_classified.csv\")\n",
    "                destination_filepath_csv = os.path.join(destination_folder, csv_filename)\n",
    "\n",
    "                json_filename = filename.replace(\"_processed.pkl\", \"_classified.json\")\n",
    "                destination_filepath_json = os.path.join(destination_folder, json_filename)\n",
    "\n",
    "                pkl_filename = filename.replace(\"_processed.pkl\", \"_classified.pkl\")\n",
    "                destination_filepath_pkl = os.path.join(destination_folder, pkl_filename)\n",
    "\n",
    "                # Save the modified DataFrame to a new CSV file in the destination folder\n",
    "                #df.to_csv(destination_filepath_csv, index=False)\n",
    "                #df.to_json(destination_filepath_json, orient='records')\n",
    "                df.to_pickle(destination_filepath_pkl)\n",
    "\n",
    "                # This will hold the combined results\n",
    "                combined_dict = defaultdict(int)\n",
    "\n",
    "                # Iterate through each dictionary in the DataFrame's column\n",
    "                for index, row in df.iterrows():\n",
    "                    for key, value in row['uni_bigrams_word_counts'].items():\n",
    "                        combined_dict[key] += value\n",
    "\n",
    "                # Convert the defaultdict back to a regular dictionary for display or further use\n",
    "                result_dict_count = dict(combined_dict)\n",
    "\n",
    "                # Create a WordCloud object\n",
    "                wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
    "\n",
    "                # Generate a word cloud from frequencies\n",
    "                wordcloud.generate_from_frequencies(result_dict_count)\n",
    "\n",
    "                # Display the generated image:\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.imshow(wordcloud, interpolation='bilinear')\n",
    "                plt.axis('off')  # Do not show axes to keep it clean\n",
    "\n",
    "                pic_filename = filename.replace(\"_processed.pkl\", \"_wordcloud_count.png\")\n",
    "                destination_filepath_pic = os.path.join(destination_folder, pic_filename)\n",
    "\n",
    "                # Save the figure to a file\n",
    "                plt.savefig(destination_filepath_pic, format='png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "                dict_count_filename = filename.replace(\"_processed.pkl\", \"_dic_count.csv\")\n",
    "                destination_filepath_dict = os.path.join(destination_folder, dict_count_filename)\n",
    "\n",
    "                # Convert dictionary to DataFrame\n",
    "                result_list_count = pd.DataFrame(list(result_dict_count.items()), columns=['Word', 'Frequency'])\n",
    "\n",
    "                # Save to CSV\n",
    "                result_list_count.to_csv(destination_filepath_dict, index=False)\n",
    "\n",
    "                # This will hold the combined results\n",
    "                combined_dict = defaultdict(int)\n",
    "\n",
    "                # Iterate through each dictionary in the DataFrame's column\n",
    "                for index, row in df.iterrows():\n",
    "                    for key, value in row['uni_bigrams_word_scores'].items():\n",
    "                        combined_dict[key] += value\n",
    "\n",
    "                # Convert the defaultdict back to a regular dictionary for display or further use\n",
    "                result_dict_score = dict(combined_dict)\n",
    "\n",
    "                # Create a WordCloud object\n",
    "                wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
    "\n",
    "                # Generate a word cloud from frequencies\n",
    "                wordcloud.generate_from_frequencies(result_dict_score)\n",
    "\n",
    "                # Display the generated image:\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.imshow(wordcloud, interpolation='bilinear')\n",
    "                plt.axis('off')  # Do not show axes to keep it clean\n",
    "\n",
    "                pic_filename = filename.replace(\"_processed.pkl\", \"_wordcloud_score.png\")\n",
    "                destination_filepath_pic = os.path.join(destination_folder, pic_filename)\n",
    "\n",
    "                # Save the figure to a file\n",
    "                plt.savefig(destination_filepath_pic, format='png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "                dict_count_filename = filename.replace(\"_processed.pkl\", \"_dic_score.csv\")\n",
    "                destination_filepath_dict = os.path.join(destination_folder, dict_count_filename)\n",
    "\n",
    "                # Convert dictionary to DataFrame\n",
    "                result_list_score = pd.DataFrame(list(result_dict_score.items()), columns=['Word', 'Frequency'])\n",
    "\n",
    "                # Save to CSV\n",
    "                result_list_score.to_csv(destination_filepath_dict, index=False)\n",
    "\n",
    "                print(f\"Classified file '{filename}' successfully saved\")\n",
    "\n",
    "\n",
    "source_folder = \"./Raw_Data/Cleaned\"\n",
    "destination_folder = \"./Processed_Data\"\n",
    "\n",
    "generate_uni_bigrams_folder(source_folder, destination_folder)\n",
    "\n",
    "keywords = pd.read_excel('./Meachine Learning/gtm/agriculture_1000_0.1_human_refinement.xlsx')[\n",
    "    'keyword'].values.tolist()\n",
    "keyword_weights = dict(\n",
    "    zip(pd.read_excel('./Meachine Learning/gtm/agriculture_1000_0.1_human_refinement.xlsx')['keyword'],\n",
    "        pd.read_excel('./Meachine Learning/gtm/agriculture_1000_0.1_human_refinement.xlsx')['weight']))\n",
    "source_folder = \"./Processed_Data\"\n",
    "destination_folder = \"./Outcomes\"\n",
    "\n",
    "classification(source_folder, destination_folder, keywords, keyword_weights)\n",
    "########\n",
    "destination_folder = \"./Outcomes\"\n",
    "\n",
    "# Define the columns to keep\n",
    "columns_to_keep = ['state', 'year', 'act_num', 'uni_bigrams_occurrences', 'uni_bigrams_word_counts',\n",
    "                   'top5_uni_bigrams_word_counts', 'uni_bigrams_word_scores', 'total_weighted_score',\n",
    "                   'original_text_word_count',\n",
    "                   'total_weighted_score_per_word', 'top5_uni_bigrams_word_scores']\n",
    "\n",
    "# Initialize an empty DataFrame to hold all the data\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(destination_folder):\n",
    "    if filename.endswith('count.csv') and not filename.startswith('._'):  # Check if the file is a CSV\n",
    "        print(filename)\n",
    "        file_path = os.path.join(destination_folder, filename)\n",
    "        data = pd.read_csv(file_path)\n",
    "        #with open(file_path, 'rb') as file:\n",
    "        #data = pickle.load(file)\n",
    "        #data = data[columns_to_keep]\n",
    "        # Append the data to the main DataFrame\n",
    "        df = df._append(data, ignore_index=True)\n",
    "\n",
    "#df['top5_uni_bigrams_word_scores'] = df['uni_bigrams_word_scores'].apply(lambda d: {k: d[k] for k in list(d.keys())[:5]} if isinstance(d, dict) else d)\n",
    "df = df.groupby('Word')['Frequency'].sum().reset_index()\n",
    "df.to_csv('./Outcomes/all_words_count.csv')\n",
    "\n",
    "# Convert DataFrame to dictionary\n",
    "word_freq = dict(zip(df['Word'], df['Frequency']))\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate_from_frequencies(word_freq)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Do not show axes to keep it clean\n",
    "plt.show()\n",
    "plt.savefig('./Outcomes/counts.png', format='png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Initialize an empty DataFrame to hold all the data\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(destination_folder):\n",
    "    if filename.endswith('score.csv') and not filename.startswith('._'):  # Check if the file is a CSV\n",
    "        print(filename)\n",
    "        file_path = os.path.join(destination_folder, filename)\n",
    "        data = pd.read_csv(file_path)\n",
    "        #with open(file_path, 'rb') as file:\n",
    "        #data = pickle.load(file)\n",
    "        #data = data[columns_to_keep]\n",
    "        # Append the data to the main DataFrame\n",
    "        df = df._append(data, ignore_index=True)\n",
    "\n",
    "#df['top5_uni_bigrams_word_scores'] = df['uni_bigrams_word_scores'].apply(lambda d: {k: d[k] for k in list(d.keys())[:5]} if isinstance(d, dict) else d)\n",
    "df = df.groupby('Word')['Frequency'].sum().reset_index()\n",
    "df.to_csv('./Outcomes/all_words_score.csv')\n",
    "\n",
    "# Convert DataFrame to dictionary\n",
    "word_freq = dict(zip(df['Word'], df['Frequency']))\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate_from_frequencies(word_freq)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Do not show axes to keep it clean\n",
    "plt.show()\n",
    "plt.savefig('./Outcomes/scores.png', format='png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "df = pd.read_csv('./Outcomes/classification_results.csv')\n",
    "\n",
    "try:\n",
    "    df = df.drop(['Unnamed: 0'], axis=1)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "df.dropna(subset=['year'], inplace=True)\n",
    "\n",
    "df['year'] = df['year'].astype(str).str[:4].astype(int)\n",
    "\n",
    "df = df[(df['year'] >= 1975) & (df['year'] <= 2021)]\n",
    "\n",
    "# Group by 'year' and 'state' and calculate the count of positive scores and the total count of scores\n",
    "grouped_data = df.groupby(['year', 'state']).agg(\n",
    "    positive_score_count=('total_weighted_score_per_word', lambda x: (x > 0).sum()),\n",
    "    total_score_count=('total_weighted_score_per_word', 'count')\n",
    ")\n",
    "\n",
    "# Calculate the proportion of the count of positive scores to the total count of scores\n",
    "grouped_data['proportion'] = grouped_data['positive_score_count'] / grouped_data['total_score_count']\n",
    "\n",
    "# Reset the index to make 'year' and 'state' columns again\n",
    "grouped_data.reset_index(inplace=True)\n",
    "\n",
    "grouped_data.to_csv('./Outcomes/classification_results_short.csv')\n",
    "\n",
    "# Pivot the data for plotting\n",
    "pivot_data = grouped_data.pivot(index='year', columns='state', values='proportion')\n",
    "\n",
    "plt.figure(figsize=(30, 24))  # Increase figure size for better clarity and space\n",
    "pivot_data.plot(kind='line')\n",
    "\n",
    "# Increase font sizes for better readability\n",
    "plt.title('Proportion of Ag Legislation per Year by State', fontsize=12)\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.ylabel('Proportion of Ag Legislation', fontsize=12)\n",
    "\n",
    "# Increase tick label size\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Expand the right margin to ensure the legend and plot do not overlap\n",
    "plt.subplots_adjust(right=0.65)\n",
    "\n",
    "# Place the legend to the right of the plot, making it larger to ensure legibility\n",
    "plt.legend(title='State', loc='center left', bbox_to_anchor=(1.05, 0.5), ncol=2, fontsize=12, title_fontsize=12)\n",
    "\n",
    "# Add grid for better readability of the plot\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "state_data = df.groupby('state').agg(\n",
    "    ag_count=('total_weighted_score_per_word', lambda x: (x > 0).sum()),\n",
    "    ag_count_0004=('total_weighted_score_per_word', lambda x: (x > 0.004).sum()),\n",
    "    total_count=('total_weighted_score_per_word', 'count')\n",
    ")\n",
    "\n",
    "state_data['ag_proportion_0000'] = state_data['ag_count'] / state_data['total_count']\n",
    "state_data['ag_proportion_0004'] = state_data['ag_count_0004'] / state_data['total_count']\n",
    "state_data.to_csv('./Outcomes/state.csv')\n",
    "\n",
    "state_data = pd.read_csv('./Outcomes/state.csv')\n",
    "\n",
    "gdf = gpd.read_file('./Geo/cb_2018_us_state_500k.shp')\n",
    "\n",
    "fips = pd.read_excel('./Geo/statefp.xlsx')\n",
    "fips['STATEFP'] = fips['STATEFP'].astype(str).apply(lambda x: x.zfill(2))\n",
    "\n",
    "gdf = gdf.merge(fips, how='left', on='STATEFP')\n",
    "gdf = gdf.merge(state_data, how='left', on='state')\n",
    "\n",
    "\n",
    "# Apply this to the gdf to ensure all states are assigned colors by the same func\n",
    "def makeColorColumn(gdf, variable, vmin, vmax):\n",
    "    # apply a function to a column to create a new column of assigned colors & return full frame\n",
    "    norm = mcolors.Normalize(vmin=vmin, vmax=vmax, clip=True)\n",
    "    mapper = plt.cm.ScalarMappable(norm=norm, cmap=plt.cm.YlOrBr)\n",
    "    gdf['value_determined_color'] = gdf[variable].apply(lambda x: mcolors.to_hex(mapper.to_rgba(x)))\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# set the value column that will be visualised\n",
    "variable = 'ag_proportion_0000'\n",
    "#variable = 'ag_proportion_0004'\n",
    "\n",
    "# make a column for value_determined_color in gdf\n",
    "# set the range for the choropleth values with the upper bound the rounded up maximum value\n",
    "\n",
    "if variable == 'ag_proportion_0000':\n",
    "    gdf['ag_proportion'] = gdf['ag_proportion_0000']\n",
    "else:\n",
    "    gdf['ag_proportion'] = gdf['ag_proportion_0004']\n",
    "\n",
    "vmin, vmax = gdf.ag_proportion.min(), gdf.ag_proportion.max()\n",
    "\n",
    "# Choose the continuous colorscale \"YlOrBr\" from https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "colormap = \"YlOrBr\"\n",
    "gdf = makeColorColumn(gdf, variable, vmin, vmax)\n",
    "\n",
    "# create \"visframe\" as a re-projected gdf using EPSG 2163 for CONUS\n",
    "#visframe = gdf.to_crs({'init':'epsg:2163'})\n",
    "visframe = gdf.to_crs({'proj': 'aea', 'lat_1': 29.5, 'lat_2': 45.5, 'lon_0': -96, 'lat_0': 37.5})\n",
    "\n",
    "# create figure and axes for Matplotlib\n",
    "fig, ax = plt.subplots(1, figsize=(20, 20))\n",
    "# remove the axis box around the vis\n",
    "ax.axis('off')\n",
    "# set the font for the visualization to Helvetica\n",
    "hfont = {'fontname': 'Helvetica'}\n",
    "\n",
    "# add a title and annotation\n",
    "ax.set_title('State-Level Agricultural Legislation\\n1975-2021', **hfont, fontdict={'fontsize': '12', 'fontweight': '1'})\n",
    "\n",
    "# Create colorbar legend\n",
    "fig = ax.get_figure()\n",
    "# add colorbar axes to the figure\n",
    "# This will take some iterating to get it where you want it [l,b,w,h] right\n",
    "# l:left, b:bottom, w:width, h:height; in normalized unit (0-1)\n",
    "cbax = fig.add_axes([0.89, 0.21, 0.03, 0.31])\n",
    "\n",
    "cbax.set_title('Percentage \\n of Ag Legislation \\n (1975-2021)', **hfont,\n",
    "               fontdict={'fontsize': '12', 'fontweight': '0'})\n",
    "\n",
    "# add color scale\n",
    "sm = plt.cm.ScalarMappable(cmap=colormap, \\\n",
    "                           norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "# reformat tick labels on legend\n",
    "sm._A = []\n",
    "#comma_fmt = FuncFormatter(lambda x, p: format(x/100, '%'))\n",
    "comma_fmt = FuncFormatter(lambda x, p: \"{:.1f}%\".format(x * 100))\n",
    "fig.colorbar(sm, cax=cbax, format=comma_fmt)\n",
    "tick_font_size = 16\n",
    "cbax.tick_params(labelsize=tick_font_size)\n",
    "# annotate the data source, date of access, and hyperlink\n",
    "ax.annotate(\"Data: Authors\", xy=(0.5, .085), xycoords='figure fraction', fontsize=12, color='#555555')\n",
    "\n",
    "# create map\n",
    "# Note: we're going state by state here because of unusual coloring behavior when trying to plot the entire dataframe using the \"value_determined_color\" column\n",
    "for row in visframe.itertuples():\n",
    "    if row.state not in ['AK', 'HI']:  # Exclude Alaska and Hawaii for this part\n",
    "        vf = visframe[visframe.state == row.state]\n",
    "        if pd.isna(row.ag_proportion):  # Check if the ag_proportion is NaN\n",
    "            color = 'lightgrey'  # Set color to grey for missing data\n",
    "        else:\n",
    "            color = gdf.loc[gdf.state == row.state, 'value_determined_color'].iloc[0]\n",
    "        vf.plot(color=color, linewidth=1.5, ax=ax, edgecolor='0.8')\n",
    "\n",
    "# add Alaska\n",
    "akax = fig.add_axes([0.4, 0.25, 0.2, 0.13])\n",
    "akax.axis('off')\n",
    "# polygon to clip western islands\n",
    "polygon = Polygon([(-170, 50), (-170, 72), (-140, 72), (-140, 50)])\n",
    "alaska_gdf = gdf[gdf.state == 'AK']\n",
    "alaska_gdf.clip(polygon).plot(color=gdf[gdf.state == 'AK'].value_determined_color, linewidth=0.8, ax=akax,\n",
    "                              edgecolor='0.8')\n",
    "\n",
    "# add Hawaii\n",
    "hiax = fig.add_axes([.58, 0.25, 0.1, 0.1])\n",
    "hiax.axis('off')\n",
    "`\n",
    "# polygon to clip western islands\n",
    "hipolygon = Polygon([(-160, 0), (-160, 90), (-120, 90), (-120, 0)])\n",
    "hawaii_gdf = gdf[gdf.state == 'HI']\n",
    "\n",
    "# Clip the Hawaii GeoDataFrame to the desired area\n",
    "clipped_hawaii_gdf = hawaii_gdf.clip(hipolygon)\n",
    "\n",
    "# Plot the clipped Hawaii GeoDataFrame using the 'value_determined_color' column for color\n",
    "#color = hawaii_gdf['value_determined_color'].iloc[0] if not hawaii_gdf.empty else 'lightgrey'\n",
    "clipped_hawaii_gdf.plot(ax=hiax, color='lightgrey', linewidth=0.8, edgecolor='0.8')\n",
    "\n",
    "if variable == 'ag_proportion_0000':\n",
    "    fig.savefig(os.path.join(os.getcwd(), './Outcomes/ag_legislation_1975_2021_0000.png'), dpi=400, bbox_inches=\"tight\")\n",
    "else:\n",
    "    fig.savefig(os.path.join(os.getcwd(), './Outcomes/ag_legislation_1975_2021_0004.png'), dpi=400, bbox_inches=\"tight\")\n",
    "\n",
    "# bbox_inches=\"tight\" keeps the vis from getting cut off at the edges in the saved png\n",
    "# dip is \"dots per inch\" and controls image quality.  Many scientific journals have specifications for this\n",
    "# https://stackoverflow.com/questions/16183462/saving-images-in-python-at-a-very-high-quality\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "72db359963b886546324fdab9aa5857888ab40de550209afc182f1efe35e5205"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}