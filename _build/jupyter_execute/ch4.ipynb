{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4.1 Keywords Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "##############################   select gtm_py11 env   ##############################\n",
    "#####################################################################################\n",
    "#import libraries\n",
    "# Check the Python version\n",
    "import sys\n",
    "\n",
    "sys.version\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "working_directory = '../Guided-Topic-Modeling'\n",
    "working_directory = '/Users/long/Library/CloudStorage/OneDrive-Personal/Projects/Guided-Topic-Modeling'\n",
    "print(os.path.abspath(working_directory))\n",
    "sys.path.append(working_directory)\n",
    "print(sys.path)\n",
    "import glob\n",
    "from bertopic import BERTopic\n",
    "#from sklearn.datasets import fetch_20newsgroups\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import openpyxl\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# set up working directory\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "sys.argv = [\n",
    "    'gtm.py',\n",
    "    '--ps1', 'agriculture',\n",
    "    '--ps2', 'farm',\n",
    "    '--pw1', '1.0',\n",
    "    '--pw2', '0.000000000000000000000000001',\n",
    "    '--size', '1000',\n",
    "    '--gravity', '0.1'\n",
    "    # Add '--ns1', '--ns2', '--nw1', '--nw2' and their values if needed\n",
    "]\n",
    "\n",
    "exec(open(\"gtm.py\").read())\n",
    "\n",
    "sys.argv = [\n",
    "    'gtm.py',\n",
    "    '--ps1', 'agriculture',\n",
    "    '--ps2', 'farm',\n",
    "    '--pw1', '1.0',\n",
    "    '--pw2', '0.000000000000000000000000001',\n",
    "    '--size', '2000',\n",
    "    '--gravity', '0.1'\n",
    "    # Add '--ns1', '--ns2', '--nw1', '--nw2' and their values if needed\n",
    "]\n",
    "\n",
    "exec(open(\"gtm.py\").read())\n",
    "\n",
    "# Extracting values from sys.argv\n",
    "ps1_index = sys.argv.index('--ps1') + 1\n",
    "size_index = sys.argv.index('--size') + 1\n",
    "gravity_index = sys.argv.index('--gravity') + 1\n",
    "\n",
    "ps1_value = sys.argv[ps1_index]\n",
    "size_value = sys.argv[size_index]\n",
    "gravity_value = sys.argv[gravity_index]\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = 'output'  # Replace with the actual folder path\n",
    "print(os.path.abspath(folder_path))\n",
    "\n",
    "# Find the last CSV file\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "latest_csv_file = max(csv_files, key=os.path.getmtime)\n",
    "\n",
    "# Construct new file name\n",
    "new_file_name = f\"{ps1_value}_{size_value}_{gravity_value}.csv\"\n",
    "new_file_path = os.path.join(folder_path, new_file_name)\n",
    "\n",
    "# Rename the file\n",
    "os.rename(latest_csv_file, new_file_path)\n",
    "print(f\"File '{latest_csv_file}' has been renamed to '{new_file_path}'\")\n",
    "\n",
    "# Read the CSV file (if needed)\n",
    "topics_dict = pd.read_csv(new_file_path)\n",
    "topics_dict.rename(columns={'Unnamed: 0': 'keyword'}, inplace=True)\n",
    "\n",
    "folder_path_ssd = '/Volumes/SSD/AFRI/Data/Meachine Learning/gtm'\n",
    "new_file_path_ssd = os.path.join(folder_path_ssd, new_file_name)\n",
    "# Read the CSV file (if needed)\n",
    "topics_dict.to_csv(new_file_path_ssd)\n",
    "print(f\"File '{latest_csv_file}' has been renamed to '{new_file_path_ssd}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Topic Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.util import bigrams\n",
    "import codecs\n",
    "import ast  # Module for literal string evaluation\n",
    "\n",
    "#from word_forms.word_forms import get_word_forms\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "###\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import missingno as msno\n",
    "import os\n",
    "import wget\n",
    "import openpyxl\n",
    "import math\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "###\n",
    "\n",
    "# Set the working directory\n",
    "os.chdir('/Volumes/SSD/AFRI/Data/')\n",
    "\n",
    "# Verify the current working directory\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "######## functions\n",
    "\n",
    "def generate_uni_bigrams_individual(text):\n",
    "    # Convert non-string inputs to strings\n",
    "    text = str(text)\n",
    "\n",
    "    # Try decoding the text using ISO-8859-1 encoding\n",
    "    try:\n",
    "        text = text.decode('ISO-8859-1')\n",
    "    except AttributeError:\n",
    "        pass  # Skip if the text is already a string\n",
    "    except UnicodeDecodeError:\n",
    "        # If decoding with ISO-8859-1 fails, try UTF-8 with errors='replace'\n",
    "        text = text.decode('utf-8', errors='replace')\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove non-alphabetic tokens and lowercase the alphabetic tokens\n",
    "    unigrams = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "    # Generate bigrams from the cleaned tokens\n",
    "    bigram_tuples = list(bigrams(unigrams))\n",
    "\n",
    "    # Join the words in each bigram with an underscore\n",
    "    bigrams_formatted = ['_'.join(bigram) for bigram in bigram_tuples]\n",
    "\n",
    "    # Combine unigrams and bigrams into one list\n",
    "    combined_list = unigrams + bigrams_formatted\n",
    "\n",
    "    return combined_list\n",
    "\n",
    "\n",
    "def generate_uni_bigrams_folder(source_folder, destination_folder):\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    # Iterate over each file in the source folder\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.endswith(\".csv\") and not filename.startswith('._'):\n",
    "            print(filename)\n",
    "            # Read the CSV file into a DataFrame\n",
    "            source_filepath = os.path.join(source_folder, filename)\n",
    "            df = pd.read_csv(source_filepath)\n",
    "\n",
    "            # Drop the 'Unnamed: 0' column if it exists\n",
    "            if 'Unnamed: 0' in df.columns:\n",
    "                df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "            # Apply the function generate_uni_bigrams to create 'uni_bigrams' column\n",
    "            df['uni_bigrams'] = df['original_text'].apply(generate_uni_bigrams_individual)\n",
    "\n",
    "            # Define the new filename for the destination\n",
    "            csv_filename = filename.replace(\"_clean.csv\", \"_processed.csv\")\n",
    "            destination_filepath_csv = os.path.join(destination_folder, csv_filename)\n",
    "\n",
    "            json_filename = filename.replace(\"_clean.csv\", \"_processed.json\")\n",
    "            destination_filepath_json = os.path.join(destination_folder, json_filename)\n",
    "\n",
    "            pkl_filename = filename.replace(\"_clean.csv\", \"_processed.pkl\")\n",
    "            destination_filepath_pkl = os.path.join(destination_folder, pkl_filename)\n",
    "\n",
    "            # Save the modified DataFrame to a new CSV file in the destination folder\n",
    "            df.to_csv(destination_filepath_csv, index=False)\n",
    "            df.to_json(destination_filepath_json, orient='records')\n",
    "            df.to_pickle(destination_filepath_pkl)\n",
    "\n",
    "            print(f\"Processed file '{filename}' successfully saved\")\n",
    "\n",
    "\n",
    "def count_total_occurrences(text, keywords):\n",
    "    # Initialize a dictionary to store word counts\n",
    "    word_counts = {word: 0 for word in keywords}\n",
    "\n",
    "    for word in text:\n",
    "        if word.lower() in word_counts:  # Convert to lowercase for case-insensitive matching\n",
    "            word_counts[word.lower()] += 1\n",
    "\n",
    "    # Sum all the occurrences\n",
    "    total_occurrences = sum(word_counts.values())\n",
    "\n",
    "    return total_occurrences\n",
    "\n",
    "\n",
    "def count_individual_occurrences(text, keywords):\n",
    "    # Initialize a dictionary to store counts of keywords, starting at 0\n",
    "    word_counts = {word: 0 for word in keywords}\n",
    "\n",
    "    # Count occurrences of each keyword in the text\n",
    "    for word in text:\n",
    "        word_lower = word.lower()  # Convert word to lowercase to ensure case-insensitive matching\n",
    "        if word_lower in word_counts:\n",
    "            word_counts[word_lower] += 1\n",
    "\n",
    "    # Filter out keywords with zero occurrences\n",
    "    word_counts = {word: count for word, count in word_counts.items() if count > 0}\n",
    "\n",
    "    return word_counts\n",
    "\n",
    "\n",
    "def count_individual_score(text, keyword_weights):\n",
    "    # Initialize a dictionary to store counts of keywords, starting at 0\n",
    "    word_counts = {word: 0 for word in keywords}\n",
    "\n",
    "    # Count occurrences of each keyword in the text\n",
    "    for word in text:\n",
    "        word_lower = word.lower()  # Convert word to lowercase to ensure case-insensitive matching\n",
    "        if word_lower in word_counts:\n",
    "            word_counts[word_lower] += 1\n",
    "\n",
    "    # Filter out keywords with zero occurrences\n",
    "    word_counts = {word: count for word, count in word_counts.items() if count > 0}\n",
    "\n",
    "    return word_counts\n",
    "\n",
    "\n",
    "def calculate_keyword_scores(text, keywords, keyword_weights):\n",
    "    # Initialize a dictionary to store counts of keywords\n",
    "    word_counts = {word: 0 for word in keywords}\n",
    "\n",
    "    # Count occurrences of each keyword in the text\n",
    "    for word in text:\n",
    "        word_lower = word.lower()\n",
    "        if word_lower in word_counts:\n",
    "            word_counts[word_lower] += 1\n",
    "\n",
    "    # Calculate scores for each keyword based on its weight and occurrence count\n",
    "    keyword_scores = {word: count * keyword_weights[word] for word, count in word_counts.items() if count > 0}\n",
    "\n",
    "    # Sort the keywords by their scores in descending order\n",
    "    sorted_keyword_scores = sorted(keyword_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    # Convert the sorted list of tuples back into a dictionary\n",
    "    sorted_keyword_scores_dict = dict(sorted_keyword_scores)\n",
    "\n",
    "    return sorted_keyword_scores_dict\n",
    "\n",
    "\n",
    "def calculate_weighted_score(text, keyword_weights):\n",
    "    # Initialize the total score\n",
    "    total_score = 0\n",
    "    #print(text)\n",
    "    # Split the text into words\n",
    "    for word in text:\n",
    "        # If the word is in the list and has a weight, add its weighted score\n",
    "        # Check if the word is in the keyword_weights dictionary and add its weighted score\n",
    "        if word in keyword_weights:\n",
    "            total_score += keyword_weights[word]\n",
    "    return total_score\n",
    "\n",
    "\n",
    "def extract_top_five_items(dictionary):\n",
    "    # Convert the string representation of dictionary to a dictionary object\n",
    "    dictionary = ast.literal_eval(dictionary)\n",
    "    # Sort the dictionary by values in descending order and take the first five items\n",
    "    top_five_items = dict(sorted(dictionary.items(), key=lambda item: item[1], reverse=True)[:5])\n",
    "    return top_five_items\n",
    "\n",
    "\n",
    "def extract_top_five_items(dictionary):\n",
    "    sorted_items = sorted(dictionary.items(), key=lambda item: item[1], reverse=True)[:5]\n",
    "\n",
    "    # Convert sorted items back to a dictionary if needed\n",
    "    top_five_items = dict(sorted_items)\n",
    "    return top_five_items\n",
    "\n",
    "\n",
    "def count_words(text):\n",
    "    text = str(text)\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "\n",
    "\n",
    "def classification(source_folder, destination_folder, keywords, keyword_weights):\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    # Iterate over each file in the source folder\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.endswith(\".pkl\") and not filename.startswith('._'):\n",
    "            print(filename)\n",
    "            # Read the CSV file into a DataFrame\n",
    "            source_filepath = os.path.join(source_folder, filename)\n",
    "\n",
    "            with open(source_filepath, 'rb') as file:\n",
    "                df = pickle.load(file)\n",
    "\n",
    "                # Drop the 'Unnamed: 0' column if it exists\n",
    "                if 'Unnamed: 0' in df.columns:\n",
    "                    df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "                # Count occurrences of keywords in uni_bigrams\n",
    "                df['uni_bigrams_occurrences'] = df['uni_bigrams'].apply(lambda x: count_total_occurrences(x, keywords))\n",
    "\n",
    "                # Count occurrences of each keyword in the text\n",
    "                df['uni_bigrams_word_counts'] = df['uni_bigrams'].apply(\n",
    "                    lambda x: count_individual_occurrences(x, keywords))\n",
    "\n",
    "                # top 5 keywords with scores\n",
    "                df['top5_uni_bigrams_word_counts'] = df['uni_bigrams_word_counts'].apply(\n",
    "                    lambda x: extract_top_five_items(x))\n",
    "\n",
    "                # Calculate scores of each keyword in the text\n",
    "                df['uni_bigrams_word_scores'] = df['uni_bigrams'].apply(\n",
    "                    lambda x: calculate_keyword_scores(x, keywords, keyword_weights))\n",
    "\n",
    "                # Calculate total scores of keywords in the text\n",
    "                df['total_weighted_score'] = df['uni_bigrams'].apply(\n",
    "                    lambda x: calculate_weighted_score(x, keyword_weights))\n",
    "\n",
    "                # Count the words\n",
    "                df['original_text_word_count'] = df['original_text'].apply(count_words)\n",
    "\n",
    "                # Calculate total scores of keywords per word in the text\n",
    "                df['total_weighted_score_per_word'] = df.apply(\n",
    "                    lambda row: row['total_weighted_score'] / row['original_text_word_count'] if row[\n",
    "                                                                                                     'original_text_word_count'] > 0 else 0,\n",
    "                    axis=1)\n",
    "\n",
    "                # top 5 keywords with scores\n",
    "                df['top5_uni_bigrams_word_scores'] = df['uni_bigrams_word_scores'].apply(\n",
    "                    lambda x: extract_top_five_items(x))\n",
    "\n",
    "                #.apply(lambda x: extract_top_five_items(x) if isinstance(x, str) else {})\n",
    "\n",
    "                # Define the new filename for the destination\n",
    "                csv_filename = filename.replace(\"_processed.pkl\", \"_classified.csv\")\n",
    "                destination_filepath_csv = os.path.join(destination_folder, csv_filename)\n",
    "\n",
    "                json_filename = filename.replace(\"_processed.pkl\", \"_classified.json\")\n",
    "                destination_filepath_json = os.path.join(destination_folder, json_filename)\n",
    "\n",
    "                pkl_filename = filename.replace(\"_processed.pkl\", \"_classified.pkl\")\n",
    "                destination_filepath_pkl = os.path.join(destination_folder, pkl_filename)\n",
    "\n",
    "                # Save the modified DataFrame to a new CSV file in the destination folder\n",
    "                #df.to_csv(destination_filepath_csv, index=False)\n",
    "                #df.to_json(destination_filepath_json, orient='records')\n",
    "                df.to_pickle(destination_filepath_pkl)\n",
    "\n",
    "                # This will hold the combined results\n",
    "                combined_dict = defaultdict(int)\n",
    "\n",
    "                # Iterate through each dictionary in the DataFrame's column\n",
    "                for index, row in df.iterrows():\n",
    "                    for key, value in row['uni_bigrams_word_counts'].items():\n",
    "                        combined_dict[key] += value\n",
    "\n",
    "                # Convert the defaultdict back to a regular dictionary for display or further use\n",
    "                result_dict_count = dict(combined_dict)\n",
    "\n",
    "                # Create a WordCloud object\n",
    "                wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
    "\n",
    "                # Generate a word cloud from frequencies\n",
    "                wordcloud.generate_from_frequencies(result_dict_count)\n",
    "\n",
    "                # Display the generated image:\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.imshow(wordcloud, interpolation='bilinear')\n",
    "                plt.axis('off')  # Do not show axes to keep it clean\n",
    "\n",
    "                pic_filename = filename.replace(\"_processed.pkl\", \"_wordcloud_count.png\")\n",
    "                destination_filepath_pic = os.path.join(destination_folder, pic_filename)\n",
    "\n",
    "                # Save the figure to a file\n",
    "                plt.savefig(destination_filepath_pic, format='png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "                dict_count_filename = filename.replace(\"_processed.pkl\", \"_dic_count.csv\")\n",
    "                destination_filepath_dict = os.path.join(destination_folder, dict_count_filename)\n",
    "\n",
    "                # Convert dictionary to DataFrame\n",
    "                result_list_count = pd.DataFrame(list(result_dict_count.items()), columns=['Word', 'Frequency'])\n",
    "\n",
    "                # Save to CSV\n",
    "                result_list_count.to_csv(destination_filepath_dict, index=False)\n",
    "\n",
    "                # This will hold the combined results\n",
    "                combined_dict = defaultdict(int)\n",
    "\n",
    "                # Iterate through each dictionary in the DataFrame's column\n",
    "                for index, row in df.iterrows():\n",
    "                    for key, value in row['uni_bigrams_word_scores'].items():\n",
    "                        combined_dict[key] += value\n",
    "\n",
    "                # Convert the defaultdict back to a regular dictionary for display or further use\n",
    "                result_dict_score = dict(combined_dict)\n",
    "\n",
    "                # Create a WordCloud object\n",
    "                wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
    "\n",
    "                # Generate a word cloud from frequencies\n",
    "                wordcloud.generate_from_frequencies(result_dict_score)\n",
    "\n",
    "                # Display the generated image:\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.imshow(wordcloud, interpolation='bilinear')\n",
    "                plt.axis('off')  # Do not show axes to keep it clean\n",
    "\n",
    "                pic_filename = filename.replace(\"_processed.pkl\", \"_wordcloud_score.png\")\n",
    "                destination_filepath_pic = os.path.join(destination_folder, pic_filename)\n",
    "\n",
    "                # Save the figure to a file\n",
    "                plt.savefig(destination_filepath_pic, format='png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "                dict_count_filename = filename.replace(\"_processed.pkl\", \"_dic_score.csv\")\n",
    "                destination_filepath_dict = os.path.join(destination_folder, dict_count_filename)\n",
    "\n",
    "                # Convert dictionary to DataFrame\n",
    "                result_list_score = pd.DataFrame(list(result_dict_score.items()), columns=['Word', 'Frequency'])\n",
    "\n",
    "                # Save to CSV\n",
    "                result_list_score.to_csv(destination_filepath_dict, index=False)\n",
    "\n",
    "                print(f\"Classified file '{filename}' successfully saved\")\n",
    "\n",
    "\n",
    "source_folder = \"./Raw_Data/Cleaned\"\n",
    "destination_folder = \"./Processed_Data\"\n",
    "\n",
    "generate_uni_bigrams_folder(source_folder, destination_folder)\n",
    "\n",
    "keywords = pd.read_excel('./Meachine Learning/gtm/agriculture_1000_0.1_human_refinement.xlsx')[\n",
    "    'keyword'].values.tolist()\n",
    "keyword_weights = dict(\n",
    "    zip(pd.read_excel('./Meachine Learning/gtm/agriculture_1000_0.1_human_refinement.xlsx')['keyword'],\n",
    "        pd.read_excel('./Meachine Learning/gtm/agriculture_1000_0.1_human_refinement.xlsx')['weight']))\n",
    "source_folder = \"./Processed_Data\"\n",
    "destination_folder = \"./Outcomes\"\n",
    "\n",
    "classification(source_folder, destination_folder, keywords, keyword_weights)\n",
    "########\n",
    "destination_folder = \"./Outcomes\"\n",
    "\n",
    "# Define the columns to keep\n",
    "columns_to_keep = ['state', 'year', 'act_num', 'uni_bigrams_occurrences', 'uni_bigrams_word_counts',\n",
    "                   'top5_uni_bigrams_word_counts', 'uni_bigrams_word_scores', 'total_weighted_score',\n",
    "                   'original_text_word_count',\n",
    "                   'total_weighted_score_per_word', 'top5_uni_bigrams_word_scores']\n",
    "\n",
    "# Initialize an empty DataFrame to hold all the data\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(destination_folder):\n",
    "    if filename.endswith('count.csv') and not filename.startswith('._'):  # Check if the file is a CSV\n",
    "        print(filename)\n",
    "        file_path = os.path.join(destination_folder, filename)\n",
    "        data = pd.read_csv(file_path)\n",
    "        #with open(file_path, 'rb') as file:\n",
    "        #data = pickle.load(file)\n",
    "        #data = data[columns_to_keep]\n",
    "        # Append the data to the main DataFrame\n",
    "        df = df._append(data, ignore_index=True)\n",
    "\n",
    "#df['top5_uni_bigrams_word_scores'] = df['uni_bigrams_word_scores'].apply(lambda d: {k: d[k] for k in list(d.keys())[:5]} if isinstance(d, dict) else d)\n",
    "df = df.groupby('Word')['Frequency'].sum().reset_index()\n",
    "df.to_csv('./Outcomes/all_words_count.csv')\n",
    "\n",
    "# Convert DataFrame to dictionary\n",
    "word_freq = dict(zip(df['Word'], df['Frequency']))\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate_from_frequencies(word_freq)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Do not show axes to keep it clean\n",
    "plt.show()\n",
    "plt.savefig('./Outcomes/counts.png', format='png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Initialize an empty DataFrame to hold all the data\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(destination_folder):\n",
    "    if filename.endswith('score.csv') and not filename.startswith('._'):  # Check if the file is a CSV\n",
    "        print(filename)\n",
    "        file_path = os.path.join(destination_folder, filename)\n",
    "        data = pd.read_csv(file_path)\n",
    "        #with open(file_path, 'rb') as file:\n",
    "        #data = pickle.load(file)\n",
    "        #data = data[columns_to_keep]\n",
    "        # Append the data to the main DataFrame\n",
    "        df = df._append(data, ignore_index=True)\n",
    "\n",
    "#df['top5_uni_bigrams_word_scores'] = df['uni_bigrams_word_scores'].apply(lambda d: {k: d[k] for k in list(d.keys())[:5]} if isinstance(d, dict) else d)\n",
    "df = df.groupby('Word')['Frequency'].sum().reset_index()\n",
    "df.to_csv('./Outcomes/all_words_score.csv')\n",
    "\n",
    "# Convert DataFrame to dictionary\n",
    "word_freq = dict(zip(df['Word'], df['Frequency']))\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate_from_frequencies(word_freq)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Do not show axes to keep it clean\n",
    "plt.show()\n",
    "plt.savefig('./Outcomes/scores.png', format='png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "df = pd.read_csv('./Outcomes/classification_results.csv')\n",
    "\n",
    "try:\n",
    "    df = df.drop(['Unnamed: 0'], axis=1)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "df.dropna(subset=['year'], inplace=True)\n",
    "\n",
    "df['year'] = df['year'].astype(str).str[:4].astype(int)\n",
    "\n",
    "df = df[(df['year'] >= 1975) & (df['year'] <= 2021)]\n",
    "\n",
    "# Group by 'year' and 'state' and calculate the count of positive scores and the total count of scores\n",
    "grouped_data = df.groupby(['year', 'state']).agg(\n",
    "    positive_score_count=('total_weighted_score_per_word', lambda x: (x > 0).sum()),\n",
    "    total_score_count=('total_weighted_score_per_word', 'count')\n",
    ")\n",
    "\n",
    "# Calculate the proportion of the count of positive scores to the total count of scores\n",
    "grouped_data['proportion'] = grouped_data['positive_score_count'] / grouped_data['total_score_count']\n",
    "\n",
    "# Reset the index to make 'year' and 'state' columns again\n",
    "grouped_data.reset_index(inplace=True)\n",
    "\n",
    "grouped_data.to_csv('./Outcomes/classification_results_short.csv')\n",
    "\n",
    "# Pivot the data for plotting\n",
    "pivot_data = grouped_data.pivot(index='year', columns='state', values='proportion')\n",
    "\n",
    "plt.figure(figsize=(30, 24))  # Increase figure size for better clarity and space\n",
    "pivot_data.plot(kind='line')\n",
    "\n",
    "# Increase font sizes for better readability\n",
    "plt.title('Proportion of Ag Legislation per Year by State', fontsize=12)\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.ylabel('Proportion of Ag Legislation', fontsize=12)\n",
    "\n",
    "# Increase tick label size\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Expand the right margin to ensure the legend and plot do not overlap\n",
    "plt.subplots_adjust(right=0.65)\n",
    "\n",
    "# Place the legend to the right of the plot, making it larger to ensure legibility\n",
    "plt.legend(title='State', loc='center left', bbox_to_anchor=(1.05, 0.5), ncol=2, fontsize=12, title_fontsize=12)\n",
    "\n",
    "# Add grid for better readability of the plot\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "state_data = df.groupby('state').agg(\n",
    "    ag_count=('total_weighted_score_per_word', lambda x: (x > 0).sum()),\n",
    "    ag_count_0004=('total_weighted_score_per_word', lambda x: (x > 0.004).sum()),\n",
    "    total_count=('total_weighted_score_per_word', 'count')\n",
    ")\n",
    "\n",
    "state_data['ag_proportion_0000'] = state_data['ag_count'] / state_data['total_count']\n",
    "state_data['ag_proportion_0004'] = state_data['ag_count_0004'] / state_data['total_count']\n",
    "state_data.to_csv('./Outcomes/state.csv')\n",
    "\n",
    "state_data = pd.read_csv('./Outcomes/state.csv')\n",
    "\n",
    "gdf = gpd.read_file('./Geo/cb_2018_us_state_500k.shp')\n",
    "\n",
    "fips = pd.read_excel('./Geo/statefp.xlsx')\n",
    "fips['STATEFP'] = fips['STATEFP'].astype(str).apply(lambda x: x.zfill(2))\n",
    "\n",
    "gdf = gdf.merge(fips, how='left', on='STATEFP')\n",
    "gdf = gdf.merge(state_data, how='left', on='state')\n",
    "\n",
    "\n",
    "# Apply this to the gdf to ensure all states are assigned colors by the same func\n",
    "def makeColorColumn(gdf, variable, vmin, vmax):\n",
    "    # apply a function to a column to create a new column of assigned colors & return full frame\n",
    "    norm = mcolors.Normalize(vmin=vmin, vmax=vmax, clip=True)\n",
    "    mapper = plt.cm.ScalarMappable(norm=norm, cmap=plt.cm.YlOrBr)\n",
    "    gdf['value_determined_color'] = gdf[variable].apply(lambda x: mcolors.to_hex(mapper.to_rgba(x)))\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# set the value column that will be visualised\n",
    "variable = 'ag_proportion_0000'\n",
    "#variable = 'ag_proportion_0004'\n",
    "\n",
    "# make a column for value_determined_color in gdf\n",
    "# set the range for the choropleth values with the upper bound the rounded up maximum value\n",
    "\n",
    "if variable == 'ag_proportion_0000':\n",
    "    gdf['ag_proportion'] = gdf['ag_proportion_0000']\n",
    "else:\n",
    "    gdf['ag_proportion'] = gdf['ag_proportion_0004']\n",
    "\n",
    "vmin, vmax = gdf.ag_proportion.min(), gdf.ag_proportion.max()\n",
    "\n",
    "# Choose the continuous colorscale \"YlOrBr\" from https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "colormap = \"YlOrBr\"\n",
    "gdf = makeColorColumn(gdf, variable, vmin, vmax)\n",
    "\n",
    "# create \"visframe\" as a re-projected gdf using EPSG 2163 for CONUS\n",
    "#visframe = gdf.to_crs({'init':'epsg:2163'})\n",
    "visframe = gdf.to_crs({'proj': 'aea', 'lat_1': 29.5, 'lat_2': 45.5, 'lon_0': -96, 'lat_0': 37.5})\n",
    "\n",
    "# create figure and axes for Matplotlib\n",
    "fig, ax = plt.subplots(1, figsize=(20, 20))\n",
    "# remove the axis box around the vis\n",
    "ax.axis('off')\n",
    "# set the font for the visualization to Helvetica\n",
    "hfont = {'fontname': 'Helvetica'}\n",
    "\n",
    "# add a title and annotation\n",
    "ax.set_title('State-Level Agricultural Legislation\\n1975-2021', **hfont, fontdict={'fontsize': '12', 'fontweight': '1'})\n",
    "\n",
    "# Create colorbar legend\n",
    "fig = ax.get_figure()\n",
    "# add colorbar axes to the figure\n",
    "# This will take some iterating to get it where you want it [l,b,w,h] right\n",
    "# l:left, b:bottom, w:width, h:height; in normalized unit (0-1)\n",
    "cbax = fig.add_axes([0.89, 0.21, 0.03, 0.31])\n",
    "\n",
    "cbax.set_title('Percentage \\n of Ag Legislation \\n (1975-2021)', **hfont,\n",
    "               fontdict={'fontsize': '12', 'fontweight': '0'})\n",
    "\n",
    "# add color scale\n",
    "sm = plt.cm.ScalarMappable(cmap=colormap, \\\n",
    "                           norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "# reformat tick labels on legend\n",
    "sm._A = []\n",
    "#comma_fmt = FuncFormatter(lambda x, p: format(x/100, '%'))\n",
    "comma_fmt = FuncFormatter(lambda x, p: \"{:.1f}%\".format(x * 100))\n",
    "fig.colorbar(sm, cax=cbax, format=comma_fmt)\n",
    "tick_font_size = 16\n",
    "cbax.tick_params(labelsize=tick_font_size)\n",
    "# annotate the data source, date of access, and hyperlink\n",
    "ax.annotate(\"Data: Authors\", xy=(0.5, .085), xycoords='figure fraction', fontsize=12, color='#555555')\n",
    "\n",
    "# create map\n",
    "# Note: we're going state by state here because of unusual coloring behavior when trying to plot the entire dataframe using the \"value_determined_color\" column\n",
    "for row in visframe.itertuples():\n",
    "    if row.state not in ['AK', 'HI']:  # Exclude Alaska and Hawaii for this part\n",
    "        vf = visframe[visframe.state == row.state]\n",
    "        if pd.isna(row.ag_proportion):  # Check if the ag_proportion is NaN\n",
    "            color = 'lightgrey'  # Set color to grey for missing data\n",
    "        else:\n",
    "            color = gdf.loc[gdf.state == row.state, 'value_determined_color'].iloc[0]\n",
    "        vf.plot(color=color, linewidth=1.5, ax=ax, edgecolor='0.8')\n",
    "\n",
    "# add Alaska\n",
    "akax = fig.add_axes([0.4, 0.25, 0.2, 0.13])\n",
    "akax.axis('off')\n",
    "# polygon to clip western islands\n",
    "polygon = Polygon([(-170, 50), (-170, 72), (-140, 72), (-140, 50)])\n",
    "alaska_gdf = gdf[gdf.state == 'AK']\n",
    "alaska_gdf.clip(polygon).plot(color=gdf[gdf.state == 'AK'].value_determined_color, linewidth=0.8, ax=akax,\n",
    "                              edgecolor='0.8')\n",
    "\n",
    "# add Hawaii\n",
    "hiax = fig.add_axes([.58, 0.25, 0.1, 0.1])\n",
    "hiax.axis('off')\n",
    "`\n",
    "# polygon to clip western islands\n",
    "hipolygon = Polygon([(-160, 0), (-160, 90), (-120, 90), (-120, 0)])\n",
    "hawaii_gdf = gdf[gdf.state == 'HI']\n",
    "\n",
    "# Clip the Hawaii GeoDataFrame to the desired area\n",
    "clipped_hawaii_gdf = hawaii_gdf.clip(hipolygon)\n",
    "\n",
    "# Plot the clipped Hawaii GeoDataFrame using the 'value_determined_color' column for color\n",
    "#color = hawaii_gdf['value_determined_color'].iloc[0] if not hawaii_gdf.empty else 'lightgrey'\n",
    "clipped_hawaii_gdf.plot(ax=hiax, color='lightgrey', linewidth=0.8, edgecolor='0.8')\n",
    "\n",
    "if variable == 'ag_proportion_0000':\n",
    "    fig.savefig(os.path.join(os.getcwd(), './Outcomes/ag_legislation_1975_2021_0000.png'), dpi=400, bbox_inches=\"tight\")\n",
    "else:\n",
    "    fig.savefig(os.path.join(os.getcwd(), './Outcomes/ag_legislation_1975_2021_0004.png'), dpi=400, bbox_inches=\"tight\")\n",
    "\n",
    "# bbox_inches=\"tight\" keeps the vis from getting cut off at the edges in the saved png\n",
    "# dip is \"dots per inch\" and controls image quality.  Many scientific journals have specifications for this\n",
    "# https://stackoverflow.com/questions/16183462/saving-images-in-python-at-a-very-high-quality\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "72db359963b886546324fdab9aa5857888ab40de550209afc182f1efe35e5205"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}