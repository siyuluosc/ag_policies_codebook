{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ch 43 North Carolina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " [The North Carolina General Assembly](https://www.ncleg.gov/Laws/SessionLawPublications) stores all session laws of North Carolina under session law publications. On the website, there is an *Acts Archives* section covering acts.\n",
    "\n",
    "![NC Legislature](pics/nc_web.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import libraries\n",
    "As introduced in the chapter 1, we need to import some libraries as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m By\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupport\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mui\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WebDriverWait\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Browser setup\n",
    "\n",
    "Before scraping, we need to set up the browser. Here we use ChromeDriver for Google Chrome. You can download ChromeDriver from https://chromedriver.chromium.org/downloads following the version of Google Chrome you use on your laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "driver_path = 'yourpath/driver'\n",
    "\n",
    "# Change the working directory to your path on your computer\n",
    "os.chdir('yourpath')\n",
    "\n",
    "# Get the working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Set up the driver\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "dnldpath = {\"download.default_directory\": \"your_download_path\"}\n",
    "chromeOptions.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": dnldpath,  #Change default directory for downloads\n",
    "    \"download.prompt_for_download\": False,  #To auto download the file\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True,  #It will not show PDF directly in chrome\n",
    "    \"--enable-javascript\": True\n",
    "})\n",
    "# Open the driver\n",
    "driver = webdriver.Chrome(executable_path=driver_path, options=chromeOptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## PDF file download\n",
    "All session laws on the website are stored as the PDF files. The main goal is to download all session law PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "driver.get(\"https://www.ncleg.gov/Laws/SessionLawPublications\")\n",
    "urls =[]\n",
    "sessions = driver.find_elements_by_css_selector('div.docSiteFiles.mt-3 > div a')\n",
    "\n",
    "for session in sessions:\n",
    "    url = session.get_attribute('href')\n",
    "    urls.append(url)\n",
    "\n",
    "for idx,url in enumerate(urls):\n",
    "    driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Text extraction and saving\n",
    "\n",
    "In this section, we used the below code to extract all full texts from PDF files and save all act full texts into several format files including excel, csv, pickle and json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = \"your_download_path\"\n",
    "files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "\n",
    "acttxts = []\n",
    "\n",
    "for file in files:\n",
    "    with open(os.path.join(os.getcwd(), file), 'r') as f:  # open in readonly mode\n",
    "        # creating a pdf File object of original pdf\n",
    "        pdfFileObj = open(file, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        pagenumber =  pdfReader.numPages\n",
    "        acttxt =[]\n",
    "        pgtxts = []\n",
    "        for p in range(pagenumber):\n",
    "            pageObj = pdfReader.getPage(p)\n",
    "            pgtxt = pageObj.extractText()\n",
    "            pgtxts.append(pgtxt)\n",
    "            acttxt = '\\n'.join(pgtxts)\n",
    "        acttxts.append(acttxt)\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'Full text': acttxts\n",
    "})\n",
    "\n",
    "datasource.drop_duplicates(subset = ['Full text'],\n",
    "                     keep = 'first', inplace = True, ignore_index= True)\n",
    "\n",
    "datasource.to_excel('NC_Leginfo.xlsx')\n",
    "datasource.to_csv('NC_Leginfo.csv')\n",
    "datasource.to_pickle('NC_Leginfo.pkl')\n",
    "datasource.to_json('NC_Leginfo.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import libraries\n",
    "As introduced in the chapter 1, we need to import some libraries as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Browser setup\n",
    "\n",
    "Before scraping, we need to set up the browser. Here we use ChromeDriver for Google Chrome. You can download ChromeDriver from https://chromedriver.chromium.org/downloads following the version of Google Chrome you use on your laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "driver_path = 'yourpath/driver'\n",
    "\n",
    "# Change the working directory to your path on your computer\n",
    "os.chdir('yourpath')\n",
    "\n",
    "# Get the working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Set up the driver\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "dnldpath = {\"download.default_directory\": \"your_download_path\"}\n",
    "chromeOptions.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": dnldpath,  #Change default directory for downloads\n",
    "    \"download.prompt_for_download\": False,  #To auto download the file\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True,  #It will not show PDF directly in chrome\n",
    "    \"--enable-javascript\": True\n",
    "})\n",
    "# Open the driver\n",
    "driver = webdriver.Chrome(executable_path=driver_path, options=chromeOptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## Direct webscraping\n",
    "All session laws on the website are stored as the html file. We can webscrape all session laws directly. Here we adopt a strategy to get all urls for all session laws, and then visit the url to extract the full text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "driver.get(\"https://www.scstatehouse.gov/aacts.php\")\n",
    "sessions = driver.find_elements_by_css_selector(\"div#contentsection a\")\n",
    "\n",
    "for idx, session in enumerate(sessions):\n",
    "    sessionname = session.text\n",
    "    if \"Excel\" in sessionname:\n",
    "        print(sessionname)\n",
    "    else:\n",
    "        session_url = session.get_attribute('href')\n",
    "        session_urls.append(session_url)\n",
    "for idx, session_url in enumerate(session_urls):\n",
    "    driver.get(session_url)\n",
    "    sleep(1)\n",
    "    WebDriverWait(driver, 300).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"h2.barheader\"))\n",
    "    )\n",
    "    yr = driver.find_element_by_css_selector(\"h2.barheader\").text.rsplit(\"Acts\")[1].rsplit(\"Session\")[0].rsplit(\" \")[0]\n",
    "    if yr in specialyears:\n",
    "        sessiongroup = driver.find_elements_by_css_selector(\"div#resultsbox dl dd a\")\n",
    "        for element in sessiongroup:\n",
    "            sgurl = element.get_attribute('href')\n",
    "            driver.get(sgurl)\n",
    "            WebDriverWait(driver, 300).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div#contentsection\"))\n",
    "            )\n",
    "            lawlist = driver.find_elements_by_css_selector(\"div#contentsection dl dt a\")\n",
    "            for element in lawlist:\n",
    "                url = element.get_attribute('href')\n",
    "                urls.append(url)\n",
    "    else:\n",
    "        WebDriverWait(driver, 300).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div#resultsbox dl dd\"))\n",
    "        )\n",
    "        lawlist = driver.find_elements_by_xpath('//*[@id=\"resultsbox\"]/ dl / dt / a[1]')\n",
    "        for element in lawlist:\n",
    "            url = element.get_attribute('href')\n",
    "            urls.append(url)\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    try:\n",
    "        textloc = driver.find_element_by_css_selector(\"div.bill-list-item a.nodisplay\")\n",
    "        docurl = textloc.get_attribute('href')\n",
    "        docurls.append(docurl)\n",
    "        sleep(1)\n",
    "        driver.get(docurl)\n",
    "    except:\n",
    "        docurls.append(url)\n",
    "    body = driver.find_element_by_css_selector(\"body\")\n",
    "    acttxt = body.text\n",
    "    acttxts.append(acttxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Text saving\n",
    "\n",
    "In this section, we used the below code to extract all full texts from PDF files and save all act full texts into several format files including excel, csv, pickle and json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasource = pd.DataFrame({\n",
    "    'Full text': acttxts\n",
    "})\n",
    "\n",
    "datasource.drop_duplicates(subset = ['Full text'],\n",
    "                     keep = 'first', inplace = True, ignore_index= True)\n",
    "\n",
    "datasource.to_excel('NH_Leginfo.xlsx')\n",
    "datasource.to_csv('NH_Leginfo.csv')\n",
    "datasource.to_pickle('NH_Leginfo.pkl')\n",
    "datasource.to_json('NH_Leginfo.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "72db359963b886546324fdab9aa5857888ab40de550209afc182f1efe35e5205"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}