{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ch 7 California"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[California Legislative information](https://leginfo.legislature.ca.gov/faces/advance/advance.xhtml) is an official website that stores all acts passed in California. The web address is https://leginfo.legislature.ca.gov/faces/home.xhtml.\n",
    "\n",
    "![California Legislative information](pics/ca_web.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "California Legislative information Website provides [*Advanced Search*](https://leginfo.legislature.ca.gov/faces/advance/advance.xhtml) function. Through selecting all sessions as shown below, we can get all bills information available on the website. Under the column *Status*, we can tell which bill is chaptered, i.e.,becoming a law. Then, the task of webscraping is to extract the information of all chaptered bills on the website.\n",
    "\n",
    "![California Legislative information](pics/ca_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## import libraries\n",
    "As introduced in the chapter 1, we need to import some libraries as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Browser setup\n",
    "\n",
    "Before scraping, we need to set up the browser. Here we use ChromeDriver for Google Chrome. You can download ChromeDriver from https://chromedriver.chromium.org/downloads following the version of Google Chrome you use on your laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "driver_path = '/yourpath/driver'\n",
    "\n",
    "# Change the working directory to your path on your computer\n",
    "os.chdir('/yourpath/')\n",
    "\n",
    "# Get the working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Set up the driver\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "dnldpath = {\"download.default_directory\": \"/your_download_path\"}\n",
    "chromeOptions.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": dnldpath,  #Change default directory for downloads\n",
    "    \"download.prompt_for_download\": False,  #To auto download the file\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True,  #It will not show PDF directly in chrome\n",
    "    \"--enable-javascript\": True\n",
    "})\n",
    "# Open the driver\n",
    "driver = webdriver.Chrome(executable_path=driver_path, options=chromeOptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Direct webscraping\n",
    "As shown on [California Legislative information Website](https://leginfo.legislature.ca.gov/faces/advance/advance.xhtml) after searching all sessions, we can see returned bills one by one. Clicking the bill under column *Measure*, there is the full text of the bill. For efficiency purposes, we first get all urls for chaptered bills and extract the full texts of the chaptered bills after clicking each url.\n",
    "### Get all urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sessions = ['2021 - 2022', '2019 - 2020', '2017 - 2018', '2015 - 2016', '2013 - 2014', '2011 - 2012', '2009 - 2010', '2007 - 2008', '2005 - 2006', '2003 - 2004', '2001 - 2002', '1999 - 2000']\n",
    "\n",
    "urls = []\n",
    "measures = []\n",
    "subjects = []\n",
    "authors = []\n",
    "statuss = []\n",
    "\n",
    "for session in sessions:\n",
    "    print(session)\n",
    "    driver.get(\"https://leginfo.legislature.ca.gov/faces/billSearchClient.xhtml\")\n",
    "    WebDriverWait(driver, 300).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"#attrSearch\"))\n",
    "    )\n",
    "    select_session = Select(driver.find_element_by_css_selector('select#session_year'))\n",
    "    select_session.select_by_visible_text(session)\n",
    "    search = driver.find_element_by_css_selector('#attrSearch')\n",
    "    search.click()\n",
    "\n",
    "    WebDriverWait(driver, 300).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"tbody tr\")))\n",
    "    bills = driver.find_elements_by_css_selector('tbody tr')\n",
    "    for bill in bills:\n",
    "        if bill.find_element_by_css_selector(\"td:nth-child(4)\").text == 'Chaptered':\n",
    "           url = bill.find_element_by_css_selector(\"td:nth-child(1) a\").get_attribute('href')\n",
    "           urls.append(url)\n",
    "           measure = bill.find_element_by_css_selector(\"td:nth-child(1) a\").text\n",
    "           measures.append(measure)\n",
    "           subject = bill.find_element_by_css_selector(\"td:nth-child(2)\").text\n",
    "           subjects.append(subject)\n",
    "           author = bill.find_element_by_css_selector(\"td:nth-child(3)\").text\n",
    "           authors.append(author)\n",
    "           status = bill.find_element_by_css_selector(\"td:nth-child(4)\").text\n",
    "           statuss.append(status)\n",
    "        else:\n",
    "           print(\"Bill Died\")\n",
    "        time.sleep(0.01)\n",
    "print(\"url done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Extract full texts\n",
    "With the urls we scraped from last subsection, we click each url and extract the full bill text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    driver.get(url)\n",
    "    state = \"CA\"\n",
    "    states.append(state)\n",
    "\n",
    "    billnumber = driver.find_element_by_css_selector('#bill_header > div:nth-child(3) > h1')\n",
    "    billnumbers.append(billnumber.text.rsplit(\" \")[0])\n",
    "\n",
    "    title = driver.find_element_by_css_selector('#bill_header > div:nth-child(3) > h1')\n",
    "    title = title.text.split(\" \",1)[1]\n",
    "    titles.append((title.rsplit(\"(\",1)[0]))\n",
    "\n",
    "    textlinks.append(url)\n",
    "    fulltext = driver.find_element_by_css_selector(\"#bill_all\").text\n",
    "    fulltexts.append(fulltext)\n",
    "    fulltextpage = driver.find_element_by_css_selector(\"#centercolumn\").text\n",
    "    fulltextpages.append(fulltextpage)\n",
    "    sleeptime = randint(1,11)*0.1\n",
    "    time.sleep(sleeptime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Text saving and output\n",
    "\n",
    "After webscraping all chaptered bills' full texts, we saved them into different formats of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasource = pd.DataFrame({\n",
    "    'Year': years,\n",
    "    'State': states,\n",
    "    'Session Year': sessionyears,\n",
    "    'Bill Number': billnumbers,\n",
    "    'Name': titles,\n",
    "    'Brief Summary': briefsummarys,\n",
    "    'Introduced Date': introduceddates,\n",
    "    'Date it was signed':signingdates,\n",
    "    'Date effective':effectivedates,\n",
    "    'Expiration date':expireddates,\n",
    "    'Introducer':leadauthors,\n",
    "    'Full text': fulltexts,\n",
    "    'Link to full text':textlinks\n",
    "})\n",
    "\n",
    "# save bill info into files\n",
    "datasource.to_excel('CA_Leginfo.xlsx')\n",
    "datasource.to_csv('CA_Leginfo.csv')\n",
    "datasource.to_pickle('CA_Leginfo.pkl')\n",
    "datasource.to_json('CA_Leginfo.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Jun  1 2022, 06:36:29) \n[Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "72db359963b886546324fdab9aa5857888ab40de550209afc182f1efe35e5205"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}