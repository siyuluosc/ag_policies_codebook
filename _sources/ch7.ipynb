{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chapter 7 Appendix\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7.1 Web Scraping Script for U.S. States and Washington, D.C."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "#import PyPDF2\n",
    "#import glob\n",
    "\n",
    "driver_path = 'C:/Users/longy/OneDrive/Projects/AFRI/windows/chromedriver.exe' #for windows\n",
    "# driver_path = '/Users/Yanxu/OneDrive/Projects/AgPolicyCoding/chromedriver'\n",
    "# driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "\n",
    "# Change the working directory\n",
    "#os.chdir('/Users/long/OneDrive/Projects/AFRI')\n",
    "os.chdir('C:/Users/longy/OneDrive/Projects/AFRI')\n",
    "# os.chdir('/Volumes/GoogleDrive/Shared drives/UCSC-UMN AFRI project/Data/GA')\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "# for mac\n",
    "#dnldpath = {\"download.default_directory\" : \"/Users/long/OneDrive/Projects/AFRI/data/SD/\"}\n",
    "# for windows\n",
    "dnldpath = {\"download.default_directory\" :\"C:/Users/longy/OneDrive/Projects/AFRI/data/SD/\"}\n",
    "chromeOptions.add_experimental_option(\"prefs\",dnldpath)\n",
    "driver = webdriver.Chrome(executable_path=driver_path, options=chromeOptions)\n",
    "\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "\n",
    "\n",
    "urls=[]\n",
    "sessionurls = []\n",
    "driver.get(\"https://www.sdlegislature.gov/Session/Archived\")\n",
    "#sessions = driver.find_elements_by_link_text(\"Session Laws\")\n",
    "sessions = driver.find_elements_by_css_selector('tbody tr td:nth-child(7) a')\n",
    "\n",
    "for session in sessions:\n",
    "    sessionurl = session.get_attribute('href')\n",
    "    sessionurls.append(sessionurl)\n",
    "\n",
    "for sessionurl in sessionurls:\n",
    "    driver.get(sessionurl)\n",
    "    try:\n",
    "        list = driver.find_elements_by_css_selector(\"tbody tr td:nth-child(1) span a\")\n",
    "        for row in list:\n",
    "            url = row.get_attribute('href')\n",
    "            urls.append(url)\n",
    "    except:\n",
    "        print(sessionurl)\n",
    "print(\"urls are done\")\n",
    "\n",
    "acttxts = []\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "    acttxt = driver.find_element_by_css_selector(\"body\").text\n",
    "    acttxts.append(acttxt)\n",
    "    sleep(1)\n",
    "    #sleep(randint(1,2))\n",
    "datasource = pd.DataFrame({\n",
    "    'Link to full text':urls,\n",
    "    'Full text': acttxts\n",
    "})\n",
    "datasource.to_excel('SD_Leginfo.xlsx')\n",
    "datasource.to_csv('SD_Leginfo.csv')\n",
    "datasource.to_pickle('SD_Leginfo.pkl')\n",
    "datasource.to_json('SD_Leginfo.json')\n",
    "# End of SD_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of IA_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "## selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "## others\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import inflect\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import fitz\n",
    "import re\n",
    "\n",
    "# set up working directory\n",
    "working_directory = '/Volumes/SSD/AFRI/'\n",
    "print(os.path.abspath(working_directory))\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"./Data/Raw_Data/IA/PDFs\"\n",
    "\n",
    "print(os.path.abspath(download_dir))\n",
    "\n",
    "\n",
    "# Specify the path to the Firefox binary\n",
    "firefox_binary_path = '/Applications/Firefox.app/Contents/MacOS/firefox'\n",
    "\n",
    "# Create a Firefox profile and configure preferences\n",
    "options = Options()\n",
    "options.binary_location = firefox_binary_path\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path = './Data/geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "# Initialize the WebDriver with the specified path\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def get_mdls_where_from(file_path):\n",
    "    try:\n",
    "        result = subprocess.run(['mdls', '-name', 'kMDItemWhereFroms', '-raw', file_path],\n",
    "                                stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        if result.returncode == 0:\n",
    "            where_from_info = result.stdout.strip()\n",
    "            if where_from_info:\n",
    "                # The output might be a JSON-like list, so we print it directly\n",
    "                return where_from_info\n",
    "            else:\n",
    "                print(\"No 'Where from' information available.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"Error running mdls command: {result.stderr}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        # Open the PDF file\n",
    "        pdf_document = fitz.open(file_path)\n",
    "        text = \"\"\n",
    "\n",
    "        # Iterate through each page and extract text\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "urls=[]\n",
    "acttxts = []\n",
    "session_nos = []\n",
    "session_names = []\n",
    "for session in range(1,166):\n",
    "\n",
    "    session_url = \"https://www.legis.iowa.gov/law/statutory/acts/actsChapter?ssid=\" + str(session)\n",
    "\n",
    "    # Navigate to the website\n",
    "    driver.get(session_url)\n",
    "\n",
    "    # Wait for the links to be loaded on the page\n",
    "    timeout = 5  # Adjust the timeout value as needed\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.grid_9:nth-child(2) > h2:nth-child(1)')))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        driver.quit()\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    session_no = int(driver.find_element(By.CSS_SELECTOR, 'div.grid_9:nth-child(2) > h2:nth-child(1)').text.replace(' ', '').split('(')[0].replace('IowaActsChapters-', ''))\n",
    "\n",
    "    session_name = driver.find_element(By.CSS_SELECTOR, 'div.grid_9:nth-child(2) > h2:nth-child(1)').text\n",
    "\n",
    "    if 65 < session_no < 90:\n",
    "        print(session_no)\n",
    "        try:\n",
    "            chapters = driver.find_elements(By.PARTIAL_LINK_TEXT,'Chapter')\n",
    "            for chapter in chapters:\n",
    "                print(chapter.text)\n",
    "                url = chapter.get_attribute('href')\n",
    "                urls.append(url)\n",
    "                chapter.click()\n",
    "                session_nos.append(session_no)\n",
    "                session_names.append(session_name)\n",
    "        except:\n",
    "            print(\"no Chapter found\")\n",
    "            print(url)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "folder_path = \"./Data/Raw_Data/IA/PDFs\"\n",
    "list_of_files = glob.glob(os.path.join(folder_path, '*.pdf'))\n",
    "#latest_file = max(list_of_files, key=os.path.getctime)\n",
    "acttxts = []\n",
    "urls = []\n",
    "file_names = []\n",
    "for idx,file in enumerate(list_of_files):\n",
    "    print(idx)\n",
    "    acttxt = extract_text_from_pdf(file)\n",
    "    acttxts.append(acttxt)\n",
    "    url = get_mdls_where_from(file).strip('()\\n ').strip('\"').strip('\\n').replace(' ', '').replace('\",\\n', '')\n",
    "    urls.append(url)\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'original_text': acttxts,\n",
    "    'link': urlss,\n",
    "    'file_name': list_of_files,\n",
    "    'session_number': session_no,\n",
    "    'session_name': session_names\n",
    "})\n",
    "s\n",
    "datasource['year'] = datasource['session_name'].apply(lambda x: x.split('(')[1][:4])\n",
    "\n",
    "datasource['original_act_num'] = datasource['file_name'].apply(lambda x: x.split('PDFs/')[1][:6])\n",
    "\n",
    "datasource['act_num'] = \"IA\" + datasource['year'] + datasource['original_act_num']\n",
    "\n",
    "datasource = datasource.assign(state='IA')\n",
    "\n",
    "datasource.to_csv('./Data/Raw_Data/IA/IA_Leginfo.csv')\n",
    "datasource.to_pickle('./Data/Raw_Data/IA/IA_Leginfo.pkl')\n",
    "datasource.to_json('./Data/Raw_Data/IA/IA_Leginfo.json')\n",
    "# End of IA_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of ME_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import fitz\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "#driver_path = 'C:/Users/longy/OneDrive/Projects/AFRI/windows/chromedriver.exe' #for windows\n",
    "driver_path = '/Users/long/OneDrive/Projects/AFRI/chromedriver'\n",
    "driver_path = '/Volumes/SSD/AFRI/Data/chromedriver'\n",
    "# driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir('/Volumes/SSD/AFRI/Data/Raw_Data/ME')\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_directory = \"/Volumes/SSD/AFRI/Data/Raw_Data/ME\"\n",
    "\n",
    "# Set Chrome options to automatically download files to the specified directory\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (without opening the browser window)\n",
    "chrome_options.add_experimental_option(\"prefs\", {\n",
    "    \"download.default_directory\": download_directory,\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True  # Open PDF files in an external PDF viewer\n",
    "})\n",
    "\n",
    "# Create the WebDriver instance\n",
    "driver = webdriver.Chrome(executable_path=driver_path, options=chrome_options)\n",
    "\n",
    "\n",
    "\n",
    "# import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import fitz\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "#driver_path = 'C:/Users/longy/OneDrive/Projects/AFRI/windows/chromedriver.exe' #for windows\n",
    "driver_path = '/Users/long/OneDrive/Projects/AFRI/chromedriver'\n",
    "driver_path = '/Volumes/SSD/AFRI/Data/chromedriver'\n",
    "# driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir('/Volumes/SSD/AFRI/Data/Raw_Data/ME')\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"/Volumes/SSD/AFRI/Data/Raw_Data/ME\"\n",
    "\n",
    "# Create a Firefox profile and configure preferences\n",
    "firefox_profile = webdriver.FirefoxProfile()\n",
    "\n",
    "# Set the download directory\n",
    "firefox_profile.set_preference(\"browser.download.folderList\", 2)\n",
    "firefox_profile.set_preference(\"browser.download.dir\", download_dir)\n",
    "\n",
    "# Disable the download dialog and set MIME types for automatic download\n",
    "firefox_profile.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "firefox_profile.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/pdf\")\n",
    "firefox_profile.set_preference(\"browser.helperApps.neverAsk.openFile\", \"application/pdf\")\n",
    "\n",
    "# Disable Firefox's built-in PDF viewer\n",
    "firefox_profile.set_preference(\"pdfjs.disabled\", True)\n",
    "\n",
    "# Start Firefox with the customized profile\n",
    "\n",
    "driver = webdriver.Firefox(executable_path='/Volumes/SSD/AFRI/Data/geckodriver',firefox_profile=firefox_profile)\n",
    "\n",
    "\n",
    "driver.get('http://lldc.mainelegislature.org/Open/Laws/')\n",
    "\n",
    "sessions = ['1975', '1977', '1979', '1981', '1983', '1985', '1987', '1989', '1991', '1993', '1995', '1997', '1999', '2001', '2003', '2005', '2007', '2009', '2011', '2013', '2015', '2017', '2019', '2021']\n",
    "\n",
    "urls = []\n",
    "for session in sessions:\n",
    "    driver.get('http://lldc.mainelegislature.org/Open/Laws/')\n",
    "    driver.find_element(By.PARTIAL_LINK_TEXT, session).click()\n",
    "    pdfs = driver.find_elements(By.PARTIAL_LINK_TEXT, session)\n",
    "    for pdf in pdfs:\n",
    "        pdf.get_attribute('href')\n",
    "        url = pdf.get_attribute('href')\n",
    "        if \"_PL_\" in url:\n",
    "            urls.append(url)\n",
    "        else:\n",
    "            pass\n",
    "        urls.append(url)\n",
    "\n",
    "\n",
    "bad_act_urls = []\n",
    "\n",
    "path = \"/Volumes/SSD/AFRI/Data/Raw_Data/ME/PDFs\"\n",
    "\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    # Ensure the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Write the content of the response to a file\n",
    "        with open(path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "    else:\n",
    "        print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "        bad_act_urls.append(url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "act_txts = []\n",
    "pgtxts = []\n",
    "\n",
    "path = \"/Volumes/SSD/AFRI/Data/Raw_Data/ME/PDFs\"\n",
    "files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "\n",
    "for file in files:\n",
    "    doc = fitz.open(file)\n",
    "    pgtxts = []\n",
    "    for page in doc:\n",
    "        pgtxt = page.get_text()\n",
    "        pgtxts.append(pgtxt)\n",
    "        acttxt = ' '.join(pgtxts)\n",
    "    act_txts.append(acttxt)\n",
    "    doc.close()\n",
    "\n",
    "\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'links':urls,\n",
    "    'original_text': act_txts,\n",
    "    'file_name': files\n",
    "})\n",
    "\n",
    "datasource.to_excel('ME_Leginfo.xlsx')\n",
    "datasource.to_csv('ME_Leginfo.csv')\n",
    "datasource.to_pickle('ME_Leginfo.pkl')\n",
    "datasource.to_json('ME_Leginfo.json')\n",
    "\n",
    "\n",
    "# End of ME_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of IL_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "## selenium\n",
    "import selenium\n",
    "\n",
    "print(selenium.__version__)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.support.ui import Select\n",
    "## others\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "import requests\n",
    "import fitz\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# set up working directory\n",
    "working_directory = '/Volumes/SSD/AFRI/Data/Raw_Data/IL'\n",
    "print(os.path.abspath(working_directory))\n",
    "\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"./PDFs/\"\n",
    "print(os.path.abspath(download_dir))\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path = '../../geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "years = []\n",
    "subtitles = []\n",
    "bill_nums = []\n",
    "filenames = []\n",
    "act_urls = []\n",
    "sessions = []\n",
    "broken_act_urls = []\n",
    "act_txts = []\n",
    "bill_status_urls = []\n",
    "bill_status_pages = []\n",
    "effective_dates = []\n",
    "orginal_act_nums = []\n",
    "\n",
    "for num in range(90, 103):\n",
    "    if num < 93:\n",
    "        print(num)\n",
    "        for act_num in range(1, 1300):\n",
    "            print(act_num)\n",
    "            start_year = 1817 + num * 2\n",
    "            end_year = 1818 + num * 2\n",
    "            session = f\"({start_year}-{end_year})\"\n",
    "            sessions.append(session)\n",
    "\n",
    "            url = 'https://www.ilga.gov/legislation/publicacts/pubact' + str(num) + \"/acts/\" + str(num) + \"-\" + str(\n",
    "                act_num).zfill(4) + \".html\"\n",
    "\n",
    "            act_urls.append(url)\n",
    "\n",
    "            try:\n",
    "                # Navigate to the webpage\n",
    "                driver.get(url)\n",
    "            except:\n",
    "                break\n",
    "\n",
    "            WebDriverWait(driver, 60).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"body\"))\n",
    "            )\n",
    "            act_txts.append(driver.find_element(By.CSS_SELECTOR, \"body\").text)\n",
    "\n",
    "            try:\n",
    "                bill_status.append(driver.find_element(By.LINK_TEXT, \"Bill Status\").get_attribute('href'))\n",
    "            except:\n",
    "                bill_status.append('NA')\n",
    "\n",
    "            try:\n",
    "                act_pdf_urls.append(driver.find_element(By.LINK_TEXT, \"PDF\").get_attribute('href'))\n",
    "            except:\n",
    "                act_pdf_urls.append('NA')\n",
    "\n",
    "    else:\n",
    "        print(num)\n",
    "        for act_num in range(1, 1300):\n",
    "            print(act_num)\n",
    "            start_year = 1817 + num * 2\n",
    "            end_year = 1818 + num * 2\n",
    "            session = f\"({start_year}-{end_year})\"\n",
    "            sessions.append(session)\n",
    "\n",
    "            url = 'https://www.ilga.gov/legislation/publicacts/fulltext.asp?Name=' + str(num).zfill(3) + \"-\" + str(\n",
    "                act_num).zfill(4) + \"&GA=\" + str(num)\n",
    "\n",
    "            act_urls.append(url)\n",
    "\n",
    "            try:\n",
    "                # Navigate to the webpage\n",
    "                driver.get(url)\n",
    "            except:\n",
    "                break\n",
    "\n",
    "            WebDriverWait(driver, 60).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"body\"))\n",
    "            )\n",
    "            act_txts.append(driver.find_element(By.CSS_SELECTOR, \"body\").text)\n",
    "\n",
    "            try:\n",
    "                bill_status_urls.append(driver.find_element(By.LINK_TEXT, \"Bill Status\").get_attribute('href'))\n",
    "            except:\n",
    "                bill_status_urls.append('NA')\n",
    "\n",
    "            try:\n",
    "                act_pdf_urls.append(driver.find_element(By.LINK_TEXT, \"PDF\").get_attribute('href'))\n",
    "            except:\n",
    "                act_pdf_urls.append('NA')\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'session': sessions,\n",
    "    'original_text': act_txts,\n",
    "    'link': act_urls,\n",
    "    'bill_status_urls': bill_status_urls,\n",
    "    'pdf_url': act_pdf_urls,\n",
    "    'state': 'IL',\n",
    "    'original_text': act_txts\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "datasource = datasource['original_text'].str.replace['Translate Website\\n\\n\\n  Home     Legislation & Laws     Senate     House     My Legislation     Site Map    \\n\\nPrevious General Assemblies\\n\\n','']\n",
    "\n",
    "datasource['original_text'] = datasource['original_text'].str.replace('Translate Website\\n\\n\\n  Home     Legislation & Laws     Senate     House     My Legislation     Site Map    \\n\\nPrevious General Assemblies\\n\\n','',regex=False)\n",
    "\n",
    "datasource['original_text'] = datasource['original_text'].str.replace('State of Illinois\\nPublic Acts\\n90th General Assembly\\n[ Home ] [ Public Acts ] [ ILCS ] [ Search ] [ Bottom ]\\n','',regex=False)\n",
    "\n",
    "datasource['original_text'] = datasource['original_text'].str.replace('State of Illinois\\n91st General Assembly\\nPublic Acts\\n[ Home ]  [ ILCS ] [ Search ] [ Bottom ]\\n [ Other General Assemblies ]\\n','', regex=False)\n",
    "\n",
    "datasource = datasource[~datasource['original_text'].str.contains(\"No LegislationDocumentId returned.\")]\n",
    "\n",
    "for index,url in enumerate(datasource['bill_status']):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"body\"))\n",
    "        )\n",
    "        bill_status_pages.append(driver.find_element(By.CSS_SELECTOR, \"body\").text)\n",
    "    except:\n",
    "        bill_status_pages.append('NA')\n",
    "\n",
    "datasource['bill_status_pages'] = bill_status_pages\n",
    "\n",
    "datasource = datasource[~datasource['bill_status_pages'].str.contains(\"Request Not Found\")]\n",
    "\n",
    "# Reset the index\n",
    "datasource = datasource.reset_index(drop=True)\n",
    "\n",
    "for index, text in enumerate(datasource['bill_status_pages']):\n",
    "    print(index)\n",
    "    # Regular expression pattern to find dates in the format MM/DD/YYYY\n",
    "    year_pattern = r'\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b'\n",
    "\n",
    "    # Find all occurrences of the pattern in the text\n",
    "    dates_found = re.findall(pattern, text)\n",
    "\n",
    "    # Convert the string dates to datetime objects and find the earliest\n",
    "    if dates_found:\n",
    "        # Convert each found date string to a datetime object\n",
    "        date_objects = [datetime.strptime(date, '%B %d, %Y') for date in dates_found]\n",
    "        # Find the earliest date\n",
    "        earliest_date = min(date_objects)\n",
    "        print(f\"Earliest date found: {earliest_date.strftime('%m/%d/%Y')}\")\n",
    "        year = earliest_date.year\n",
    "        years.append(year)\n",
    "    else:\n",
    "        print(\"No dates found in the text.\")\n",
    "        years.append('')\n",
    "\n",
    "    effective_date_pattern = r'Effective Date (\\w+ \\d{1,2}, \\d{4})'\n",
    "    # Search for the pattern in the text\n",
    "    effective_date_match = re.search(effective_date_pattern, text)\n",
    "\n",
    "    # Check if a match was found\n",
    "    if effective_date_match:\n",
    "        # Extract the date string\n",
    "        date_str = effective_date_match.group(1)\n",
    "\n",
    "        # Parse the date string into a datetime object\n",
    "        date_obj = datetime.strptime(date_str, '%B %d, %Y')  # '%B %d, %Y' is the format like 'March 17, 2023'\n",
    "\n",
    "        # Convert the datetime object to the desired format MM/DD/YYYY\n",
    "        effective_date = date_obj.strftime('%m/%d/%Y')\n",
    "        effective_dates.append(effective_date)\n",
    "    else:\n",
    "        print(\"No date found after 'Effective Date' in the given text.\")\n",
    "        effective_dates.append('')\n",
    "\n",
    "datasource['year'] = years\n",
    "datasource['effective_date'] = effective_dates\n",
    "\n",
    "# PA(90-0564) 12-22-97\n",
    "# PA(91-0676) 12-23-99\n",
    "# PA(92-0505) 12-20-01\n",
    "\n",
    "for text in datasource['original_text']:\n",
    "    #Regular expression pattern to find numbers after 'Public Act'\n",
    "    pattern = r'Public Act (\\d+-\\d+)'\n",
    "\n",
    "    # Search for the pattern in the text\n",
    "    match = re.search(pattern, text)\n",
    "    # Check if any matches were found and print them\n",
    "    if match:\n",
    "        orginal_act_nums.append(match.group(1))\n",
    "    else:\n",
    "        print(\"No numbers found after 'Public Act' in the given text.\")\n",
    "        orginal_act_nums.append('')\n",
    "\n",
    "datasource['orginal_act_num'] = orginal_act_nums\n",
    "\n",
    "for index, row in datasource.iterrows():\n",
    "    num = row['orginal_act_num']\n",
    "    if row['year'] == '':\n",
    "        parts = num.split('-')\n",
    "        if int(parts[0]) == 90:\n",
    "            if int(parts[1]) <= 564:\n",
    "                datasource.at[index, 'year'] = 1997\n",
    "            else:\n",
    "                datasource.at[index, 'year'] = 1998\n",
    "        elif int(parts[0]) == 91:\n",
    "            if int(parts[1]) <= 676:\n",
    "                datasource.at[index, 'year'] = 1999\n",
    "            else:\n",
    "                datasource.at[index, 'year'] = 2000\n",
    "        else:\n",
    "            if int(parts[1]) <= 505:\n",
    "                datasource.at[index, 'year'] = 2001\n",
    "            else:\n",
    "                datasource.at[index, 'year'] = 2002\n",
    "\n",
    "\n",
    "\n",
    "datasource = datasource[~datasource['original_text'].str.contains('404 - File or directory not found')]\n",
    "\n",
    "datasource.to_excel('IL_Leginfo.xlsx')\n",
    "datasource.to_csv('IL_Leginfo.csv')\n",
    "datasource.to_pickle('IL_Leginfo.pkl')\n",
    "datasource.to_json('IL_Leginfo.json')\n",
    "\n",
    "\n",
    "# End of IL_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of KY_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "import fitz\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "#import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "#import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "import pdfplumber\n",
    "import PyPDF2\n",
    "import requests\n",
    "import re\n",
    "# Set the working directory\n",
    "os.chdir('/Volumes/SSD/AFRI/Data/Raw_Data/KY')\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "\n",
    "\n",
    "#driver_path = r'C:/Users/longy/OneDrive/Projects/AFRI/windows/chromedriver.exe' #for windows\n",
    "driver_path = '/Users/long/Library/CloudStorage/OneDrive-Personal/Projects/AFRI/chromedriver'\n",
    "# driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "chromeOptions.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": \"/Volumes/SSD/AFRI/Data/Raw_Data/KY\", #Change default directory for downloads\n",
    "    \"download.prompt_for_download\": True, #To auto download the file\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True, #It will not show PDF directly in chrome\n",
    "    \"--enable-javascript\":True})\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "\n",
    "\n",
    "# driver = webdriver.Firefox(executable_path='/path_to_geckodriver_folder/geckodriver')\n",
    "# If geckodriver is in your PATH, you can initiate it directly\n",
    "#driver = webdriver.Firefox()\n",
    "\n",
    "\n",
    "# Set up browser options to auto-download PDFs\n",
    "options = Options()\n",
    "options.set_preference(\"browser.download.folderList\", 2)\n",
    "options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "options.set_preference(\"browser.download.dir\", '/Volumes/SSD/AFRI/Data/Raw_Data/KY/acts_PDFs')\n",
    "\n",
    "driver = webdriver.Firefox(options=options)\n",
    "\n",
    "\n",
    "session_url = []\n",
    "act_urls = []\n",
    "bad_act_urls = []\n",
    "bill_num = []\n",
    "driver.get(\"https://legislature.ky.gov/Law/Pages/KyActs.aspx\")\n",
    "sessions = driver.find_elements(By.PARTIAL_LINK_TEXT, \"Session\")\n",
    "for index,session in enumerate(sessions[:37]):\n",
    "    session_url.append(session.get_attribute('href'))\n",
    "\n",
    "for index,url in enumerate(session_url):\n",
    "    if index < 19:\n",
    "        driver.get(url)\n",
    "        driver.find_element(By.LINK_TEXT, \"Acts Table by Chapter\").click()\n",
    "        sleep(3)\n",
    "        bills = driver.find_elements(By.CSS_SELECTOR, \".table > tbody:nth-child(1) > tr > td:nth-child(2)\")\n",
    "        for bill in bills:\n",
    "            bill_num.append(bill.text)\n",
    "        acts = driver.find_elements(By.CSS_SELECTOR, \".table > tbody:nth-child(1) > tr > td:nth-child(1) > a:nth-child(1)\")\n",
    "        for act in acts:\n",
    "            act_url = act.get_attribute('href')\n",
    "            act_urls.append(act_url)\n",
    "            try:\n",
    "                name = str(act_url).split(\"acts/\")[1].split(\".pdf\")[0].replace(\"/\",\"\").replace(\"documents\",\"\")\n",
    "            except:\n",
    "                name = str(act_url).split(\"ACTS\")[1].replace(\"/\",\"\").replace(\".pdf\",\"\")\n",
    "            path = \"/Volumes/SSD/AFRI/Data/Raw_Data/KY/acts_PDFs/\" + name + \".pdf\"\n",
    "            # Send a GET request to the URL\n",
    "            response = requests.get(act_url)\n",
    "            # Ensure the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Write the content of the response to a file\n",
    "                with open(path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "            else:\n",
    "                print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "                bad_act_urls.append(act_url)\n",
    "    elif index < 22:\n",
    "        driver.get(url)\n",
    "        driver.find_element(By.LINK_TEXT, \"Acts Table by Chapter\").click()\n",
    "        sleep(3)\n",
    "        bills = driver.find_elements(By.CSS_SELECTOR,\n",
    "                                     \"body > table:nth-child(6) > tbody:nth-child(1) > tr > td:nth-child(2)\")\n",
    "        for bill in bills:\n",
    "            bill_num.append(bill.text)\n",
    "        acts = driver.find_elements(By.CSS_SELECTOR,\n",
    "                                    \"body > table:nth-child(6) > tbody:nth-child(1) > tr > td:nth-child(1) > a:nth-child(1)\")\n",
    "        for act in acts:\n",
    "            act_url = act.get_attribute('href')\n",
    "            act_urls.append(act_url)\n",
    "            name = str(act_url).split(\"acts/\")[1].split(\".pdf\")[0].replace(\"/\", \"\").replace(\"documents\", \"\")\n",
    "            path = \"/Volumes/SSD/AFRI/Data/Raw_Data/KY/acts_PDFs/\" + name + \".pdf\"\n",
    "            # Send a GET request to the URL\n",
    "            response = requests.get(act_url)\n",
    "            # Ensure the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Write the content of the response to a file\n",
    "                with open(path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "            else:\n",
    "                print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "                bad_act_urls.append(act_url)\n",
    "    elif index == 22:\n",
    "        driver.get(url)\n",
    "        driver.find_element(By.LINK_TEXT, \"Acts Table by Chapter\").click()\n",
    "        sleep(3)\n",
    "        bills = driver.find_elements(By.CSS_SELECTOR, \"body > table > tbody > tr> td:nth-child(1)\")\n",
    "        for bill in bills:\n",
    "            bill_num.append(bill.text)\n",
    "        acts = driver.find_elements(By.CSS_SELECTOR,\n",
    "                                    \"body > table > tbody > tr > td:nth-child(2) > a\")\n",
    "        for act in acts:\n",
    "            act_url = act.get_attribute('href')\n",
    "            act_urls.append(act_url)\n",
    "            try:\n",
    "                name = str(act_url).split(\"acts/\")[1].split(\".pdf\")[0].replace(\"/\", \"\").replace(\"documents\", \"\")\n",
    "            except:\n",
    "                name = str(act_url).split(\"ACTS\")[1].replace(\"/\", \"\").replace(\".pdf\", \"\")\n",
    "            path = \"/Volumes/SSD/AFRI/Data/Raw_Data/KY/acts_PDFs/\" + name + \".pdf\"\n",
    "            # Send a GET request to the URL\n",
    "            response = requests.get(act_url)\n",
    "            # Ensure the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Write the content of the response to a file\n",
    "                with open(path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "            else:\n",
    "                print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "                bad_act_urls.append(act_url)\n",
    "    elif index < 33:\n",
    "        driver.get(url)\n",
    "        driver.find_element(By.LINK_TEXT, \"Acts Table by Chapter\").click()\n",
    "        sleep(3)\n",
    "        bills = driver.find_elements(By.CSS_SELECTOR,\n",
    "                                     \"body > table:nth-child(6) > tbody:nth-child(1) > tr > td:nth-child(2)\")\n",
    "        for bill in bills:\n",
    "            bill_num.append(bill.text)\n",
    "        acts = driver.find_elements(By.CSS_SELECTOR,\n",
    "                                    \"body > table:nth-child(6) > tbody:nth-child(1) > tr > td:nth-child(1) > a:nth-child(1)\")\n",
    "        for act in acts:\n",
    "            act_url = act.get_attribute('href')\n",
    "            act_urls.append(act_url)\n",
    "            name = str(act_url).split(\"acts/\")[1].split(\".pdf\")[0].replace(\"/\", \"\").replace(\"documents\", \"\")\n",
    "            path = \"/Volumes/SSD/AFRI/Data/Raw_Data/KY/acts_PDFs/\" + name + \".pdf\"\n",
    "            # Send a GET request to the URL\n",
    "            response = requests.get(act_url)\n",
    "            # Ensure the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Write the content of the response to a file\n",
    "                with open(path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "            else:\n",
    "                print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "                bad_act_urls.append(act_url)\n",
    "    else:\n",
    "        driver.get(url)\n",
    "        driver.find_element(By.LINK_TEXT, \"Acts Table by Chapter\").click()\n",
    "        sleep(3)\n",
    "        bills = driver.find_elements(By.CSS_SELECTOR, \"body > table > tbody > tr> td:nth-child(2)\")\n",
    "        for bill in bills:\n",
    "            bill_num.append(bill.text)\n",
    "        acts = driver.find_elements(By.CSS_SELECTOR, \"body > table > tbody > tr> td:nth-child(1) > a\")\n",
    "        for act in acts:\n",
    "            act_url = act.get_attribute('href')\n",
    "            act_urls.append(act_url)\n",
    "            try:\n",
    "                name = str(act_url).split(\"acts/\")[1].split(\".pdf\")[0].replace(\"/\",\"\").replace(\"documents\",\"\")\n",
    "            except:\n",
    "                name = str(act_url).split(\"ACTS\")[1].replace(\"/\",\"\").replace(\".pdf\",\"\")\n",
    "            path = \"/Volumes/SSD/AFRI/Data/Raw_Data/KY/acts_PDFs/\" + name + \".pdf\"\n",
    "            # Send a GET request to the URL\n",
    "            response = requests.get(act_url)\n",
    "            # Ensure the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Write the content of the response to a file\n",
    "                with open(path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "            else:\n",
    "                print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "                bad_act_urls.append(act_url)\n",
    "\n",
    "for index, url in enumerate(session_url[33:36]):\n",
    "    driver.get(url)\n",
    "    driver.find_element(By.LINK_TEXT, \"Acts Table by Chapter\").click()\n",
    "    sleep(3)\n",
    "    bills = driver.find_elements(By.CSS_SELECTOR, \"body > table > tbody:nth-child(1) > tr > td:nth-child(2)\")\n",
    "    for bill in bills:\n",
    "        bill_num.append(bill.text)\n",
    "    acts = driver.find_elements(By.CSS_SELECTOR,\n",
    "                                \"body > table > tbody:nth-child(1) > tr > td:nth-child(1) > a:nth-child(1)\")\n",
    "    for act in acts:\n",
    "        act_url = act.get_attribute('href')\n",
    "        act_urls.append(act_url)\n",
    "        name = str(act_url).split(\"acts/\")[1].split(\".pdf\")[0].replace(\"/\", \"\").replace(\"documents\", \"\")\n",
    "        path = \"/Volumes/SSD/AFRI/Data/Raw_Data/KY/acts_PDFs/\" + name + \".pdf\"\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(act_url)\n",
    "        # Ensure the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Write the content of the response to a file\n",
    "            with open(path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "            bad_act_urls.append(act_url)\n",
    "\n",
    "for index, url in enumerate(session_url[35:36]):\n",
    "    driver.get(url)\n",
    "    driver.find_element(By.LINK_TEXT, \"Acts Table by Chapter\").click()\n",
    "    sleep(3)\n",
    "    bills = driver.find_elements(By.CSS_SELECTOR, \"body > table > tbody:nth-child(1) > tr > td:nth-child(2)\")\n",
    "    for bill in bills:\n",
    "        bill_num.append(bill.text)\n",
    "    acts = driver.find_elements(By.CSS_SELECTOR,\n",
    "                                \"body > table > tbody:nth-child(1) > tr > td:nth-child(1) > a:nth-child(1)\")\n",
    "    for act in acts:\n",
    "        act_url = act.get_attribute('href')\n",
    "        act_urls.append(act_url)\n",
    "        name = str(act_url).split(\"acts/\")[1].split(\".pdf\")[0].replace(\"/\", \"\").replace(\"documents\", \"\")\n",
    "        path = \"/Volumes/SSD/AFRI/Data/Raw_Data/KY/acts_PDFs/\" + name + \".pdf\"\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(act_url)\n",
    "        # Ensure the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Write the content of the response to a file\n",
    "            with open(path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "            bad_act_urls.append(act_url)\n",
    "\n",
    "\n",
    "act_txts = []\n",
    "\n",
    "act_urls1 = list(set(act_urls))\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'bill_num':bill_num,\n",
    "    'act_urls': act_urls\n",
    "})\n",
    "\n",
    "# Remove duplicates based on 'act_urls'\n",
    "datasource = datasource.drop_duplicates(subset='act_urls', keep='first')\n",
    "\n",
    "datasource = datasource[datasource['act_urls'] != \"http://www.lrc.ky.gov/Statrev/ACTS2003/0194.pdf\"]\n",
    "\n",
    "act_txts = []\n",
    "\n",
    "bad_files = []\n",
    "path = \"/Volumes/SSD/AFRI/Data/Raw_Data/KY/acts_PDFs\"\n",
    "files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        doc = fitz.open(file)\n",
    "        pgtxts = []\n",
    "        for page in doc:\n",
    "            pgtxt = page.get_text()\n",
    "            pgtxts.append(pgtxt)\n",
    "            acttxt = ' '.join(pgtxts)\n",
    "        act_txts.append(acttxt)\n",
    "        doc.close()\n",
    "    except:\n",
    "        print(file)\n",
    "        bad_files.append(file)\n",
    "\n",
    "# Replace 'path_to_pdf' with the path to your PDF file\n",
    "file = \"./KY_1998.pdf\"\n",
    "\n",
    "doc = fitz.open(file)\n",
    "pgtxts = []\n",
    "for page in doc:\n",
    "    pgtxt = page.get_text()\n",
    "    pgtxts.append(pgtxt)\n",
    "    acttxt = ' '.join(pgtxts)\n",
    "acttxt_backup = acttxt\n",
    "year_1998_acts = acttxt.split(\"Section 1.   \")\n",
    "\n",
    "\n",
    "def split_by_chapter(text):\n",
    "    # Define the pattern to split on\n",
    "    pattern = re.compile(r'CHAPTER \\d+ \\n\\(')\n",
    "\n",
    "    # Split the text using the pattern\n",
    "    chapters = re.split(pattern, text)\n",
    "\n",
    "    # Filter out empty strings\n",
    "    chapters = [chapter for chapter in chapters if chapter]\n",
    "\n",
    "    return chapters\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "chapters = split_by_chapter(acttxt)\n",
    "\n",
    "year_1998_chapters =  chapters[1:]\n",
    "\n",
    "year_1998_chapters = [f\"Chapter {i+1} {item}\" for i, item in enumerate(year_1998_chapters)]\n",
    "\n",
    "year_1998_bill_num = []\n",
    "\n",
    "for act in chapters[1:]:\n",
    "    year_1998_bill_num.append(act.split(\") \\n\")[0].replace(\" \", \"\"))\n",
    "\n",
    "# Creating DataFrame\n",
    "df_1998 = pd.DataFrame({'original_text': year_1998_chapters, 'bill_num': year_1998_bill_num})\n",
    "df_1998['act_urls'] = 'https://apps.legislature.ky.gov/law/acts/98RS/actsmas.pdf'\n",
    "df_1998['file_name'] = \"KY_1998.pdf\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('./KY_Leginfo.csv', index_col=0)\n",
    "\n",
    "df = pd.concat([df, df_1998], ignore_index=True)\n",
    "\n",
    "df['state'] = \"KY\"\n",
    "\n",
    "\n",
    "df.to_excel('KY_Leginfo.xlsx')\n",
    "df.to_csv('KY_Leginfo.csv')\n",
    "df.to_pickle('KY_Leginfo.pkl')\n",
    "df.to_json('KY_Leginfo.json')\n",
    "\n",
    "\n",
    "# End of KY_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of NJ_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "## selenium\n",
    "import selenium\n",
    "print(selenium.__version__)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.support.ui import Select\n",
    "## others\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "import requests\n",
    "import fitz\n",
    "import re\n",
    "\n",
    "# set up working directory\n",
    "working_directory = '/Volumes/SSD/AFRI/Data/Raw_Data/NJ'\n",
    "print(os.path.abspath(working_directory))\n",
    "\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"./PDFs/\"\n",
    "print(os.path.abspath(download_dir))\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path = '../../geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service = service)\n",
    "\n",
    "# empty list to store info\n",
    "\n",
    "act_pdf_urls = []\n",
    "act_html_urls = []\n",
    "bill_nums = []\n",
    "chapter_nums = []\n",
    "years = []\n",
    "sessions = []\n",
    "broken_act_urls = []\n",
    "act_txts = []\n",
    "act_names = []\n",
    "names = []\n",
    "#approved_dates = []\n",
    "file_names = []\n",
    "\n",
    "\n",
    "# New Jersey Legislature Website: https://www.njleg.state.nj.us/chapter-laws\n",
    "\n",
    "driver.get('https://www.njleg.state.nj.us/chapter-laws')\n",
    "\n",
    "WebDriverWait(driver, 60).until(\n",
    "    EC.element_to_be_clickable((By.CSS_SELECTOR, '.chapter-laws-filter_yearSelect__G_DM8'))\n",
    ")\n",
    "\n",
    "select_session = Select(driver.find_element(By.CSS_SELECTOR,'.chapter-laws-filter_yearSelect__G_DM8'))\n",
    "\n",
    "sessions = driver.find_elements(By.CSS_SELECTOR,'.chapter-laws-filter_yearSelect__G_DM8  > option')\n",
    "\n",
    "for session in sessions:\n",
    "    if int(session.text) < 2022:\n",
    "        print(session.text)\n",
    "        select_session.select_by_visible_text(session.text)\n",
    "\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, 'button.chapter-laws-filter_filterButtons__NFfv4:nth-child(1)'))\n",
    "        )\n",
    "\n",
    "        driver.find_element(By.CSS_SELECTOR,'button.chapter-laws-filter_filterButtons__NFfv4:nth-child(1)').click()\n",
    "\n",
    "        total_page = int(driver.find_element(By.CSS_SELECTOR, 'div.chapter-laws-pagination_pagination__ulkGd:nth-child(1) > div:nth-child(1)').text.split('of ')[1])\n",
    "\n",
    "        for page in range(0,total_page):\n",
    "            print(page)\n",
    "\n",
    "            chapter_num_elements = driver.find_elements(By.CSS_SELECTOR,\n",
    "                                                        'div.chapter-laws-list_chapterLaw__KotAo > div:nth-child(1) > div:nth-child(1) > span:nth-child(1)')\n",
    "\n",
    "            # Iterate over each element and get the text\n",
    "            for element in chapter_num_elements:\n",
    "                chapter_nums.append(element.text)\n",
    "\n",
    "\n",
    "            bill_num_elements = driver.find_elements(By.CSS_SELECTOR,\n",
    "                                                     'div.chapter-laws-list_chapterLaw__KotAo > div:nth-child(1) > div:nth-child(1) > a:nth-child(2)')\n",
    "\n",
    "            # Iterate over each element and get the text\n",
    "            for element in bill_num_elements:\n",
    "                bill_nums.append(element.text)\n",
    "\n",
    "\n",
    "            summmary_elements = driver.find_elements(By.CSS_SELECTOR,\n",
    "                                                     'div.chapter-laws-list_chapterLaw__KotAo > div:nth-child(2) > p:nth-child(1)')\n",
    "\n",
    "            # Iterate over each element and get the text\n",
    "            for element in summmary_elements:\n",
    "                names.append(element.text)\n",
    "\n",
    "\n",
    "            act_pdf_url_elements = driver.find_elements(By.CSS_SELECTOR,\n",
    "                                                        'div.chapter-laws-list_chapterLaw__KotAo > div:nth-child(1) > div:nth-child(2) > a:nth-child(1)')\n",
    "\n",
    "            # Iterate over each element and get the text\n",
    "            for element in act_pdf_url_elements:\n",
    "                act_pdf_urls.append(element.get_attribute('href'))\n",
    "\n",
    "            #act_html_url_elements = driver.find_elements(By.CSS_SELECTOR,\n",
    "            # 'div.chapter-laws-list_chapterLaw__KotAo > div:nth-child(1) > div:nth-child(2) > a:nth-child(2)')\n",
    "\n",
    "            # Iterate over each element and get the text\n",
    "            #for element in act_html_url_elements:\n",
    "            #    act_html_urls.append(element.get_attribute('href'))\n",
    "\n",
    "            #approved_date_elements = driver.find_elements(By.CSS_SELECTOR,\n",
    "            #  'div.chapter-laws-list_chapterLaw__KotAo > div:nth-child(1) > div:nth-child(2) > p:nth-child(3)')\n",
    "\n",
    "            # Iterate over each element and get the text\n",
    "            #for element in approved_date_elements:\n",
    "            #    approved_dates.append(element.text)\n",
    "\n",
    "            try:\n",
    "                driver.find_element(By.CSS_SELECTOR,'div.chapter-laws-pagination_pagination__ulkGd:nth-child(1) > div:nth-child(2) > a:last-child  > svg:nth-child(1) > path:nth-child(1)').click()\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# download corresponding pdf file\n",
    "\n",
    "for act_url in act_pdf_urls:\n",
    "    print(index)\n",
    "    print(act_url)\n",
    "    name = act_url.replace('/','').split('nj.usBills')[1]\n",
    "    file_names.append(name)\n",
    "    print(name)\n",
    "\n",
    "    path = download_dir + name\n",
    "    print(os.path.abspath(path))\n",
    "\n",
    "    time.sleep(randint(1, 100) * 0.005)\n",
    "\n",
    "    response = requests.get(act_url)\n",
    "    # Ensure the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Write the content of the response to a file\n",
    "        with open(path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "    else:\n",
    "        print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "        broken_act_urls.append(act_url)\n",
    "datasource= pd.DataFrame({\n",
    "    'bill_num': bill_nums,\n",
    "    'chapter_num': chapter_nums,\n",
    "    'link': act_pdf_urls,\n",
    "    'state': \"NJ\",\n",
    "    'name': names\n",
    "})\n",
    "datasource['file_name'] = datasource['link'].apply(lambda x: x.replace('/', '').split('nj.usBills')[1])\n",
    "\n",
    "#read pdfs\n",
    "folder_path = \"./PDFs\"\n",
    "print(os.path.abspath(folder_path))\n",
    "\n",
    "files = glob.glob(os.path.join(folder_path, '*.PDF'))\n",
    "\n",
    "broken_files = []\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        doc = fitz.open(file)\n",
    "        pgtxts = []\n",
    "        for page in doc:\n",
    "            pgtxt = page.get_text()\n",
    "            pgtxts.append(pgtxt)\n",
    "            acttxt = ' '.join(pgtxts)\n",
    "        act_txts.append(acttxt)\n",
    "        doc.close()\n",
    "    except:\n",
    "        act_txts.append('NA')\n",
    "        broken_files.append(file)\n",
    "\n",
    "file_names = []\n",
    "\n",
    "for file in files:\n",
    "    file_names.append(file.split(\"./PDFs/\")[1])\n",
    "\n",
    "datasource_pdf = pd.DataFrame({\n",
    "    'original_text': act_txts,\n",
    "    'file_name': file_names,\n",
    "})\n",
    "\n",
    "\n",
    "datasource = pd.merge(datasource,datasource_pdf,on='file_name', how='left')\n",
    "\n",
    "# delete one act with missing pdf from the url https://pub.njleg.state.nj.us/Bills/2010/PL11/209_.PDF\n",
    "datasource = datasource.dropna(subset=['original_text'])\n",
    "# delete duplicates\n",
    "datasource = datasource.drop_duplicates(subset=['original_text'])\n",
    "\n",
    "\n",
    "datasource.to_excel('NJ_Leginfo.xlsx')\n",
    "datasource.to_csv('NJ_Leginfo.csv')\n",
    "datasource.to_pickle('NJ_Leginfo.pkl')\n",
    "datasource.to_json('NJ_Leginfo.json')\n",
    "\n",
    "# End of NJ_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of WV_Leginfo.py\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import fitz\n",
    "\n",
    "\n",
    "#driver_path = 'I:\\Projects\\AFRI/chromedriver.exe' #for windows\n",
    "driver_path = '/Users/long/Library/CloudStorage/OneDrive-Personal/Projects/AFRI/chromedriver'\n",
    "# driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "\n",
    "# Change the working directory\n",
    "#os.chdir('/Users/long/OneDrive/Projects/AFRI')\n",
    "os.chdir('/Volumes/SSD/AFRI/Data/Raw_Data/WV')\n",
    "# os.chdir('/Volumes/GoogleDrive/Shared drives/UCSC-UMN AFRI project/Data/GA')\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "chromeOptions.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": '/Users/long/Library/CloudStorage/OneDrive-Personal/Projects/AFRI/data/WV', #Change default directory for downloads\n",
    "    \"download.prompt_for_download\": False, #To auto download the file\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True #It will not show PDF directly in chrome\n",
    "})\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=driver_path, options=chromeOptions)\n",
    "\n",
    "act_urls = []\n",
    "checklist = []\n",
    "session_acts = []\n",
    "acttxts = []\n",
    "\n",
    "driver.get(\"https://www.wvlegislature.gov/Educational/publications.cfm#acts\")\n",
    "\n",
    "#most sessions except 2003\n",
    "\n",
    "sessions = driver.find_elements_by_partial_link_text(\"Vol.\")\n",
    "\n",
    "for session in sessions:\n",
    "    url = session.get_attribute(\"href\")\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "\n",
    "othersessions = [\"https://www.wvlegislature.gov/legisdocs/publications/acts/Acts_2003_Vol_1.pdf\",\"https://www.wvlegislature.gov/legisdocs/publications/acts/Acts_2003_Vol_2.pdf\",\"https://www.wvlegislature.gov/legisdocs/publications/acts/Acts_2003_2ES.pdf\"]\n",
    "for url in othersessions:\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "acttxts = []\n",
    "path = \"/Volumes/SSD/AFRI/Data/Raw_Data/WV/PDFs\"\n",
    "files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "for file in files:\n",
    "    doc = fitz.open(file)\n",
    "    pgtxts = []\n",
    "    for page in doc:\n",
    "        pgtxt = page.get_text()\n",
    "        pgtxts.append(pgtxt)\n",
    "        acttxt = ' '.join(pgtxts)\n",
    "    acttxts.append(acttxt)\n",
    "    doc.close()\n",
    "\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'original_text': acttxts,\n",
    "    'file_name': files\n",
    "})\n",
    "\n",
    "# save bill info into files\n",
    "datasource.to_excel('WV_Leginfo.xlsx')\n",
    "datasource.to_csv('WV_Leginfo.csv')\n",
    "datasource.to_pickle('WV_Leginfo.pkl')\n",
    "datasource.to_json('WV_Leginfo.json')\n",
    "# End of WV_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of ID_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "## selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "## others\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import inflect\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import fitz\n",
    "import re\n",
    "\n",
    "\n",
    "state = \"ID\"\n",
    "# Read the CSV file into a DataFrame\n",
    "work_dir = f'/Volumes/SSD/AFRI/Data/Raw_Data/{state}/'\n",
    "# Change the working directory\n",
    "os.chdir(work_dir)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "\n",
    "# Specify the path to the Firefox binary\n",
    "firefox_binary_path = '/Applications/Firefox.app/Contents/MacOS/firefox'\n",
    "\n",
    "# Create a Firefox profile and configure preferences\n",
    "options = Options()\n",
    "options.binary_location = firefox_binary_path\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path = './Data/geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "# Initialize the WebDriver with the specified path\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "\n",
    "\n",
    "driver.get('https://legislature.idaho.gov/statutesrules/sessionlaws/')\n",
    "\n",
    "sessions = driver.find_elements(By.PARTIAL_LINK_TEXT, \"Volume\")\n",
    "\n",
    "session_urls = []\n",
    "\n",
    "for session in sessions:\n",
    "    session_url = session.get_attribute('href')\n",
    "    session_urls.append(session_url)\n",
    "for idx, url in enumerate(session_urls):\n",
    "    driver.get(url)\n",
    "    print(idx)\n",
    "    sleep(6)\n",
    "\n",
    "\n",
    "path = \"PDFs\"\n",
    "files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "\n",
    "acttxts = []\n",
    "year = []\n",
    "chapter_num = []\n",
    "for file in files:\n",
    "    pgtxts = []\n",
    "    doc = fitz.open(file)  # Open the file explicitly\n",
    "    try:\n",
    "        for page in doc:\n",
    "            pgtxt = page.get_text()\n",
    "            pgtxts.append(pgtxt)\n",
    "\n",
    "        acttxt = '  '.join(pgtxts)\n",
    "    finally:\n",
    "        doc.close()  # Ensure the file is closed after processing\n",
    "    acttxt = acttxt.split('RESOLUTIONS \\n')[0]\n",
    "    parts = re.split(r'\\nCHAPTER \\d+ \\n', acttxt)\n",
    "    acttxts.extend(parts[1:])\n",
    "    chapter_num.extend(re.findall(r'\\nCHAPTER (\\d+) \\n', acttxt))\n",
    "    # Append the same number of rows (for demonstration, we'll append a placeholder)\n",
    "    for i in range(len(parts[1:])):\n",
    "        year.append(file[-8:-4])\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'original_text': acttxts,\n",
    "    'year': year,\n",
    "    'chapter_num': chapter_num,\n",
    "    'state': \"ID\"\n",
    "})\n",
    "\n",
    "datasource.to_excel('ID_Leginfo.xlsx')\n",
    "datasource.to_csv('ID_Leginfo.csv')\n",
    "datasource.to_pickle('ID_Leginfo.pkl')\n",
    "datasource.to_json('ID_Leginfo.json')\n",
    "\n",
    "\n",
    "# End of ID_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of LA_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "## selenium\n",
    "import selenium\n",
    "print(selenium.__version__)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
    "## others\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "import requests\n",
    "import fitz\n",
    "import re\n",
    "import math\n",
    "\n",
    "# set up working directory\n",
    "working_directory = '/Volumes/SSD/AFRI/Data/Raw_Data/LA'\n",
    "print(os.path.abspath(working_directory))\n",
    "\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"./PDFs/\"\n",
    "print(os.path.abspath(download_dir))\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path = '../../geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service = service)\n",
    "\n",
    "\n",
    "driver.get('https://www.legis.la.gov/Legis/SessionInfo/SessionInfo.aspx')\n",
    "\n",
    "\n",
    "acts = []\n",
    "act_urls = []\n",
    "act_names = []\n",
    "act_nums = []\n",
    "bill_nums = []\n",
    "years = []\n",
    "records = []\n",
    "\n",
    "years = ['97RS','971ES','972ES','973ES','98RS','981ES','982ES','983ES','99RS','991ES','992ES','993ES','00RS','001ES','002ES','003ES','01RS','011ES','012ES','013ES','02RS','021ES','022ES','023ES','03RS','031ES','032ES','033ES','04RS','041ES','042ES','043ES','05RS','051ES','052ES','053ES','06RS','061ES','062ES','063ES','07RS','071ES','072ES','073ES','08RS','081ES','082ES','083ES','09RS','091ES','092ES','093ES','10RS','101ES','102ES','103ES','11RS','111ES','112ES','113ES','12RS','121ES','122ES','123ES','13RS','131ES','132ES','133ES','14RS','141ES','142ES','143ES','15RS','151ES','152ES','153ES','16RS','161ES','162ES','163ES','17RS','171ES','172ES','173ES','18RS','181ES','182ES','183ES','19RS','191ES','192ES','193ES','20RS','201ES','202ES','203ES','21RS','211ES','212ES','213ES']\n",
    "\n",
    "for year in years[1:]:\n",
    "    url = 'https://www.legis.la.gov/Legis/ActNumbers.aspx?sid=' + year\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"body\"))\n",
    "        )\n",
    "\n",
    "        act_elements = driver.find_elements(By.CSS_SELECTOR,'.ResultsListTable > tbody:nth-child(2) > *')\n",
    "\n",
    "        for element in act_elements:\n",
    "\n",
    "            act_nums.append(element.find_element(By.CSS_SELECTOR, ':nth-child(1)').text)\n",
    "\n",
    "            bill_nums.append(element.find_element(By.CSS_SELECTOR, ':nth-child(2)').text)\n",
    "\n",
    "            act_urls.append(element.find_element(By.PARTIAL_LINK_TEXT, 'B').get_attribute('href'))\n",
    "\n",
    "            act_names.append(element.find_element(By.CSS_SELECTOR, ':nth-child(4)').text)\n",
    "\n",
    "        num_pages = math.floor(int(re.findall(r'\\d+', driver.find_element(By.CSS_SELECTOR,'#ctl00_ctl00_PageBody_PageContent_LabelTotalInstruments').text)[0])/100) + 1\n",
    "\n",
    "        if num_pages > 1:\n",
    "            for n in range(2,num_pages+1):\n",
    "\n",
    "                print(n)\n",
    "\n",
    "                if n == 11:\n",
    "                    next_page = driver.find_element(By.LINK_TEXT,'...').click()\n",
    "                else:\n",
    "                    next_page = driver.find_element(By.LINK_TEXT,f\"{n}\").click()\n",
    "\n",
    "                time.sleep(3)\n",
    "\n",
    "                act_elements = driver.find_elements(By.CSS_SELECTOR, '.ResultsListTable > tbody:nth-child(2) > *')\n",
    "\n",
    "                for element in act_elements:\n",
    "                    act_nums.append(element.find_element(By.CSS_SELECTOR, ':nth-child(1)').text)\n",
    "\n",
    "                    bill_nums.append(element.find_element(By.CSS_SELECTOR, ':nth-child(2)').text)\n",
    "\n",
    "                    act_urls.append(element.find_element(By.PARTIAL_LINK_TEXT, 'B').get_attribute('href'))\n",
    "\n",
    "                    act_names.append(element.find_element(By.CSS_SELECTOR, ':nth-child(4)').text)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "pdf_urls = []\n",
    "for act_url in act_urls:\n",
    "    driver.get(act_url)\n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"body\"))\n",
    "    )\n",
    "    year = driver.find_element(By.CSS_SELECTOR, \"#ctl00_PageBody_LabelSession\").text.split(' REGULAR')[0]\n",
    "    years.append(year)\n",
    "    records.append(driver.find_element(By.CSS_SELECTOR, \"#ctl00_PageBody_PanelBillInfo\").text)\n",
    "\n",
    "    driver.get(driver.find_element(By.LINK_TEXT,'Text').get_attribute('href'))\n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"body\"))\n",
    "    )\n",
    "    pdf_urls.append(driver.find_element(By.CSS_SELECTOR, \"a\").get_attribute('href'))\n",
    "\n",
    "    time.sleep(randint(1, 100)*0.005)\n",
    "\n",
    "\n",
    "file_names = []\n",
    "\n",
    "for index, link in enumerate(pdf_urls):\n",
    "    print(index)\n",
    "    name = str(link).split(\"d=\")[1]\n",
    "    file_names.append(name + \".pdf\")\n",
    "    path = download_dir + name + \".pdf\"\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(link)\n",
    "    # Ensure the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Write the content of the response to a file\n",
    "        with open(path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "    else:\n",
    "        print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "        bad_act_urls.append(link)\n",
    "    time.sleep(randint(1, 100)*0.005)\n",
    "\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'act_name': act_names,\n",
    "    'bill_num': bill_nums,\n",
    "    'act_num': act_nums,\n",
    "    'link': act_urls,\n",
    "    'pdf_url': pdf_urls,\n",
    "    'file_name': file_names,\n",
    "    'year': years,\n",
    "    'record': records\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "files = glob.glob(os.path.join(download_dir, '*.pdf'))\n",
    "\n",
    "file_names = []\n",
    "\n",
    "broken_files = []\n",
    "\n",
    "act_txts = []\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        print(file)\n",
    "        doc = fitz.open(file)\n",
    "        pgtxts = []\n",
    "        for page in doc:\n",
    "            pgtxt = page.get_text()\n",
    "            pgtxts.append(pgtxt)\n",
    "            act_txt = ' '.join(pgtxts)\n",
    "        act_txts.append(act_txt)\n",
    "        doc.close()\n",
    "        file_names.append(file.split('PDFs/')[1])\n",
    "    except:\n",
    "        broken_files.append(file)\n",
    "\n",
    "\n",
    "\n",
    "file_df = pd.DataFrame({\n",
    "    'file_name': file_names,\n",
    "    'original_text': act_txts\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "datasource.to_csv('LA_Leginfo.csv', index=False)\n",
    "datasource.to_excel('LA_Leginfo.xlsx', index=False)\n",
    "datasource.to_pickle('LA_Leginfo.pkl')\n",
    "datasource.to_json('LA_Leginfo.json', index=False)\n",
    "\n",
    "#datasource = pd.read_csv('LA_Leginfo.csv', index_col=False)\n",
    "# End of LA_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of MT_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "## selenium\n",
    "import selenium\n",
    "print(selenium.__version__)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.support.ui import Select\n",
    "## others\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "import requests\n",
    "import fitz\n",
    "import re\n",
    "# Counting the occurrence of each item in the list\n",
    "from collections import Counter\n",
    "\n",
    "# set up working directory\n",
    "working_directory = '/Volumes/SSD/AFRI/Data/Raw_Data/MT'\n",
    "print(os.path.abspath(working_directory))\n",
    "\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"./PDFs/\"\n",
    "print(os.path.abspath(download_dir))\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path = '../../geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service = service)\n",
    "\n",
    "# empty list to store info\n",
    "\n",
    "act_pdf_urls = []\n",
    "act_html_urls = []\n",
    "pdf_file_names = []\n",
    "html_file_names = []\n",
    "bill_nums = []\n",
    "chapter_nums = []\n",
    "years = []\n",
    "sessions = []\n",
    "broken_act_urls = []\n",
    "act_txts = []\n",
    "act_names = []\n",
    "names = []\n",
    "file_names = []\n",
    "summaries = []\n",
    "session_urls = []\n",
    "statuss = []\n",
    "rowtxts = []\n",
    "\n",
    "# Montana Legislature Website: https://laws.leg.mt.gov/legprd/law0203w$.startup\n",
    "\n",
    "for i in ['20211', '20191', '20171', '20172', '20151', '20131', '20111', '20091', '20071', '20072', '20051', '20051', '20031', '20011', '19991']:\n",
    "    print(i)\n",
    "    url = 'https://laws.leg.mt.gov/legprd/LAW0217W$BAIV.return_all_bills?P_SESS=' + i\n",
    "    session_urls.append(url)\n",
    "    driver.get(url)\n",
    "\n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, 'body'))\n",
    "    )\n",
    "\n",
    "    rows = driver.find_elements(By.CSS_SELECTOR,'body > table:nth-child(13) > tbody:nth-child(1) > tr')\n",
    "\n",
    "    for row in rows[1:]:\n",
    "        if \"*\" not in row.text:\n",
    "\n",
    "            print(row.text)\n",
    "            rowtxts.append(row.text)\n",
    "\n",
    "            act_html_url = row.find_element(By.CSS_SELECTOR, 'td:nth-child(1) > a:nth-child(2)').get_attribute('href')\n",
    "            act_html_urls.append(act_html_url)\n",
    "\n",
    "            act_pdf_url = row.find_element(By.CSS_SELECTOR,'td:nth-child(1) > a:nth-child(3)').get_attribute('href')\n",
    "            act_pdf_urls.append(act_pdf_url)\n",
    "\n",
    "            bill_num = row.find_element(By.CSS_SELECTOR,'td:nth-child(1) > a:nth-child(1)').text\n",
    "            bill_nums.append(bill_num.replace(' ',''))\n",
    "\n",
    "            chapter_num = row.find_element(By.CSS_SELECTOR,'td:nth-child(2)').text\n",
    "            chapter_nums.append(chapter_num)\n",
    "\n",
    "            status = row.find_element(By.CSS_SELECTOR,'td:nth-child(4)').text\n",
    "            statuss.append(status)\n",
    "\n",
    "            year = row.find_element(By.CSS_SELECTOR,'td:nth-child(5)').text\n",
    "            years.append(year[-4:])\n",
    "\n",
    "            name = row.find_element(By.CSS_SELECTOR,'td:nth-child(6)').text\n",
    "            names.append(name)\n",
    "\n",
    "            time.sleep(randint(1, 3)*0.001)\n",
    "\n",
    "\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'bill_num': rowtxts,\n",
    "    'chapter_num': chapter_nums,\n",
    "    'act_pdf_url': act_pdf_urls,\n",
    "    'act_html_url': act_html_urls,\n",
    "    'state': \"MT\",\n",
    "    'name':names,\n",
    "    'year': years,\n",
    "    'status':statuss\n",
    "})\n",
    "\n",
    "file_names = []\n",
    "\n",
    "for url in datasource['act_pdf_url']:\n",
    "    file_names.append(url.split('/billpdf')[0][-4:] + url.split('billpdf/')[1].split('.htm')[0])\n",
    "\n",
    "datasource['file_name'] = file_names\n",
    "\n",
    "act_txts = []\n",
    "file_names = []\n",
    "for act_url in act_pdf_urls:\n",
    "    print(act_url)\n",
    "    name = act_url.split('/billpdf')[0][-4:] + act_url.split('billpdf/')[1].split('.htm')[0]\n",
    "    file_names.append(name)\n",
    "    print(name)\n",
    "\n",
    "    path = download_dir + name\n",
    "    print(os.path.abspath(path))\n",
    "\n",
    "    time.sleep(randint(1, 100) * 0.005)\n",
    "\n",
    "    response = requests.get(act_url)\n",
    "    # Ensure the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Write the content of the response to a file\n",
    "        with open(path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "    else:\n",
    "        print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "        broken_act_urls.append(act_url)\n",
    "\n",
    "folder_path = \"./PDFs\"\n",
    "print(os.path.abspath(folder_path))\n",
    "\n",
    "files = glob.glob(os.path.join(folder_path, '*.pdf'))\n",
    "\n",
    "broken_files = []\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        doc = fitz.open(file)\n",
    "        pgtxts = []\n",
    "        for page in doc:\n",
    "            pgtxt = page.get_text()\n",
    "            pgtxts.append(pgtxt)\n",
    "            acttxt = ' '.join(pgtxts)\n",
    "        act_txts.append(acttxt)\n",
    "        doc.close()\n",
    "    except:\n",
    "        act_txts.append('NA')\n",
    "        broken_files.append(file)\n",
    "\n",
    "file_names = []\n",
    "\n",
    "for file in files:\n",
    "    file_names.append(file.split(\"./PDFs/\")[1])\n",
    "\n",
    "datasource_pdf = pd.DataFrame({\n",
    "    'original_text': act_txts,\n",
    "    'file_name': file_names,\n",
    "})\n",
    "\n",
    "\n",
    "datasource = pd.merge(datasource, datasource_pdf, on='file_name', how= 'left')\n",
    "\n",
    "\n",
    "\n",
    "# missing url 'http://leg.mt.gov/bills/2007/billpdf/SB0382.pdf'\n",
    "datasource = datasource.dropna(subset=['original_text'])\n",
    "# delete duplicates\n",
    "datasource = datasource.drop_duplicates(subset=['original_text'])\n",
    "\n",
    "datasource.to_excel('MT_Leginfo.xlsx')\n",
    "datasource.to_csv('MT_Leginfo.csv')\n",
    "datasource.to_pickle('MT_Leginfo.pkl')\n",
    "datasource.to_json('MT_Leginfo.json')\n",
    "\n",
    "# End of MT_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of FL_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "## selenium\n",
    "import selenium\n",
    "print(selenium.__version__)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "## others\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "import requests\n",
    "import fitz\n",
    "import re\n",
    "\n",
    "# set up working directory\n",
    "working_directory = '/Volumes/SSD/AFRI/Data/Raw_Data/FL'\n",
    "#working_directory = '/Volumes/SSD/AFRI/Data/Raw_Data/FL'\n",
    "print(os.path.abspath(working_directory))\n",
    "\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"./PDFs/\"\n",
    "print(os.path.abspath(download_dir))\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path = '../../geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service = service)\n",
    "\n",
    "# empty list to store info\n",
    "file_names = []\n",
    "act_urls = []\n",
    "bill_nums = []\n",
    "chapter_nums = []\n",
    "years = []\n",
    "sessions = []\n",
    "broken_act_urls = []\n",
    "act_txts = []\n",
    "act_names = []\n",
    "\n",
    "#for 1975-1996\n",
    "session_urls = []\n",
    "years_1975_1996 = []\n",
    "file_names_1975_1996 = []\n",
    "# there are two sources to web-scrape for FL leginfo.\n",
    "# for 1975-1996\n",
    "\n",
    "# 1975 special session\n",
    "name = '1975' + '_' + 'special.pdf'\n",
    "file_names.append(name)\n",
    "path = download_dir + name\n",
    "years_1975_1996.append(1975)\n",
    "response = requests.get('http://edocs.dlis.state.fl.us/fldocs/leg/actsflorida/1975/LOF1975V1Ch001-306.pdf')\n",
    "session_urls.append('http://edocs.dlis.state.fl.us/fldocs/leg/actsflorida/1975/LOF1975V1Ch001-306.pdf')\n",
    "\n",
    "# Ensure the request was successful\n",
    "# Write the content of the response to a file\n",
    "with open(path, 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "driver.get('http://edocs.dlis.state.fl.us/fldocs/leg/actsflorida/index.htm')\n",
    "# regular sessions in 1975-1996\n",
    "for index, year in enumerate(range(1975, 1997)):\n",
    "\n",
    "    print(year)\n",
    "\n",
    "    # get the last two digits of the year\n",
    "    partial_link = f\"{year % 100}-\"\n",
    "\n",
    "    session_elements = WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_all_elements_located((By.PARTIAL_LINK_TEXT, partial_link))\n",
    "    )\n",
    "\n",
    "    for index, element in enumerate(session_elements):\n",
    "        session_url = element.get_attribute(\"href\")\n",
    "        print(session_url)\n",
    "        session_urls.append(session_url)\n",
    "\n",
    "        name = str(year) + '_' + str(index+1) + '.pdf'\n",
    "        file_names_1975_1996.append(name)\n",
    "        years_1975_1996.append(year)\n",
    "\n",
    "\n",
    "        path = download_dir + '/1975-1996/'+ name\n",
    "        print(os.path.abspath(path))\n",
    "\n",
    "        time.sleep(randint(1, 100) * 0.005)\n",
    "\n",
    "        response = requests.get(pdf_url)\n",
    "        # Ensure the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Write the content of the response to a file\n",
    "            with open(path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "            broken_act_urls.append(pdf_url)\n",
    "\n",
    "\n",
    "datasource_1975_1996 = pd.DataFrame({\n",
    "    'file_name': file_names,\n",
    "    'year':years,\n",
    "    'link':session_urls\n",
    "})\n",
    "\n",
    "folder_path = \"./PDFs/1975_1996/\"\n",
    "print(os.path.abspath(folder_path))\n",
    "\n",
    "files = glob.glob(os.path.join(folder_path, '*.pdf'))\n",
    "\n",
    "broken_files = []\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        doc = fitz.open(file)\n",
    "        pgtxts = []\n",
    "        for page in doc:\n",
    "            pgtxt = page.get_text()\n",
    "            pgtxts.append(pgtxt)\n",
    "            session_txt = ' '.join(pgtxts)\n",
    "        session_txts.append(session_txt)\n",
    "        doc.close()\n",
    "    except:\n",
    "        print('NA')\n",
    "        session_txts.append('NA')\n",
    "        broken_files.append(file)\n",
    "\n",
    "#creat a key to match\n",
    "file_names = []\n",
    "\n",
    "for file in files:\n",
    "    file_names.append(file.split(\"1975_1996/\")[1])\n",
    "\n",
    "datasource_1975_1996_new = pd.DataFrame({\n",
    "    'original_text': session_txts,\n",
    "    'file_name': file_names\n",
    "})\n",
    "\n",
    "datasource_1975_1996 = pd.merge(datasource_1975_1996,datasource_1975_1996_new, on='file_name', how='left')\n",
    "\n",
    "act_txts = []\n",
    "years = []\n",
    "file_names = []\n",
    "act_urls = []\n",
    "\n",
    "for index, row in datasource_1975_1996.iterrows():\n",
    "    text = row['original_text']\n",
    "    year = row['year']\n",
    "    file_name = row['file_name']\n",
    "    act_url = row['link']\n",
    "\n",
    "    page_header_pattern = r'LAWS OF FLORIDA\\s+\\n+CHAPTER | CHAPTER+.*?\\s+\\n+LAWS OF FLORIDA | CHAPTER+.*?\\s+\\n+LAWS OF FLORIDA\\s+\\n+CHAPTER '\n",
    "    # Finding all occurrences of the pattern in the text\n",
    "    matches = re.findall(page_header_pattern, text)\n",
    "\n",
    "    # Print the matches\n",
    "    for match in matches:\n",
    "        text = text.replace(match, '')\n",
    "\n",
    "    # split the chapters\n",
    "    chapters =[]\n",
    "    chap_pattern  = r'Filed in Office Secretary of State \\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\b \\d{1,2}, \\d{4}\\.'\n",
    "    # Split the text using the modified pattern\n",
    "    chapters = re.split(chap_pattern, text)\n",
    "    for i,chapter in enumerate(chapters):\n",
    "        try:\n",
    "            act_txts.append('CHAPTER' + chapter.split('CHAPTER')[1])\n",
    "        except:\n",
    "            act_txts.append(chapter)\n",
    "        years.append(year)\n",
    "        file_names.append(file_name)\n",
    "        act_urls.append(act_url)\n",
    "\n",
    "datasource_1975_1996 = pd.DataFrame({\n",
    "    'original_text': act_txts,\n",
    "    'year': years,\n",
    "    'link':act_urls,\n",
    "    'file_name':file_names,\n",
    "    'state': 'FL'\n",
    "})\n",
    "\n",
    "# for 1997-2021\n",
    "\n",
    "driver.get('https://laws.flrules.org/node?field_list_year_nid=8184')\n",
    "\n",
    "session_id_elements = driver.find_elements(By.CSS_SELECTOR,'#edit-field-list-year-nid option')\n",
    "\n",
    "# Assuming session_id_elements is a list of elements\n",
    "session_id_dict = {}\n",
    "\n",
    "for index, element in enumerate(session_id_elements):\n",
    "    year = element.text\n",
    "    session_id = element.get_attribute(\"value\")\n",
    "    session_id_dict[index] = (year, session_id)\n",
    "\n",
    "for index, (year, session_id) in session_id_dict.items():\n",
    "    # Now you can use index, year, and session_id within your loop\n",
    "    print(f\"Index: {index}, Year: {year}, Session ID: {session_id}\")\n",
    "    if int(year) < 2022:\n",
    "        print(year)\n",
    "        session_url = 'https://laws.flrules.org/node?field_list_year_nid=' + session_id\n",
    "        driver.get(session_url)\n",
    "\n",
    "        act_elements = driver.find_elements(By.CSS_SELECTOR,'table.views-table:nth-child(1) > tbody:nth-child(3) > tr')\n",
    "\n",
    "        for index, element in enumerate(act_elements):\n",
    "            #print(element.text)\n",
    "            if 'Ch' in element.text:\n",
    "                print(element.text)\n",
    "\n",
    "                act_url = element.find_element(By.PARTIAL_LINK_TEXT,'.pdf').get_attribute(\"href\")\n",
    "                act_urls.append(act_url)\n",
    "                act_name = element.find_element(By.CSS_SELECTOR,':nth-child(3)').text\n",
    "                act_names.append(act_name)\n",
    "                bill_num = element.find_element(By.CSS_SELECTOR,':nth-child(4)').text.replace(' ','')\n",
    "                bill_nums.append(bill_num)\n",
    "\n",
    "                years.append(year)\n",
    "\n",
    "                # download corresponding pdf file\n",
    "                name =  element.find_element(By.CSS_SELECTOR,':nth-child(1)').text\n",
    "                file_names.append(name)\n",
    "                print(name)\n",
    "\n",
    "                path = download_dir + name\n",
    "                print(os.path.abspath(path))\n",
    "\n",
    "                time.sleep(randint(1, 100) * 0.005)\n",
    "\n",
    "                response = requests.get(act_url)\n",
    "                # Ensure the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Write the content of the response to a file\n",
    "                    with open(path, 'wb') as file:\n",
    "                        file.write(response.content)\n",
    "                else:\n",
    "                    print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "                    broken_act_urls.append(act_url)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "folder_path = \"./PDFs\"\n",
    "print(os.path.abspath(folder_path))\n",
    "\n",
    "files = glob.glob(os.path.join(folder_path, '*.pdf'))\n",
    "\n",
    "broken_files = []\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        doc = fitz.open(file)\n",
    "        pgtxts = []\n",
    "        for page in doc:\n",
    "            pgtxt = page.get_text()\n",
    "            pgtxts.append(pgtxt)\n",
    "            acttxt = ' '.join(pgtxts)\n",
    "        act_txts.append(acttxt)\n",
    "        doc.close()\n",
    "    except:\n",
    "        act_txts.append('NA')\n",
    "        broken_files.append(file)\n",
    "\n",
    "file_names = []\n",
    "\n",
    "for file in files:\n",
    "    file_names.append(file.split(\"./PDFs/\")[1])\n",
    "\n",
    "file_names_new = []\n",
    "\n",
    "for file in file_names:\n",
    "    # Remove 'Ch_' and '.pdf', then replace '_' with '-' if it exists\n",
    "    file = file.replace('Ch_', '').replace('.pdf', '').replace('_', '-')\n",
    "    parts = file.split('-')  # Split at '-'\n",
    "    year = parts[0]\n",
    "    number = int(parts[1])  # Convert to int to remove leading zeros\n",
    "    file_names_new.append(f\"{year}/{number}\")\n",
    "\n",
    "datasource_1997_2021_pdf = pd.DataFrame({\n",
    "    'original_text': act_txts,\n",
    "    'file_name': file_names_new\n",
    "})\n",
    "\n",
    "\n",
    "datasource_1997_2021 = pd.DataFrame({\n",
    "    'bill_num': bill_nums,\n",
    "    'link': act_urls,\n",
    "    'state': \"FL\",\n",
    "    'act_name': act_names\n",
    "})\n",
    "\n",
    "# delete the duplicates from the websites, like https://laws.flrules.org/node?field_list_year_nid=5235\n",
    "datasource_1997_2021 = datasource.drop_duplicates(keep='first')\n",
    "\n",
    "datasource_1997_2021['file_name'] = datasource_1997_2021['link'].str.split('https://laws.flrules.org/').str[1]\n",
    "\n",
    "datasource_1997_2021 = pd.merge(datasource_1997_2021_pdf, datasource_1997_2021, on='file_name', how='inner')\n",
    "\n",
    "datasource = pd.concat([datasource_1975_1996, datasource_1997_2021], ignore_index=True)\n",
    "\n",
    "datasource['year'] = datasource['file_name'].str[:4]\n",
    "\n",
    "# Filter out the rows where the specified text is found\n",
    "datasource = datasource[~datasource['original_text'].str.contains(\"\\nHouse Concurrent Resolution No.\")].reset_index(drop=True)\n",
    "\n",
    "datasource.to_excel('FL_Leginfo.xlsx')\n",
    "datasource.to_csv('FL_Leginfo.csv')\n",
    "datasource.to_pickle('FL_Leginfo.pkl')\n",
    "datasource.to_json('FL_Leginfo.json')\n",
    "# End of FL_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of CA_leginfo_Galina.py\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import openai\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import inflection\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "#import PyPDF2\n",
    "#import glob\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "# working path\n",
    "os.chdir('/Users/long/Library/CloudStorage/GoogleDrive-long0555@umn.edu/Shared drives/UCSC-UMN AFRI project/Data/Raw_Data/CA/')\n",
    "os.chdir('/Users/long/Documents/AFRI_AI/Inputs')\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "driver_path = '/Users/long/Documents/AFRI_AI/chromedriver.exe'\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "dnldpath = {\"download.default_directory\" :\"/Users/long/Documents/AFRI_AI/Inputs\"}\n",
    "chromeOptions.add_experimental_option(\"prefs\",dnldpath)\n",
    "chromeOptions.add_argument('--no-sandbox')\n",
    "chromeOptions.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome(executable_path=driver_path, options=chromeOptions)\n",
    "\n",
    "\n",
    "acts_Galina = pd.read_excel(\"StateLawsCA_by year_by category.xls\")\n",
    "\n",
    "\n",
    "\n",
    "# set up list titles\n",
    "years = []\n",
    "states = []\n",
    "sessionyears = []\n",
    "billnumbers = []\n",
    "titles = []\n",
    "briefsummarys = []\n",
    "introduceddates = []\n",
    "signingdates = []\n",
    "effectivedates = []\n",
    "expireddates = []\n",
    "leadauthors = []\n",
    "textlinks = []\n",
    "fulltexts = []\n",
    "fulltextpages = []\n",
    "\n",
    "urls = acts_Galina[\"Link to full text\"]\n",
    "\n",
    "# extract bill information on each bill\n",
    "\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "    state = \"CA\"\n",
    "    states.append(state)\n",
    "\n",
    "    billnumber = driver.find_element(By.CSS_SELECTOR,'#bill_header > div:nth-child(3) > h1')\n",
    "    billnumbers.append(billnumber.text.rsplit(\" \")[0])\n",
    "\n",
    "    title = driver.find_element(By.CSS_SELECTOR,'#bill_header > div:nth-child(3) > h1')\n",
    "    title = title.text.split(\" \",1)[1]\n",
    "    titles.append((title.rsplit(\"(\",1)[0]))\n",
    "\n",
    "    textlinks.append(url)\n",
    "    fulltext = driver.find_element(By.CSS_SELECTOR,\"#bill_all\").text\n",
    "    fulltexts.append(fulltext)\n",
    "    fulltextpage = driver.find_element(By.CSS_SELECTOR,\"#centercolumn\").text\n",
    "    fulltextpages.append(fulltextpage)\n",
    "    sleeptime = randint(1,11)*0.01\n",
    "    time.sleep(sleeptime)\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'State': states,\n",
    "    'Bill Number': billnumbers,\n",
    "    'Name': titles,\n",
    "    'Full text': fulltexts,\n",
    "    'Link to full text':textlinks\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# Python program to read\n",
    "# json file\n",
    "\n",
    "\n",
    "\n",
    "# Opening JSON file\n",
    "\n",
    "os.chdir('/Volumes/GoogleDrive/Shared drives/UCSC-UMN AFRI project/Data/CA/')\n",
    "f = open('CA_Leginfo.json')\n",
    "# os.chdir('/Volumes/GoogleDrive/Shared drives/UCSC-UMN AFRI project/Data/GA')\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "\n",
    "#datasource = json.load(f)\n",
    "\n",
    "#datasource = pd.read_pickle(\"CA_Leginfo.pkl\")\n",
    "#datasource = datasource.drop(['authors','Introduced Date','Date effective','Expiration date'], axis = 1)\n",
    "\n",
    "# save bill info into files\n",
    "datasource.to_excel('CA_Leginfo_Galina.xlsx')\n",
    "datasource.to_csv('CA_Leginfo_Galina.csv')\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite://', echo=False)\n",
    "datasource.to_sql('CA_Leginfo.sql',con=engine)\n",
    "datasource.to_pickle('CA_Leginfo.pkl')\n",
    "datasource.to_json('CA_Leginfo.json')\n",
    "\n",
    "\n",
    "datasource.to_stata('CA_Leginfo.dta')\n",
    "\n",
    "print(\"download finished\")\n",
    "# End of CA_leginfo_Galina.py\n",
    "\n",
    "\n",
    "# Start of NASS.py\n",
    "\n",
    "# import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "#import PyPDF2\n",
    "#import glob\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "driver_path = 'C:/Users/longy/OneDrive/Projects/AFRI/windows/chromedriver.exe' #for windows\n",
    "# driver_path = '/Users/Yanxu/OneDrive/Projects/AgPolicyCoding/chromedriver'\n",
    "# driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "\n",
    "# Change the working directory\n",
    "#os.chdir('/Users/long/OneDrive/Projects/AFRI')\n",
    "os.chdir('C:/Users/longy/OneDrive/Projects/AFRI')\n",
    "# os.chdir('/Volumes/GoogleDrive/Shared drives/UCSC-UMN AFRI project/Data/GA')\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "# for mac\n",
    "#dnldpath = {\"download.default_directory\" : \"/Users/long/OneDrive/Projects/AFRI/data/SD/\"}\n",
    "# for windows\n",
    "dnldpath = {\"download.default_directory\" :\"C:/Users/longy/OneDrive/Projects/AFRI/data/CA/\"}\n",
    "chromeOptions.add_experimental_option(\"prefs\",dnldpath)\n",
    "chromeOptions.add_argument('--no-sandbox')\n",
    "chromeOptions.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome(executable_path=driver_path, options=chromeOptions)\n",
    "\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "\n",
    "\n",
    "sessions = ['2021 - 2022', '2019 - 2020', '2017 - 2018', '2015 - 2016', '2013 - 2014', '2011 - 2012', '2009 - 2010', '2007 - 2008', '2005 - 2006', '2003 - 2004', '2001 - 2002', '1999 - 2000']\n",
    "\n",
    "urls = []\n",
    "measures = []\n",
    "subjects = []\n",
    "authors = []\n",
    "statuss = []\n",
    "subjects1 = []\n",
    "authors1 = []\n",
    "statuss1 = []\n",
    "# search bills by iterating sessionyears first and then keywords\n",
    "for session in sessions:\n",
    "    print(session)\n",
    "    driver.get(\"https://leginfo.legislature.ca.gov/faces/billSearchClient.xhtml\")\n",
    "    WebDriverWait(driver, 300).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"#attrSearch\"))\n",
    "    )\n",
    "    select_session = Select(driver.find_element_by_css_selector('select#session_year'))\n",
    "    select_session.select_by_visible_text(session)\n",
    "    search = driver.find_element_by_css_selector('#attrSearch')\n",
    "    search.click()\n",
    "\n",
    "    WebDriverWait(driver, 300).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"tbody tr\")))\n",
    "    bills = driver.find_elements_by_css_selector('tbody tr')\n",
    "    for bill in bills:\n",
    "        if bill.find_element_by_css_selector(\"td:nth-child(4)\").text == 'Chaptered':\n",
    "            url = bill.find_element_by_css_selector(\"td:nth-child(1) a\").get_attribute('href')\n",
    "            urls.append(url)\n",
    "            measure = bill.find_element_by_css_selector(\"td:nth-child(1) a\").text\n",
    "            measures.append(measure)\n",
    "            subject = bill.find_element_by_css_selector(\"td:nth-child(2)\").text\n",
    "            subjects.append(subject)\n",
    "            author = bill.find_element_by_css_selector(\"td:nth-child(3)\").text\n",
    "            authors.append(author)\n",
    "            status = bill.find_element_by_css_selector(\"td:nth-child(4)\").text\n",
    "            statuss.append(status)\n",
    "        else:\n",
    "            print(\"Do not pass\")\n",
    "        time.sleep(0.01)\n",
    "print(\"url done\")\n",
    "\n",
    "# set up list titles\n",
    "years = []\n",
    "states = []\n",
    "sessionyears = []\n",
    "billnumbers = []\n",
    "titles = []\n",
    "briefsummarys = []\n",
    "introduceddates = []\n",
    "signingdates = []\n",
    "effectivedates = []\n",
    "expireddates = []\n",
    "leadauthors = []\n",
    "textlinks = []\n",
    "fulltexts = []\n",
    "fulltextpages = []\n",
    "\n",
    "# extract bill information on each bill\n",
    "\n",
    "for url in urls[6313:]:\n",
    "    driver.get(url)\n",
    "    state = \"CA\"\n",
    "    states.append(state)\n",
    "\n",
    "    billnumber = driver.find_element_by_css_selector('#bill_header > div:nth-child(3) > h1')\n",
    "    billnumbers.append(billnumber.text.rsplit(\" \")[0])\n",
    "\n",
    "    title = driver.find_element_by_css_selector('#bill_header > div:nth-child(3) > h1')\n",
    "    title = title.text.split(\" \",1)[1]\n",
    "    titles.append((title.rsplit(\"(\",1)[0]))\n",
    "\n",
    "    textlinks.append(url)\n",
    "    fulltext = driver.find_element_by_css_selector(\"#bill_all\").text\n",
    "    fulltexts.append(fulltext)\n",
    "    fulltextpage = driver.find_element_by_css_selector(\"#centercolumn\").text\n",
    "    fulltextpages.append(fulltextpage)\n",
    "    sleeptime = randint(1,11)*0.1\n",
    "    time.sleep(sleeptime)\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'Year': years,\n",
    "    'State': states,\n",
    "    'Session Year': sessionyears,\n",
    "    'Bill Number': billnumbers,\n",
    "    'Name': titles,\n",
    "    'Brief Summary': briefsummarys,\n",
    "    'Introduced Date': introduceddates,\n",
    "    'Date it was signed':signingdates,\n",
    "    'Date effective':effectivedates,\n",
    "    'Expiration date':expireddates,\n",
    "    'Introducer':leadauthors,\n",
    "    'Full text': fulltexts,\n",
    "    'Link to full text':textlinks\n",
    "})\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'Year': years,\n",
    "    'State': states,\n",
    "    'Session Year': sessionyears,\n",
    "    'Bill Number': billnumbers,\n",
    "    'Name': titles,\n",
    "    'authors':authors,\n",
    "    'Brief Summary': briefsummarys,\n",
    "    'Introduced Date': introduceddates,\n",
    "    'Date effective':effectivedates,\n",
    "    'Expiration date':expireddates,\n",
    "    'Full text': fulltexts,\n",
    "    'Link to full text':textlinks\n",
    "})\n",
    "\n",
    "\n",
    "# Python program to read\n",
    "# json file\n",
    "\n",
    "\n",
    "\n",
    "# Opening JSON file\n",
    "\n",
    "os.chdir('/Volumes/GoogleDrive/Shared drives/UCSC-UMN AFRI project/Data/CA/')\n",
    "f = open('CA_Leginfo.json')\n",
    "# os.chdir('/Volumes/GoogleDrive/Shared drives/UCSC-UMN AFRI project/Data/GA')\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "\n",
    "#datasource = json.load(f)\n",
    "\n",
    "#datasource = pd.read_pickle(\"CA_Leginfo.pkl\")\n",
    "#datasource = datasource.drop(['authors','Introduced Date','Date effective','Expiration date'], axis = 1)\n",
    "\n",
    "# save bill info into files\n",
    "datasource.to_excel('CA_Leginfo.xlsx')\n",
    "datasource.to_csv('CA_Leginfo.csv')\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite://', echo=False)\n",
    "datasource.to_sql('CA_Leginfo.sql',con=engine)\n",
    "datasource.to_pickle('CA_Leginfo.pkl')\n",
    "datasource.to_json('CA_Leginfo.json')\n",
    "\n",
    "\n",
    "datasource.to_stata('CA_Leginfo.dta')\n",
    "\n",
    "print(\"download finished\")\n",
    "# End of NASS.py\n",
    "\n",
    "\n",
    "# Start of CA_leginfo_v3.py\n",
    "\n",
    "# import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "#import PyPDF2\n",
    "#import glob\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "driver_path = 'C:/Users/longy/OneDrive/Projects/AFRI/windows/chromedriver.exe' #for windows\n",
    "# driver_path = '/Users/Yanxu/OneDrive/Projects/AgPolicyCoding/chromedriver'\n",
    "# driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "\n",
    "# Change the working directory\n",
    "#os.chdir('/Users/long/OneDrive/Projects/AFRI')\n",
    "os.chdir('C:/Users/longy/OneDrive/Projects/AFRI')\n",
    "# os.chdir('/Volumes/GoogleDrive/Shared drives/UCSC-UMN AFRI project/Data/GA')\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "# for mac\n",
    "#dnldpath = {\"download.default_directory\" : \"/Users/long/OneDrive/Projects/AFRI/data/SD/\"}\n",
    "# for windows\n",
    "dnldpath = {\"download.default_directory\" :\"C:/Users/longy/OneDrive/Projects/AFRI/data/CA/\"}\n",
    "chromeOptions.add_experimental_option(\"prefs\",dnldpath)\n",
    "chromeOptions.add_argument('--no-sandbox')\n",
    "chromeOptions.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome(executable_path=driver_path, options=chromeOptions)\n",
    "\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "\n",
    "\n",
    "sessions = ['2021 - 2022', '2019 - 2020', '2017 - 2018', '2015 - 2016', '2013 - 2014', '2011 - 2012', '2009 - 2010', '2007 - 2008', '2005 - 2006', '2003 - 2004', '2001 - 2002', '1999 - 2000']\n",
    "\n",
    "urls = []\n",
    "measures = []\n",
    "subjects = []\n",
    "authors = []\n",
    "statuss = []\n",
    "subjects1 = []\n",
    "authors1 = []\n",
    "statuss1 = []\n",
    "# search bills by iterating sessionyears first and then keywords\n",
    "for session in sessions:\n",
    "    print(session)\n",
    "    driver.get(\"https://leginfo.legislature.ca.gov/faces/billSearchClient.xhtml\")\n",
    "    WebDriverWait(driver, 300).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"#attrSearch\"))\n",
    "    )\n",
    "    select_session = Select(driver.find_element_by_css_selector('select#session_year'))\n",
    "    select_session.select_by_visible_text(session)\n",
    "    search = driver.find_element_by_css_selector('#attrSearch')\n",
    "    search.click()\n",
    "\n",
    "    WebDriverWait(driver, 300).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"tbody tr\")))\n",
    "    bills = driver.find_elements_by_css_selector('tbody tr')\n",
    "    for bill in bills:\n",
    "        if bill.find_element_by_css_selector(\"td:nth-child(4)\").text == 'Chaptered':\n",
    "            url = bill.find_element_by_css_selector(\"td:nth-child(1) a\").get_attribute('href')\n",
    "            urls.append(url)\n",
    "            measure = bill.find_element_by_css_selector(\"td:nth-child(1) a\").text\n",
    "            measures.append(measure)\n",
    "            subject = bill.find_element_by_css_selector(\"td:nth-child(2)\").text\n",
    "            subjects.append(subject)\n",
    "            author = bill.find_element_by_css_selector(\"td:nth-child(3)\").text\n",
    "            authors.append(author)\n",
    "            status = bill.find_element_by_css_selector(\"td:nth-child(4)\").text\n",
    "            statuss.append(status)\n",
    "        else:\n",
    "            print(\"Do not pass\")\n",
    "        time.sleep(0.01)\n",
    "print(\"url done\")\n",
    "\n",
    "# set up list titles\n",
    "years = []\n",
    "states = []\n",
    "sessionyears = []\n",
    "billnumbers = []\n",
    "titles = []\n",
    "briefsummarys = []\n",
    "introduceddates = []\n",
    "signingdates = []\n",
    "effectivedates = []\n",
    "expireddates = []\n",
    "leadauthors = []\n",
    "textlinks = []\n",
    "fulltexts = []\n",
    "fulltextpages = []\n",
    "\n",
    "# extract bill information on each bill\n",
    "\n",
    "for url in urls[6313:]:\n",
    "    driver.get(url)\n",
    "    state = \"CA\"\n",
    "    states.append(state)\n",
    "\n",
    "    billnumber = driver.find_element_by_css_selector('#bill_header > div:nth-child(3) > h1')\n",
    "    billnumbers.append(billnumber.text.rsplit(\" \")[0])\n",
    "\n",
    "    title = driver.find_element_by_css_selector('#bill_header > div:nth-child(3) > h1')\n",
    "    title = title.text.split(\" \",1)[1]\n",
    "    titles.append((title.rsplit(\"(\",1)[0]))\n",
    "\n",
    "    textlinks.append(url)\n",
    "    fulltext = driver.find_element_by_css_selector(\"#bill_all\").text\n",
    "    fulltexts.append(fulltext)\n",
    "    fulltextpage = driver.find_element_by_css_selector(\"#centercolumn\").text\n",
    "    fulltextpages.append(fulltextpage)\n",
    "    sleeptime = randint(1,11)*0.1\n",
    "    time.sleep(sleeptime)\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'Year': years,\n",
    "    'State': states,\n",
    "    'Session Year': sessionyears,\n",
    "    'Bill Number': billnumbers,\n",
    "    'Name': titles,\n",
    "    'Brief Summary': briefsummarys,\n",
    "    'Introduced Date': introduceddates,\n",
    "    'Date it was signed':signingdates,\n",
    "    'Date effective':effectivedates,\n",
    "    'Expiration date':expireddates,\n",
    "    'Introducer':leadauthors,\n",
    "    'Full text': fulltexts,\n",
    "    'Link to full text':textlinks\n",
    "})\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'Year': years,\n",
    "    'State': states,\n",
    "    'Session Year': sessionyears,\n",
    "    'Bill Number': billnumbers,\n",
    "    'Name': titles,\n",
    "    'authors':authors,\n",
    "    'Brief Summary': briefsummarys,\n",
    "    'Introduced Date': introduceddates,\n",
    "    'Date effective':effectivedates,\n",
    "    'Expiration date':expireddates,\n",
    "    'Full text': fulltexts,\n",
    "    'Link to full text':textlinks\n",
    "})\n",
    "\n",
    "\n",
    "# Python program to read\n",
    "# json file\n",
    "\n",
    "\n",
    "\n",
    "# Opening JSON file\n",
    "\n",
    "os.chdir('/Volumes/GoogleDrive/Shared drives/UCSC-UMN AFRI project/Data/CA/')\n",
    "f = open('CA_Leginfo.json')\n",
    "# os.chdir('/Volumes/GoogleDrive/Shared drives/UCSC-UMN AFRI project/Data/GA')\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "\n",
    "#datasource = json.load(f)\n",
    "\n",
    "#datasource = pd.read_pickle(\"CA_Leginfo.pkl\")\n",
    "#datasource = datasource.drop(['authors','Introduced Date','Date effective','Expiration date'], axis = 1)\n",
    "\n",
    "# save bill info into files\n",
    "datasource.to_excel('CA_Leginfo.xlsx')\n",
    "datasource.to_csv('CA_Leginfo.csv')\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite://', echo=False)\n",
    "datasource.to_sql('CA_Leginfo.sql',con=engine)\n",
    "datasource.to_pickle('CA_Leginfo.pkl')\n",
    "datasource.to_json('CA_Leginfo.json')\n",
    "\n",
    "\n",
    "datasource.to_stata('CA_Leginfo.dta')\n",
    "\n",
    "print(\"download finished\")\n",
    "# End of CA_leginfo_v3.py\n",
    "\n",
    "\n",
    "# Start of NM_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "## selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "## others\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import inflect\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import fitz\n",
    "import re\n",
    "\n",
    "# set up working directory\n",
    "working_directory = '/Volumes/SSD/AFRI/'\n",
    "print(os.path.abspath(working_directory))\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"./Data/Raw_Data/NM/PDFs\"\n",
    "\n",
    "print(os.path.abspath(download_dir))\n",
    "\n",
    "\n",
    "\n",
    "# Specify the path to the Firefox binary\n",
    "firefox_binary_path = '/Applications/Firefox.app/Contents/MacOS/firefox'\n",
    "\n",
    "# Create a Firefox profile and configure preferences\n",
    "options = Options()\n",
    "options.binary_location = firefox_binary_path\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path = './Data/geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "# Initialize the WebDriver with the specified path\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "\n",
    "driver.get('https://www.nmlegis.gov/Legislation/BillFinder/Chaptered')\n",
    "\n",
    "session_info_elements = driver.find_elements(By.CSS_SELECTOR, '#MainContent_gridViewLegislation > tbody > tr:')\n",
    "\n",
    "sessions_info = []\n",
    "\n",
    "for session_element in session_info_elements:\n",
    "    sessions_info.append(session_element.text)\n",
    "\n",
    "sessions_info[1]\n",
    "\n",
    "bill_nums = []\n",
    "\n",
    "bill_elements = driver.find_elements(By.CSS_SELECTOR, '#MainContent_gridViewLegislation > tbody > tr > td:nth-child(1)')\n",
    "\n",
    "for element in bill_elements:\n",
    "    bill_nums.append(element.text)\n",
    "\n",
    "len(bill_nums)\n",
    "\n",
    "#MainContent_gridViewLegislation_lblSession_0\n",
    "#MainContent_gridViewLegislation > tbody:nth-child(2) > tr:nth-child(1) > td:nth-child(5)\n",
    "#MainContent_gridViewLegislation > tbody:nth-child(2) > tr:nth-child(1) > td:nth-child(5)\n",
    "sessions = []\n",
    "\n",
    "session_elements = driver.find_elements(By.CSS_SELECTOR, '#MainContent_gridViewLegislation > tbody > tr > td:nth-child(5)')\n",
    "\n",
    "for session_element in session_elements:\n",
    "    print(session_element.text)\n",
    "    sessions.append(session_element.text)\n",
    "\n",
    "len(sessions)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'bill_num': bill_nums,\n",
    "    'session': sessions\n",
    "})\n",
    "\n",
    "df.to_csv(\"/Volumes/SSD/AFRI/Data/Raw_Data/NM/NM_leginfo_re.csv\")\n",
    "\n",
    "df = pd.read_csv(\"/Volumes/SSD/AFRI/Data/Raw_Data/NM/NM_leginfo_re.csv\")\n",
    "df = df.dropna(subset='session')\n",
    "df.head\n",
    "urls = []\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    year = int(row['session'][:4])\n",
    "    session_part = row['session'][2:].replace('1st','').replace('2nd','')\n",
    "    bill_num = row['bill_num'].replace('*', '')\n",
    "    type = bill_num[:2]\n",
    "    bill_part = bill_num.split(' ')[1].zfill(4)\n",
    "    if year > 2003:\n",
    "        # Construct the URL based on conditions\n",
    "        urls.append(f'https://www.nmlegis.gov/Sessions/{session_part}/final/{type}{bill_part}.pdf')\n",
    "        urls.append(f'https://www.nmlegis.gov/Sessions/{session_part}2/final/{type}{bill_part}.pdf')\n",
    "    else:\n",
    "        urls.append(f'https://www.nmlegis.gov/Sessions/{session_part}/FinalVersions/{type}{bill_part}.pdf')\n",
    "\n",
    "for url in urls:\n",
    "    print(url)\n",
    "\n",
    "# Specify the path to the Firefox binary\n",
    "firefox_binary_path = '/Applications/Firefox.app/Contents/MacOS/firefox'\n",
    "\n",
    "# Create a Firefox profile and configure preferences\n",
    "options = Options()\n",
    "options.binary_location = firefox_binary_path\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path = './geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "# Initialize the WebDriver with the specified path\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "\n",
    "for url in urls:\n",
    "    print(f\"Processing URL: {url}\")\n",
    "    time.sleep(randint(1,5)*0.001)\n",
    "    # Navigate to the URL using JavaScript\n",
    "    driver.execute_script(f\"window.location.href = '{url}'\")\n",
    "    driver.execute_script(\"window.location.reload();\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# End of NM_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of ML_test.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "import re\n",
    "\n",
    "\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 28 * 28))\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.encoder(x)\n",
    "        return embedding\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "\n",
    "# data\n",
    "dataset = MNIST('', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_train, mnist_val = random_split(dataset, [55000, 5000])\n",
    "\n",
    "train_loader = DataLoader(mnist_train, batch_size=32)\n",
    "val_loader = DataLoader(mnist_val, batch_size=32)\n",
    "\n",
    "# model\n",
    "model = LitAutoEncoder()\n",
    "\n",
    "# training\n",
    "trainer = pl.Trainer(gpus=4, num_nodes=8, precision=16, limit_train_batches=0.5)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "\n",
    "# End of ML_test.py\n",
    "\n",
    "\n",
    "# Start of OR_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import pytest\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# driver_path = 'C:/Program Files (x86)/Google/Chrome/Application/chromedriver.exe' #for windows\n",
    "# driver_path = '/Users/Yanxu/OneDrive/Projects/AgPolicyCoding/chromedriver'\n",
    "# driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir('/Users/long/OneDrive/Projects/AFRI')\n",
    "# os.chdir('/Volumes/GoogleDrive/Shared drives/UCSC-UMN AFRI project/Data/GA')\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "dnldpath = {\"download.default_directory\" : \"/Users/long/OneDrive/Projects/AFRI/data/OR/\"}\n",
    "chromeOptions.add_experimental_option(\"prefs\",dnldpath)\n",
    "driver = webdriver.Chrome(executable_path='./chromedriver', options=chromeOptions)\n",
    "driver.get(\"https://www.oregonlegislature.gov/bills_laws/Pages/Oregon-Laws.aspx\")\n",
    "\n",
    "\n",
    "urls=[]\n",
    "acttxts = []\n",
    "\n",
    "sessionlist = driver.find_elements_by_css_selector('span.ms-commentexpand-iconouter')\n",
    "\n",
    "for session in sessionlist:\n",
    "    session.click()\n",
    "    sleep(1)\n",
    "\n",
    "chapters = driver.find_elements_by_css_selector('td.ms-cellstyle.ms-vb2 a')\n",
    "for chapter in chapters:\n",
    "    url = chapter.get_attribute('href')\n",
    "    urls.append(url)\n",
    "\n",
    "urlslist = list(set(urls))\n",
    "\n",
    "urlslist.sort()\n",
    "\n",
    "for idx,url in enumerate(urlslist):\n",
    "    driver.get(url)\n",
    "    sleep(1)\n",
    "    if '.pdf' in url:\n",
    "        path = \"/Users/long/OneDrive/Projects/AFRI/data/OR/\"\n",
    "        list_of_files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "        latest_file = max(list_of_files, key=os.path.getctime)\n",
    "        print(latest_file)\n",
    "        with open(os.path.join(os.getcwd(), latest_file), 'r') as f:  # open in readonly mode\n",
    "            # creating a pdf File object of original pdf\n",
    "            pdfFileObj = open(latest_file, 'rb')\n",
    "            pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "            pagenumber = pdfReader.numPages\n",
    "            acttxt = []\n",
    "            pgtxts = []\n",
    "            for p in range(pagenumber):\n",
    "                pageObj = pdfReader.getPage(p)\n",
    "                pgtxt = pageObj.extractText()\n",
    "                pgtxts.append(pgtxt)\n",
    "                acttxt = '\\n'.join(pgtxts)\n",
    "    else:\n",
    "        html = requests.get(url)\n",
    "        soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "        tag = soup.body\n",
    "        acttxt = []\n",
    "        for string in tag.strings:\n",
    "            acttxt.append(string)\n",
    "    acttxts.append(acttxt)\n",
    "datasource = pd.DataFrame({\n",
    "    'Session Laws': acttxts,\n",
    "    'Link to full text': urls,\n",
    "})\n",
    "\n",
    "datasource.to_excel('OR_Leginfo.xlsx')\n",
    "datasource.to_csv('OR_Leginfo.csv')\n",
    "\n",
    "\n",
    "print(\"Web-scrapting finished\")\n",
    "# End of OR_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of CA_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "#import calendar\n",
    "import os\n",
    "\n",
    "#driver_path = 'C:/Program Files (x86)/Google/Chrome/Application/chromedriver.exe' #for windows\n",
    "#driver_path = '/Users/Yanxu/OneDrive/Projects/AgPolicyCoding/chromedriver'\n",
    "#driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "#Change the working directory\n",
    "os.chdir('/Users/long/OneDrive/Projects/AFRI')\n",
    "#Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "\n",
    "# set up webdriver by short path\n",
    "#driver = webdriver.Chrome('/Users/Yanxu/OneDrive/Projects/AgPolicyCoding/chromedriver')\n",
    "driver = webdriver.Chrome('./chromedriver')\n",
    "\n",
    "# input keyword(s)\n",
    "keywords = ['agriculture']\n",
    "\n",
    "# choose sessionyears\n",
    "#input_sessionyears = ['2021 - 2022']\n",
    "input_sessionyears = ['2015 - 2016', '2013 - 2014', '2011 - 2012', '2009 - 2010', '2007 - 2008']\n",
    "#input_sessionyears = ['2015 - 2016', '2013 - 2014', '2011 - 2012', '2009 - 2010', '2007 - 2008']\n",
    "#input_sessionyears = ['2021 - 2022', '2019 - 2020', '2017 - 2018', '2015 - 2016', '2013 - 2014', '2011 - 2012', '2009 - 2010', '2007 - 2008', '2005 - 2006', '2003 - 2004', '2001 - 2002', '1999 - 2000']\n",
    "\n",
    "\n",
    "urls = []\n",
    "# search bills by iterating sessionyears first and then keywords\n",
    "for input_sessionyear in input_sessionyears:\n",
    "    for keyword in keywords:\n",
    "        driver.get(\"https://leginfo.legislature.ca.gov/faces/billSearchClient.xhtml\")\n",
    "\n",
    "        # select session year\n",
    "        select_sessionyear = Select(driver.find_element_by_css_selector('select#session_year'))\n",
    "        select_sessionyear.select_by_visible_text(input_sessionyear)\n",
    "\n",
    "        # search by keywords\n",
    "        input_keywords = driver.find_element_by_css_selector(\"input#keyword.keyword_text\")\n",
    "        input_keywords.send_keys(keyword)\n",
    "        search_button = driver.find_element_by_css_selector('input#attrSearch')\n",
    "        search_button.click()\n",
    "\n",
    "        no_bills_returned = int(driver.find_element_by_css_selector(\"#text_bill_returned\").text.split(\":\")[1].split(\"Bills\")[0])\n",
    "\n",
    "        if no_bills_returned >0:\n",
    "            # get urls in the first page\n",
    "            searchresult = driver.find_elements_by_css_selector('div#searchresult tbody a')\n",
    "            basicinfo = driver.find_elements_by_css_selector('div.commdataRow')\n",
    "\n",
    "            for i in range(len(searchresult)):\n",
    "                #for i in range(10):\n",
    "                url = searchresult[i].get_attribute('href')\n",
    "                urls.append(url)\n",
    "\n",
    "            #%%\n",
    "            # get urls in the middle pages\n",
    "            text_bill_returned= driver.find_element_by_css_selector('div#text_bill_returned').text\n",
    "            n_pages = int(text_bill_returned.rsplit('of ')[1].rsplit(\" \")[0])\n",
    "            page_index = int(text_bill_returned.rsplit('Page ')[1].rsplit(' of')[0])\n",
    "            while page_index < n_pages:\n",
    "                WebDriverWait(driver, 300).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"input#nextTen\"))\n",
    "                )\n",
    "                next_page = driver.find_element_by_css_selector('input#nextTen')\n",
    "                next_page.click()\n",
    "                WebDriverWait(driver, 300).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"div#searchresult tbody a\"))\n",
    "                )\n",
    "                searchresult = driver.find_elements_by_css_selector('div#searchresult tbody a')\n",
    "                basicinfo = driver.find_elements_by_css_selector('div.commdataRow')\n",
    "                for i in range(len(searchresult)):\n",
    "                    url = searchresult[i].get_attribute('href')\n",
    "                    urls.append(url)\n",
    "                WebDriverWait(driver, 300).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"div#text_bill_returned\"))\n",
    "                )\n",
    "                text_bill_returned = driver.find_element_by_css_selector('div#text_bill_returned').text\n",
    "                page_index = int(text_bill_returned.rsplit('Page ')[1].rsplit(' of')[0])\n",
    "                sleep(3)\n",
    "print(\"done with urls\")\n",
    "\n",
    "\n",
    "# set up list titles\n",
    "years = []\n",
    "states = []\n",
    "sessionyears = []\n",
    "billnumbers = []\n",
    "titles = []\n",
    "briefsummarys = []\n",
    "introduceddates = []\n",
    "signingdates = []\n",
    "effectivedates = []\n",
    "expireddates = []\n",
    "leadauthors = []\n",
    "textlinks = []\n",
    "\n",
    "\n",
    "# extract bill information on each bill\n",
    "\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "    version = driver.find_element_by_css_selector(\"select#version.bill_version_select\")\n",
    "\n",
    "    if \"Chaptered\" in version.text:\n",
    "        introduceddate = version.text.split(\"- Introduced\")[0]\n",
    "        introduceddate = introduceddate.split(\" \")[-2]\n",
    "        introduceddates.append(introduceddate)\n",
    "\n",
    "        state = \"CA\"\n",
    "        states.append(state)\n",
    "\n",
    "        billnumber = driver.find_element_by_css_selector('div#bill_title')\n",
    "        billnumbers.append(billnumber.text.rsplit(\" \")[0])\n",
    "\n",
    "        title = driver.find_element_by_css_selector('div#bill_title h2')\n",
    "        title = title.text.split(\" \",1)[1]\n",
    "        titles.append((title.rsplit(\"(\",1)[0]))\n",
    "\n",
    "        try:\n",
    "            briefsummaryresolution = driver.find_element_by_css_selector('span.Resolution').text\n",
    "        except:\n",
    "            briefsummaryresolution = \"N/A\"\n",
    "\n",
    "        briefsummarydigest = driver.find_element_by_css_selector('span#digesttext').text\n",
    "\n",
    "        if not briefsummarydigest:\n",
    "            briefsummary = briefsummaryresolution\n",
    "        else:\n",
    "            briefsummary = briefsummarydigest\n",
    "\n",
    "        briefsummarys.append(briefsummary)\n",
    "\n",
    "        if not version.text.find(\"Chaptered\"):\n",
    "            signingdate = 'N/A'\n",
    "        else:\n",
    "            signingdate = version.text.split(\"- Chaptered\")[0].split(\" \")[-2]\n",
    "\n",
    "        signingdates.append(signingdate)\n",
    "\n",
    "        signingdate_str = str(signingdate)\n",
    "        signingdate_length = len(signingdate_str)\n",
    "        shortyear = int(str(signingdate)[signingdate_length-2: signingdate_length])\n",
    "        if shortyear > 22:\n",
    "            f\"{shortyear:02}\"\n",
    "        else:\n",
    "            if shortyear < 10:\n",
    "                year = \"200\" + str(shortyear)\n",
    "            else:\n",
    "                year = \"20\" + str(shortyear)\n",
    "        years.append(year)\n",
    "\n",
    "        if (int(year) % 2) == 0:\n",
    "            sessionyear = str(int(year)-1)+\"-\"+str(year)\n",
    "        else:\n",
    "            sessionyear = str(year)+str(\"-\")+str(int(year)+1)\n",
    "        sessionyears.append(sessionyear)\n",
    "\n",
    "        if \"take effect immediately\" in briefsummary:\n",
    "            effectivedate = signingdate\n",
    "        else:\n",
    "            if \"take effect on or before\" in briefsummary:\n",
    "                effectivedate = briefsummary.split(\"take effect on or before\")[1].split(\".\")[0]\n",
    "            elif \"take effect on\" in briefsummary:\n",
    "                effectivedate = briefsummary.split(\"take effect on\")[1].split(\".\")[0]\n",
    "            else:\n",
    "                effectivedate = \"Need to check\"\n",
    "\n",
    "        effectivedates.append(effectivedate)\n",
    "\n",
    "        if datefinder.find_dates(briefsummary):\n",
    "            dates = datefinder.find_dates(briefsummary)\n",
    "            datelist = []\n",
    "            try:\n",
    "                for date in dates:\n",
    "                    datelist.append(date.date())\n",
    "                exdtkeys = ['expired', 'repeal', 'extend', 'repealed', 'repeals']\n",
    "                if any(key in briefsummary for key in exdtkeys):\n",
    "                    expireddate = max(datelist)\n",
    "                else:\n",
    "                    expireddate = 'N/A'\n",
    "            except:\n",
    "                expireddate = 'Need to check'\n",
    "        else:\n",
    "            expireddate = 'N/A'\n",
    "\n",
    "        expireddates.append(expireddate)\n",
    "\n",
    "        driver.find_element_by_css_selector(\"#nav_bar_top_status > span\").click()\n",
    "        leadauthor = driver.find_element_by_css_selector('#leadAuthors').text\n",
    "        #try:\n",
    "        #    leadauthor = driver.find_element_by_css_selector('#leadAuthors').text.replace(\" (A)\",\"\")\n",
    "        #except:\n",
    "        #    leadauthor = driver.find_element_by_css_selector('#leadAuthors').text.replace(\" (S)\",\"\")\n",
    "        leadauthors.append(leadauthor)\n",
    "\n",
    "        textlinks.append(url)\n",
    "        sleep(5)\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'Year': years,\n",
    "    'State': states,\n",
    "    'Session Year': sessionyears,\n",
    "    'Bill Number': billnumbers,\n",
    "    'Name': titles,\n",
    "    'Brief Summary': briefsummarys,\n",
    "    'Introduced Date': introduceddates,\n",
    "    'Date it was signed':signingdates,\n",
    "    'Date effective':effectivedates,\n",
    "    'Expiration date':expireddates,\n",
    "    'Introducer':leadauthors,\n",
    "    'Link to full text':textlinks\n",
    "})\n",
    "\n",
    "# drop duplicates by \"Official ID\"\n",
    "datasource.drop_duplicates(subset = ['Bill Number', 'Year'],\n",
    "                           keep = 'first', inplace = True, ignore_index= True)\n",
    "\n",
    "# save bill info into excel file\n",
    "datasource.to_excel('CA_Leginfo_food.xlsx')\n",
    "print(\"download finished\")\n",
    "# End of CA_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of NASS_data.py\n",
    "\n",
    "# import libraries\n",
    "## selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "## others\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import inflect\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up working directory\n",
    "working_directory = '/Volumes/SSD/AFRI/Data/'\n",
    "print(os.path.abspath(working_directory))\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"./NASS_Commodities\"\n",
    "\n",
    "print(os.path.abspath(download_dir))\n",
    "\n",
    "\n",
    "# Specify the path to the Firefox binary\n",
    "firefox_binary_path = '/Applications/Firefox.app/Contents/MacOS/firefox'\n",
    "\n",
    "# Create a Firefox profile and configure preferences\n",
    "options = Options()\n",
    "options.binary_location = firefox_binary_path\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path = './geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "# Initialize the WebDriver with the specified path\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "\n",
    "\n",
    "driver.get('https://quickstats.nass.usda.gov/')\n",
    "\n",
    "states = [\n",
    "    'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA',\n",
    "    'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD',\n",
    "    'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ',\n",
    "    'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC',\n",
    "    'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY'\n",
    "]\n",
    "\n",
    "commodities = []\n",
    "\n",
    "commodity_elements = driver.find_elements(By.CSS_SELECTOR, '#commodity_desc > option')\n",
    "\n",
    "for element in commodity_elements:\n",
    "    commodities.append(element.text)\n",
    "\n",
    "df = pd.DataFrame(commodities, columns=['Commodity'])\n",
    "\n",
    "df.to_csv('NASS_commodities.csv', index=False)\n",
    "\n",
    "# Replace 'your_api_key' with your actual QuickStats API key\n",
    "api_key = '07D7A7C0-032B-3C59-BCD4-3ECB111B0BE6'\n",
    "url = 'https://quickstats.nass.usda.gov/api/api_GET/'\n",
    "\n",
    "commodity_list = []\n",
    "state_list = []\n",
    "download_status = []\n",
    "\n",
    "#index = commodities.index('CRUSTACEANS')\n",
    "\n",
    "for commodity in commodities:\n",
    "    commodity_list.append(commodity)\n",
    "    # Loop over states\n",
    "    for state in states:\n",
    "        state_list.append(state)\n",
    "        # Example parameters for the API request\n",
    "        params = {\n",
    "            'key': api_key,\n",
    "            'commodity_desc': commodity,\n",
    "            #'year': '2022',\n",
    "            'state_alpha': state,\n",
    "            'format': 'CSV'  # Request data in CSV format\n",
    "        }\n",
    "\n",
    "        # Make the API request\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Specify the path to save the CSV file\n",
    "            csv_file_path = os.path.join(download_dir, f'quickstats_data_{commodity.replace(\" \", \"_\").lower()}_{state.lower()}.csv')\n",
    "\n",
    "            # Save the CSV data to a file\n",
    "            with open(csv_file_path, mode='wb') as file:\n",
    "                file.write(response.content)\n",
    "\n",
    "            print(f\"Data for {commodity} in {state} saved to {csv_file_path}\")\n",
    "            download_status.append('Yes')\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for {commodity} in {state}. Status code: {response.status_code}\")\n",
    "            download_status.append('No')\n",
    "\n",
    "\n",
    "# List all CSV files in the directory\n",
    "files = [file for file in os.listdir(download_dir) if file.endswith('.csv') and not file.startswith('._')]\n",
    "\n",
    "# Initialize an empty list to hold DataFrames\n",
    "data_frames = []\n",
    "\n",
    "# Specify the dtype for problematic columns (replace 'object' with appropriate dtype if known)\n",
    "dtype_spec = {\n",
    "    18: 'object',\n",
    "    21: 'object',\n",
    "    35: 'object',\n",
    "    38: 'object'\n",
    "}\n",
    "\n",
    "# Loop through the list of CSV files and read each into a DataFrame, then append to the list\n",
    "for file in files:\n",
    "    file_path = os.path.join(download_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    data_frames.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "appended_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "appended_df.to_csv(\"./NASS_Commodities/NASS_commodities_data.csv\", index=False)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./NASS_Commodities/NASS_commodities_data.csv\")\n",
    "\n",
    "df = df[(df['agg_level_desc'] == 'STATE') & (df['freq_desc'] == 'ANNUAL')]\n",
    "\n",
    "df = df.loc[:, ~(df == 'nan').all()]\n",
    "\n",
    "df['Value'] = df['Value'].fillna('')\n",
    "\n",
    "# Filter out rows where 'Value' contains '(D)'\n",
    "df = df[~df['Value'].str.contains(r'\\(D\\)')]\n",
    "\n",
    "df = df[df['sector_desc'].isin(['ANIMALS & PRODUCTS', 'CROPS'])]\n",
    "\n",
    "commodities_list = df[df['sector_desc'].isin(['ANIMALS & PRODUCTS', 'CROPS'])]\n",
    "commodities_list = commodities_list['commodity_desc'].drop_duplicates().tolist()\n",
    "commodities_list = [commodity.replace(' TOTALS', '') for commodity in commodities_list]\n",
    "\n",
    "commodities_list = list(set(commodities_list))\n",
    "\n",
    "commodities_list = pd.DataFrame(commodities_list)\n",
    "\n",
    "commodities_dict_list = []\n",
    "\n",
    "# Define the list to hold the dictionary values\n",
    "commodity_dicts = []\n",
    "\n",
    "\n",
    "# Initialize an inflect engine\n",
    "p = inflect.engine()\n",
    "\n",
    "# Iterate over each commodity description in the DataFrame\n",
    "for commodity in df['commodity_desc']:\n",
    "    # Convert the commodity description to lowercase\n",
    "    commodity_lower = commodity.lower()\n",
    "\n",
    "    # Check if the commodity is in singular form\n",
    "    if p.singular_noun(commodity_lower):\n",
    "        # If it's plural, we generate singular form and use the original as plural form\n",
    "        original_commodity = commodity_lower\n",
    "        revised_commodity = commodity_lower.replace(', other', '')\n",
    "        plural_commodity = commodity_lower\n",
    "        singular_commodity = p.singular_noun(commodity_lower)\n",
    "    else:\n",
    "        # If it's singular, we generate plural form and use the original as singular form\n",
    "        original_commodity = commodity_lower\n",
    "        revised_commodity = commodity_lower.replace(', other', '')\n",
    "        singular_commodity = commodity_lower\n",
    "        plural_commodity = p.plural(commodity_lower)\n",
    "\n",
    "\n",
    "    # Perform transformations and store results in a list\n",
    "    commodity_forms = [\n",
    "        commodity_lower,  # original_commodity\n",
    "        commodity_lower.replace(', other', ''),  # revised_commodity\n",
    "        commodity_lower,  # singular_commodity\n",
    "        p.plural(commodity_lower)  # plural_commodity\n",
    "    ]\n",
    "\n",
    "    # Create a dictionary with the original and generated forms\n",
    "    #commodity_dict = {\n",
    "    #    'original': original_commodity,\n",
    "    #    'singular': singular_commodity,\n",
    "    #    'plural': plural_commodity,\n",
    "    #    'revised': revised_commodity\n",
    "    #}\n",
    "\n",
    "    # Append the dictionary to the list\n",
    "    commodity_dicts.append(commodity_dict)\n",
    "\n",
    "# Create the new column in the DataFrame\n",
    "df['commodity_forms'] = commodity_dicts\n",
    "\n",
    "df.to_csv('NASS_commodities_animal_crop.csv', index=False)\n",
    "\n",
    "df = pd.read_csv(\"./NASS_Commodities/NASS_commodities_animal_crop.csv\")\n",
    "\n",
    "# Group by 'STATE' and save each group to a separate CSV file\n",
    "for state, group in df.groupby('state_alpha'):\n",
    "    filename = f\"./NASS_Commodities/{state}_commodity_price.csv\"\n",
    "    group.to_csv(filename, index=False)\n",
    "    print(f\"Saved {filename}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over each commodity description in commodities_list\n",
    "for commodity in df['commodity_desc']:\n",
    "    # Convert the commodity description to lowercase\n",
    "    commodity_lower = commodity.lower()\n",
    "\n",
    "    # Check if the commodity is in singular form\n",
    "    if p.singular_noun(commodity_lower):\n",
    "        # Generate plural form\n",
    "        original_commodity = commodity_lower\n",
    "        revised_commodity = commodity_lower.replace(', other', '')\n",
    "        plural_commodity = p.plural(commodity_lower)\n",
    "        singular_commodity = commodity_lower\n",
    "    else:\n",
    "        # Generate singular form\n",
    "        original_commodity = commodity_lower\n",
    "        revised_commodity = commodity_lower.replace(', other', '')\n",
    "        singular_commodity = p.singular_noun(commodity_lower)\n",
    "        plural_commodity = commodity_lower\n",
    "\n",
    "    # Create a dictionary with the original and generated forms\n",
    "    commodity_dict = {'original':commodity_lower, 'singular': singular_commodity, 'plural': plural_commodity, 'revised': revised_commodity}\n",
    "\n",
    "    # Append the dictionary to the list\n",
    "    commodities_dict_list.append(commodity_dict)\n",
    "\n",
    "commodities_list['commodities_dict'] = commodities_dict_list\n",
    "\n",
    "\n",
    "# Convert commodities_list to DataFrame with a column name 'commodity_desc'\n",
    "commodities_list_df = pd.DataFrame({'commodity_desc': commodities_list})\n",
    "\n",
    "# Combine both DataFrames horizontally\n",
    "commodities_df = pd.concat([commodities_dict_df, commodities_list_df], axis=1)\n",
    "\n",
    "\n",
    "df.to_csv(\"NASS_commodities_state_annual.csv\", index=False)\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "df.columns()\n",
    "\n",
    "print(df.columns.tolist())\n",
    "\n",
    "\n",
    "# List of columns to keep\n",
    "columns_to_keep = ['source_desc', 'sector_desc', 'group_desc', 'commodity_desc']\n",
    "\n",
    "# Keep only the specified columns using filter\n",
    "df_filtered = df.filter(items=columns_to_keep)\n",
    "\n",
    "df_filtered.to_csv(\"NASS_commodities_list.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('/Volumes/SSD/AFRI/Data/NASS_Commodities/combined_data/NASS_commodities_state.csv')\n",
    "\n",
    "\n",
    "\n",
    "df = df.drop_duplicates(subset=['commodity_desc', 'class_desc', 'year', 'state_alpha'])\n",
    "\n",
    "\n",
    "df = df[df['class_desc'] != 'ALL CLASSES']\n",
    "\n",
    "df_country = df.drop_duplicates(subset=['commodity_desc', 'class_desc', 'year'])\n",
    "\n",
    "df_country.to_csv('us_commodity_classes.csv', index=False)\n",
    "\n",
    "df_apple = df_country[df_country['commodity_desc'] == 'APPLES']\n",
    "df_apple = df[df['commodity_desc'] == 'APPLES']\n",
    "\n",
    "df_apple.to_excel('apple.xlsx', index=False)\n",
    "\n",
    "df_wheat = df[df['commodity_desc'] == 'WHEAT']\n",
    "\n",
    "df_wheat.to_excel('us_commodity_classes_state.xlsx', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for state in states:\n",
    "    # Define the parameters for the API request\n",
    "    params = {\n",
    "        'key': api_key,\n",
    "        'commodity_desc': 'APPLES',  # Request data for APPLES\n",
    "        #'year': '2022',\n",
    "        'state_alpha': state,\n",
    "        'format': 'CSV'  # Request data in CSV format\n",
    "    }\n",
    "\n",
    "    # Define the URL for the API request\n",
    "    url = 'https://quickstats.nass.usda.gov/api/api_GET/'  # Replace with the actual API endpoint URL\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Specify the path to save the CSV file\n",
    "        csv_file_path = os.path.join(f'apples_{state.lower()}.csv')\n",
    "\n",
    "        # Save the CSV data to a file\n",
    "        with open(csv_file_path, mode='wb') as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "        print(f\"Data in {state} saved to {csv_file_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for {commodity} in {state}. Status code: {response.status_code}\")\n",
    "\n",
    "data_frames = []\n",
    "\n",
    "download_dir = '/Volumes/SSD/AFRI/Data/apple'\n",
    "\n",
    "# List all CSV files in the directory\n",
    "files = [file for file in os.listdir(download_dir) if file.endswith('.csv') and not file.startswith('._')]\n",
    "\n",
    "\n",
    "# Loop through the list of CSV files and read each into a DataFrame, then append to the list\n",
    "for file in files:\n",
    "    file_path = os.path.join(download_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    data_frames.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "apple = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "apple.to_excel('apple.xlsx', index=False)\n",
    "\n",
    "apple_country = apple.drop_duplicates(subset=['commodity_desc', 'class_desc', 'year'])\n",
    "apple_country.to_excel('apple.xlsx', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('/Volumes/SSD/AFRI/Data/NASS_Commodities/combined_data/NASS_commodities_state.csv')\n",
    "\n",
    "df_survey = df[df['source_desc'] == 'SURVEY' | df['source_desc'] == 'CROPS']\n",
    "\n",
    "df_survey = df_survey[df_survey['freq_desc'] == 'ANNUAL']\n",
    "\n",
    "df_survey = df_survey[df_survey['year'] != 2024]\n",
    "\n",
    "df_survey.to_csv('NASS_commodities_state_survey.csv')\n",
    "\n",
    "df_survey_us =  df_survey.drop_duplicates(subset=['commodity_desc', 'class_desc', 'year'])\n",
    "\n",
    "df_survey_us.to_excel('NASS_commodities_survey_US.xlsx', index=False)\n",
    "\n",
    "# Step 1: Group by 'commodity_desc' and 'year' and count unique 'class_desc'\n",
    "grouped = df_survey_us.groupby(['commodity_desc', 'year'])['class_desc'].nunique().reset_index(name='unique_class_count')\n",
    "\n",
    "# Step 2: Identify groups with more than 1 unique 'class_desc'\n",
    "groups_with_multiple_classes = grouped[grouped['unique_class_count'] > 2]\n",
    "\n",
    "# Step 3: Filter out 'ALL CLASSES' rows for these identified groups\n",
    "# Merge to find the rows that should be filtered out\n",
    "result_df = df_survey_us.merge(groups_with_multiple_classes[['commodity_desc', 'year']], on=['commodity_desc', 'year'], how='left', indicator=True)\n",
    "\n",
    "# Filter out rows where 'ALL CLASSES' should be removed\n",
    "df_survey_us_filtered = result_df[~((result_df['_merge'] == 'both') & (result_df['class_desc'] == 'ALL CLASSES'))].drop(columns=['_merge'])\n",
    "\n",
    "\n",
    "df_survey_us_filtered.to_excel('NASS_commodities_US_filtered.xlsx')\n",
    "\n",
    "# Group by 'year' and 'commodity_desc', then count the unique 'class_desc'\n",
    "results_table = df_survey_us_filtered.groupby(['year', 'commodity_desc'])['class_desc'].nunique().reset_index()\n",
    "\n",
    "# Rename the 'class_desc' column to 'count_class_desc' to reflect the counts\n",
    "results_table = results_table.rename(columns={'class_desc': 'count_class_desc'})\n",
    "\n",
    "results_table.to_csv('NASS_commodities_US_varieties.csv')\n",
    "results_table.to_excel('NASS_commodities_US_varieties.xlsx', index=False)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(60, 40))\n",
    "\n",
    "# Iterate over unique 'commodity_desc' to plot each as a separate line\n",
    "for commodity in results_table['commodity_desc'].unique():\n",
    "    subset = results_table[results_table['commodity_desc'] == commodity]\n",
    "    ax.plot(subset['year'], subset['count_class_desc'], marker='o', label=commodity)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Count of Unique Class Desc')\n",
    "ax.set_title('Number of Unique Class Descriptions for Each Commodity by Year')\n",
    "ax.legend(title='Commodity')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "df_census = df[df['source_desc'] == 'CENSUS']\n",
    "\n",
    "df_survey = df[(df['sector_desc'] == 'ANIMALS & PRODUCTS') | (df['sector_desc'] == 'CROPS')]\n",
    "\n",
    "df_census = df_census[df_census['freq_desc'] == 'ANNUAL']\n",
    "\n",
    "df_census = df_census[df_census['year'] != 2024]\n",
    "\n",
    "df_census.to_csv('NASS_commodities_state_census.csv')\n",
    "\n",
    "df_census_us =  df_census.drop_duplicates(subset=['commodity_desc', 'class_desc', 'year'])\n",
    "\n",
    "df_census_us.to_excel('NASS_commodities_census_US.xlsx', index=False)\n",
    "\n",
    "# Step 1: Group by 'commodity_desc' and 'year' and count unique 'class_desc'\n",
    "grouped = df_census_us.groupby(['commodity_desc', 'year'])['class_desc'].nunique().reset_index(name='unique_class_count')\n",
    "\n",
    "# Step 2: Identify groups with more than 1 unique 'class_desc'\n",
    "groups_with_multiple_classes = grouped[grouped['unique_class_count'] > 2]\n",
    "\n",
    "# Step 3: Filter out 'ALL CLASSES' rows for these identified groups\n",
    "# Merge to find the rows that should be filtered out\n",
    "result_df = df_census_us.merge(groups_with_multiple_classes[['commodity_desc', 'year']], on=['commodity_desc', 'year'], how='left', indicator=True)\n",
    "\n",
    "# Filter out rows where 'ALL CLASSES' should be removed\n",
    "df_census_us_filtered = result_df[~((result_df['_merge'] == 'both') & (result_df['class_desc'] == 'ALL CLASSES'))].drop(columns=['_merge'])\n",
    "\n",
    "\n",
    "df_census_us_filtered.to_excel('NASS_commodities_US_census_filtered.xlsx')\n",
    "\n",
    "# Group by 'year' and 'commodity_desc', then count the unique 'class_desc'\n",
    "results_table = df_census_us_filtered.groupby(['year', 'commodity_desc'])['class_desc'].nunique().reset_index()\n",
    "\n",
    "# Rename the 'class_desc' column to 'count_class_desc' to reflect the counts\n",
    "results_table = results_table.rename(columns={'class_desc': 'count_class_desc'})\n",
    "\n",
    "results_table.to_csv('NASS_commodities_US_varieties_census.csv')\n",
    "results_table.to_excel('NASS_commodities_US_varieties_census.xlsx', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# End of NASS_data.py\n",
    "\n",
    "\n",
    "# Start of NE_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from word_forms.word_forms import get_word_forms\n",
    "import datefinder\n",
    "from dateutil import parser\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "## others\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import inflect\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import fitz\n",
    "\n",
    "\n",
    "# Specify the path to the Firefox binary\n",
    "firefox_binary_path = '/Applications/Firefox.app/Contents/MacOS/firefox'\n",
    "\n",
    "# Create a Firefox profile and configure preferences\n",
    "options = Options()\n",
    "options.binary_location = firefox_binary_path\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path = './geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "# Initialize the WebDriver with the specified path\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "urls=[]\n",
    "sessionurls = []\n",
    "section_urls = []\n",
    "chs = list(range(1,91))\n",
    "for ch in chs:\n",
    "    ch = str(ch)\n",
    "    ch_url = \"https://nebraskalegislature.gov/laws/browse-chapters.php?chapter=\" + ch\n",
    "    driver.get(ch_url)\n",
    "    sections = driver.find_elements_by_css_selector(\"span.col-md-2.col-sm-3.my-auto:nth-child(1) a\")\n",
    "    for section in sections:\n",
    "        section_url = section.get_attribute('href')\n",
    "        section_urls.append(section_url)\n",
    "\n",
    "sections = []\n",
    "bill_urls = []\n",
    "for section_url in section_urls:\n",
    "    driver.get(section_url)\n",
    "    section_text = driver.find_element_by_css_selector(\"div.card-body\").text\n",
    "    sections.append(section_text)\n",
    "    try:\n",
    "        bills = driver.find_elements_by_partial_link_text(\"Laws \")\n",
    "        for bill in bills:\n",
    "            bill_url = bill.get_attribute('href')\n",
    "            bill_urls.append(bill_url)\n",
    "    except:\n",
    "        print(\"no source\")\n",
    "print(\"done with bill urls\")\n",
    "\n",
    "billurls = list(set(bill_urls))\n",
    "\n",
    "\n",
    "for url in billurls:\n",
    "    print(f\"Processing URL: {url}\")\n",
    "\n",
    "    # Navigate to the URL using JavaScript\n",
    "    driver.execute_script(f\"window.location.href = '{url}'\")\n",
    "    wait_for_page_load(driver)  # Wait for the page to load\n",
    "\n",
    "    time.sleep(randint(1, 5) * 0.01)  # Additional sleep to simulate user delay\n",
    "\n",
    "    try:\n",
    "        # Refresh the page\n",
    "        driver.execute_script(\"window.location.reload();\")\n",
    "        wait_for_page_load(driver)  # Wait again after refresh\n",
    "        time.sleep(randint(1, 5) * 0.01)  # Wait to ensure the page is properly loaded after refresh\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while refreshing the page: {e}\")\n",
    "\n",
    "act_txts = []\n",
    "urls = []\n",
    "path = f\"Raw_Data/{state}/PDFs\"\n",
    "files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "for file in files:\n",
    "    print(file)\n",
    "    urls.append(get_mdls_where_from(file))\n",
    "    doc = fitz.open(file)\n",
    "    pgtxts = []\n",
    "    for page in doc:\n",
    "        pgtxt = page.get_text()\n",
    "        pgtxts.append(pgtxt)\n",
    "        act_txt = ' '.join(pgtxts)\n",
    "    act_txts.append(act_txt)\n",
    "    doc.close()\n",
    "\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'original_text': acttxts,\n",
    "    'link':billurls\n",
    "})\n",
    "\n",
    "datasource['session'] = datasource['link'].apply(lambda x: x.split('FloorDocs/')[1].split('/PDF')[0])\n",
    "\n",
    "datasource['year'] = datasource['original_text'].apply(lambda x: x.split('AN\\nACT')[0].split('BILL')[1].replace('\\n', ' ').split('Introduced')[0].replace('(corrected)','').replace('(CORRECTED)','').replace('Corrected Copy','').split('the ')[1][-8:].replace(' ','').replace('.','')[-4:])\n",
    "\n",
    "datasource['year'][1645] = '2001'\n",
    "datasource['year'][707] = '2000'\n",
    "datasource['year'][762] = '2017'\n",
    "\n",
    "datasource['year'].astype(int)\n",
    "\n",
    "datasource['year'] = datasource['year'].astype(int)\n",
    "\n",
    "datasource = datasource[(datasource['year'] >= 1975) & (datasource['year'] <= 2021)]\n",
    "\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "datasource.to_excel(f\"{state}_Leginfo.xlsx\", index=False)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "datasource.to_csv(f'{state}_Leginfo.csv', index=False)\n",
    "\n",
    "# Save the DataFrame to a Pickle file\n",
    "datasource.to_pickle(f'{state}_Leginfo.pkl')\n",
    "\n",
    "# Save the DataFrame to a JSON file\n",
    "datasource.to_json(f'{state}_Leginfo.json', orient='records')\n",
    "\n",
    "# End of NE_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of KS_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "## selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "## others\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import fitz\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# set up working directory\n",
    "working_directory = '/Volumes/SSD/AFRI/Data/Raw_Data/KS'\n",
    "print(os.path.abspath(working_directory))\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"./PDFs/\"\n",
    "print(os.path.abspath(download_dir))\n",
    "\n",
    "# Create a Firefox profile and configure preferences\n",
    "options = Options()\n",
    "\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path='../../geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service = service)\n",
    "\n",
    "years = []\n",
    "act_urls = []\n",
    "acttxts = []\n",
    "bad_act_urls = []\n",
    "\n",
    "for year in range(1996,2011):\n",
    "    session_url = \"https://kslegislature.org/historical_data/sessionlaws/\" + str(year) + \"/\"\n",
    "    driver.get(session_url)\n",
    "    time.sleep(0.001)\n",
    "\n",
    "    if year < 2004:\n",
    "        acts = driver.find_elements(By.PARTIAL_LINK_TEXT, \"html\")\n",
    "    else:\n",
    "        acts = driver.find_elements(By.PARTIAL_LINK_TEXT, \"pdf\")\n",
    "\n",
    "    for act in acts:\n",
    "        years.append(year)\n",
    "        act_url = act.get_attribute('href')\n",
    "        act_urls.append(act_url)\n",
    "        name = str(act_url).replace(\"/\", \"\").split(\"sessionlaws\")[1]\n",
    "        path = download_dir + name\n",
    "        print(print(os.path.abspath(path)))\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(act_url)\n",
    "        # Ensure the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Write the content of the response to a file\n",
    "            with open(path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "            bad_act_urls.append(act_url)\n",
    "\n",
    "bill_urls_2011_2022 = []\n",
    "for session_url in [\"http://www.kslegislature.org/li_2022/b2021_22/measures/bills/\",\n",
    "                    \"http://www.kslegislature.org/li_2021s/b2021s/measures/bills/\",\n",
    "                    \"http://www.kslegislature.org/li_2020s/b2020s/measures/bills/\",\n",
    "                    \"http://www.kslegislature.org/li_2020/b2019_20/measures/bills/\",\n",
    "                    \"http://www.kslegislature.org/li_2018/b2017_18/measures/bills/\",\n",
    "                    \"http://www.kslegislature.org/li_2016s/b2015_16/measures/bills/\",\n",
    "                    \"http://www.kslegislature.org/li_2016/b2015_16/measures/bills/\",\n",
    "                    \"http://www.kslegislature.org/li_2014/b2013_14/measures/bills/\",\n",
    "                    \"http://www.kslegislature.org/li_2013s/b2013_14/measures/bills/\",\n",
    "                    \"http://www.kslegislature.org/li_2012/b2011_12/measures/bills/\"]:\n",
    "    print(session_url)\n",
    "    driver.get(session_url)\n",
    "    time.sleep(1)\n",
    "\n",
    "    try:\n",
    "        page_no = driver.find_element(By.CSS_SELECTOR,\"#tab-disp\").text.split(\"of \")[1]\n",
    "        \"for more than 1 pages of bills\"\n",
    "        for page in range(1,int(page_no)+1):\n",
    "            print(page)\n",
    "            page_url = session_url + \"#\" + str(page)\n",
    "            #print(page_url)\n",
    "            driver.get(page_url)\n",
    "            time.sleep(0.01)\n",
    "            WebDriverWait(driver, 60).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"#bill-tab-1\"))\n",
    "            )\n",
    "            bills = driver.find_elements(By.CSS_SELECTOR, f\"#bill-tab-{page} li a\")\n",
    "\n",
    "            for bill in bills:\n",
    "                bill_url = bill.get_attribute(\"href\")\n",
    "                bill_urls_2011_2022.append(bill_url)\n",
    "    except:\n",
    "        bills = driver.find_elements(By.CSS_SELECTOR, \"#bill-tab-1 > li a\")\n",
    "        for bill in bills:\n",
    "            bill_url = bill.get_attribute(\"href\")\n",
    "            bill_urls_2011_2022.append(bill_url)\n",
    "\n",
    "\n",
    "not_found_urls = []\n",
    "\n",
    "for url in bill_urls_2011_2022:\n",
    "    print(url)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        version = driver.find_element(By.CSS_SELECTOR, \"#version-tab-1 > tr\").text\n",
    "\n",
    "        if \"Enrolled\" in version:\n",
    "            act_url = driver.find_element(By.CSS_SELECTOR,\n",
    "                                          \"#version-tab-1 > tr:nth-child(1) > td:nth-child(2) > a\").get_attribute('href')\n",
    "\n",
    "            act_urls.append(act_url)\n",
    "\n",
    "            # get the act year\n",
    "            try:\n",
    "                year = driver.find_element(By.CSS_SELECTOR,\n",
    "                                           \"#history-tab-1 > tr:nth-child(1) > td:nth-child(1)\").text[-4:]\n",
    "            except:\n",
    "                year = driver.find_element(By.CSS_SELECTOR,\n",
    "                                           \"#version-tab-1 > tr:nth-child(1) > td:nth-child(1)\").text[-4:]\n",
    "            years.append(year)\n",
    "\n",
    "            name = year + str(act_url).replace(\"/\", \"\").split(\"documents\")[1]\n",
    "            path = download_dir + name\n",
    "            #print(os.path.abspath(path))\n",
    "            # Send a GET request to the URL\n",
    "            response = requests.get(act_url)\n",
    "            # Ensure the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Write the content of the response to a file\n",
    "                with open(path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "            else:\n",
    "                print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "                bad_act_urls.append(act_url)\n",
    "        else:\n",
    "            pass\n",
    "    except:\n",
    "        not_found_urls.append(url)\n",
    "# missing url\n",
    "# http://www.kslegislature.org/li_2014/b2013_14/measures/hb2070/\n",
    "\n",
    "# Get index of the element\n",
    "#index = bill_urls_2011_2022.index(\"http://www.kslegislature.org/li_2014/b2013_14/measures/hb2070/\")\n",
    "\n",
    "act_txts = []\n",
    "file_names = []\n",
    "\n",
    "badfiles = []\n",
    "\n",
    "folder_path = \"./PDFs\"\n",
    "print(os.path.abspath(folder_path))\n",
    "\n",
    "files = glob.glob(os.path.join(folder_path, '*'))\n",
    "\n",
    "for file in files:\n",
    "    file_names.append(file.replace(\"./PDFs/\", \"\")[4:].replace(\".pdf\",\"\").replace(\".html\",\"\"))\n",
    "    doc = fitz.open(file)\n",
    "    pgtxts = []\n",
    "    for page in doc:\n",
    "        pgtxt = page.get_text()\n",
    "        pgtxts.append(pgtxt)\n",
    "        acttxt = ' '.join(pgtxts)\n",
    "    act_txts.append(acttxt)\n",
    "    doc.close()\n",
    "\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'original_text': act_txts,\n",
    "    'year': years,\n",
    "    'state': \"KS\",\n",
    "    'file_name': file_names,\n",
    "    'link':act_urls\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "datasource.to_excel('KS_Leginfo.xlsx')\n",
    "datasource.to_csv('KS_Leginfo.csv')\n",
    "datasource.to_pickle('KS_Leginfo.pkl')\n",
    "datasource.to_json('KS_Leginfo.json')\n",
    "\n",
    "\n",
    "\n",
    "# End of KS_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of NV_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "## selenium\n",
    "import selenium\n",
    "print(selenium.__version__)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.support.ui import Select\n",
    "## others\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "import requests\n",
    "import fitz\n",
    "import re\n",
    "\n",
    "# set up working directory\n",
    "working_directory = '/Volumes/SSD/AFRI/Data/Raw_Data/NV'\n",
    "print(os.path.abspath(working_directory))\n",
    "\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"./PDFs/\"\n",
    "print(os.path.abspath(download_dir))\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path = '../../geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service = service)\n",
    "\n",
    "# empty list to store info\n",
    "\n",
    "act_pdf_urls = []\n",
    "act_html_urls = []\n",
    "bill_nums = []\n",
    "chapter_nums = []\n",
    "years = []\n",
    "sessions = []\n",
    "broken_act_urls = []\n",
    "act_txts = []\n",
    "act_names = []\n",
    "names = []\n",
    "file_names = []\n",
    "\n",
    "\n",
    "# New Jersey Legislature Website: https://www.njleg.state.nj.us/chapter-laws\n",
    "\n",
    "driver.get('https://www.leg.state.nv.us/law1.html')\n",
    "\n",
    "WebDriverWait(driver, 60).until(\n",
    "    EC.element_to_be_clickable((By.CSS_SELECTOR, 'div.col-md-2 > div'))\n",
    ")\n",
    "\n",
    "divs = driver.find_elements(By.CSS_SELECTOR,'div.col-md-2 > div')\n",
    "session_elements = []\n",
    "session_names = []\n",
    "for div in divs:\n",
    "    match = re.search(r\"Session \\((\\d{4})\\)\", div.text)\n",
    "    if match:\n",
    "        year = int(match.group(1))\n",
    "        if year > 1974 and year < 2022:\n",
    "            session_elements.append(div)\n",
    "            session_names.append(div.text.split('Bills')[0])\n",
    "\n",
    "session_content_list_urls = []\n",
    "for session in session_elements:\n",
    "    session_content_list_urls.append(session.find_element(By.PARTIAL_LINK_TEXT, 'Bills by Chapter Number').get_attribute('href'))\n",
    "\n",
    "\n",
    "datasource_session = pd.DataFrame({\n",
    "    'session_name':session_names,\n",
    "    'session_content_list_url': session_content_list_urls\n",
    "})\n",
    "\n",
    "\n",
    "session_content_lists = []\n",
    "act_html_urls = []\n",
    "session_nums = []\n",
    "\n",
    "for index,row in datasource_session.iterrows():\n",
    "    session_num = row['session_name']\n",
    "    print(session_num)\n",
    "    url = row['session_content_list_url']\n",
    "    print(url)\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, 'body'))\n",
    "    )\n",
    "    session_content_list = driver.find_element(By.CSS_SELECTOR,'body').text\n",
    "\n",
    "    rows = driver.find_elements(By.CSS_SELECTOR,'.MsoNormalTable > tbody:nth-child(1) > tr')[2:]\n",
    "    for index, row in enumerate(rows):\n",
    "        #print(row.text)\n",
    "        try:\n",
    "            page = int(row.find_element(By.CSS_SELECTOR,':nth-child(3)').text)\n",
    "            act_html_url = driver.find_element(By.LINK_TEXT, f'{page}').get_attribute('href')\n",
    "            act_html_urls.append(act_html_url)\n",
    "        except:\n",
    "            act_html_urls.append('NA')\n",
    "        session_content_lists.append(session_content_list)\n",
    "        session_nums.append(session_num)\n",
    "\n",
    "datasource_session_content = pd.DataFrame({\n",
    "    'act_html_url':act_html_urls,\n",
    "    'session_num':session_nums,\n",
    "    'session_content_list': session_content_lists\n",
    "})\n",
    "\n",
    "datasource_session_content = datasource_session_content[~(datasource_session_content['act_html_url'] == 'NA')]\n",
    "\n",
    "session_txts = []\n",
    "act_html_urls = []\n",
    "for index,row in datasource_session_content.iterrows():\n",
    "    url = row['act_html_url']\n",
    "    act_html_urls.append(url)\n",
    "    print(url)\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, 'body'))\n",
    "    )\n",
    "    session_txts.append(driver.find_element(By.CSS_SELECTOR,'body').text)\n",
    "\n",
    "datasource_session_content['session_txt'] = session_txts\n",
    "\n",
    "datasource_session_content = datasource_session_content.drop_duplicates(subset=['session_txt'])\n",
    "datasource_session_content = datasource_session_content.reset_index(drop=True)\n",
    "\n",
    "datasource_session_content = pd.read_csv('NV_Leginfo_sessionclean.csv',index_col=0)\n",
    "\n",
    "act_html_urls = []\n",
    "session_nums = []\n",
    "session_content_lists = []\n",
    "act_txts = []\n",
    "\n",
    "for index,row in datasource_session_content.iterrows():\n",
    "    acts = row['session_txt'].split('________')\n",
    "    for act in acts:\n",
    "        act_txts.append(act)\n",
    "        session_nums.append(row['session_num'])\n",
    "        act_html_urls.append(row['act_html_url'])\n",
    "        session_content_lists.append(row['session_content_list'])\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'act_url':act_html_urls,\n",
    "    'session_num':session_nums,\n",
    "    'session_content_list': session_content_lists,\n",
    "    'original_text':act_txts\n",
    "})\n",
    "\n",
    "datasource = datasource.drop_duplicates(subset=['original_text'])\n",
    "datasource = datasource.reset_index(drop=True)\n",
    "\n",
    "datasource['session_num'] = datasource['session_num'].str.replace('Statutes of Nevada','')\n",
    "datasource['session_num'] = datasource['session_num'].str.replace('\\n','')\n",
    "\n",
    "datasource['year'] = datasource['session_num'].str.split('Session').str[1].str[2:6]\n",
    "\n",
    "datasource['original_text'] = datasource['original_text'].str.replace('\\n \\n…………………………………………………………………………………………………………………','')\n",
    "datasource['original_text'] = datasource['original_text'].str.replace('…………………………………………………………………………………………………………………','')\n",
    "\n",
    "datasource = datasource[~datasource['original_text'].str.contains(\"SESSION OF THE LEGISLATURE\")]\n",
    "\n",
    "def keep_after_bill(text):\n",
    "    for phrase in [\"Assembly Bill No.\", \"Senate Bill No.\"]:\n",
    "        if phrase in text:\n",
    "            parts = text.split(phrase)\n",
    "            return phrase + parts[-1]  # Return the part after the phrase, including the phrase itself\n",
    "    return text  # Return the original text if none of the phrases are found\n",
    "\n",
    "# Apply the function to each row in the 'original_text' column\n",
    "datasource['original_text'] = datasource['original_text'].apply(keep_after_bill)\n",
    "\n",
    "datasource = datasource[datasource['original_text'].str.contains(\"Assembly Bill No.|Senate Bill No.\")]\n",
    "\n",
    "def extract_bill_number(text):\n",
    "    # Pattern to find \"Assembly Bill No. <number>\" or \"Senate Bill No. <number>\"\n",
    "    pattern = r'(Assembly Bill No\\.|Senate Bill No\\.) (\\d+)'\n",
    "    match = re.search(pattern, text)\n",
    "\n",
    "    if match:\n",
    "        bill_prefix = 'AB' if 'Assembly' in match.group(1) else 'SB'\n",
    "        bill_number = match.group(2)\n",
    "        return bill_prefix + bill_number\n",
    "    else:\n",
    "        return None  # or some default value if needed\n",
    "\n",
    "# Apply the function to the 'original_text' column and create a new 'bill_num' column\n",
    "datasource['bill_num'] = datasource['original_text'].apply(extract_bill_number)\n",
    "datasource = datasource.drop(columns=['session_content_list'])\n",
    "\n",
    "\n",
    "datasource.to_excel('NV_Leginfo.xlsx')\n",
    "datasource.to_csv('NV_Leginfo.csv')\n",
    "datasource.to_pickle('NV_Leginfo.pkl')\n",
    "datasource.to_json('NV_Leginfo.json')\n",
    "\n",
    "datasource = pd.read_csv('NV_Leginfo.csv',index_col=0)\n",
    "\n",
    "# End of NV_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of NH_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "## selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "## others\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# set up working directory\n",
    "working_directory = '/Volumes/SSD/AFRI/Data/Raw_Data/NH'\n",
    "print(os.path.abspath(working_directory))\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"./PDFs/\"\n",
    "print(os.path.abspath(download_dir))\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path='../../geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service = service)\n",
    "\n",
    "# session years: 1989-2021\n",
    "\n",
    "years = []\n",
    "act_urls = []\n",
    "original_act_num = []\n",
    "bad_act_urls = []\n",
    "\n",
    "for year in range(1989,2022):\n",
    "    print(year)\n",
    "    driver.get('https://www.gencourt.state.nh.us/bill_status/legacy/bs2016/')\n",
    "    # Find the input element by its css \"#txtsessionyear\"\n",
    "    sessionyear = driver.find_element(By.CSS_SELECTOR, \"input#txtsessionyear\")\n",
    "    # Clear the existing value in the input field\n",
    "    sessionyear.clear()\n",
    "\n",
    "    # Input the current year into the input field\n",
    "    sessionyear.send_keys(str(year))\n",
    "\n",
    "    driver.find_element(By.CSS_SELECTOR, \"#Table1 > tbody:nth-child(1) > tr:nth-child(13) > td:nth-child(1) > input:nth-child(1)\").click()\n",
    "\n",
    "    # click submit\n",
    "    driver.find_element(By.CSS_SELECTOR, \"#cmdsubmit\").click()\n",
    "\n",
    "    # Wait for a brief moment (you may adjust the sleep time as needed)\n",
    "    time.sleep(randint(1, 10))\n",
    "\n",
    "    WebDriverWait(driver, 300).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, '#btnsh'))\n",
    "    )\n",
    "\n",
    "    time.sleep(randint(1, 10)*0.01)\n",
    "\n",
    "    chapters = driver.find_elements(By.XPATH,\n",
    "                                    \"//*[contains(text(), 'SIGNED BY GOVERNOR') or contains(text(), 'LAW WITHOUT SIGNATURE') or contains(text(), 'VETO OVERRIDDEN')]/../../../../../..\")\n",
    "\n",
    "    print(len(chapters))\n",
    "\n",
    "    for index, chap in enumerate(chapters):\n",
    "\n",
    "        print(index)\n",
    "\n",
    "        time.sleep(randint(1, 10) * 0.01)\n",
    "\n",
    "        print(index)\n",
    "\n",
    "        original_act_num.append(chap.find_element(By.XPATH, \"./*[1]/*[1]\").text)\n",
    "\n",
    "        years.append(year)\n",
    "\n",
    "        try:\n",
    "\n",
    "            act_url = chap.find_element(By.XPATH, \"./*[1]//*[contains(text(), 'Bill Text')]\").get_attribute('href')\n",
    "\n",
    "        except:\n",
    "\n",
    "            act_url = chap.find_element(By.XPATH, \"./*[1]//*[contains(text(), 'HTML')]\").get_attribute('href')\n",
    "\n",
    "\n",
    "        act_urls.append(act_url)\n",
    "\n",
    "        name = str(datasource['year'][index]) + str(datasource['original_act_num'][index]) + \".html\"\n",
    "        path = download_dir + name\n",
    "        print(os.path.abspath(path))\n",
    "\n",
    "        time.sleep(randint(1, 100) * 0.02)\n",
    "\n",
    "        response = requests.get(act_url)\n",
    "        # Ensure the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Write the content of the response to a file\n",
    "            with open(path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "            bad_act_urls.append(act_url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculating the frequency of each element in the list\n",
    "frequency = Counter(years)\n",
    "\n",
    "# Printing the frequency of each element\n",
    "for element, count in frequency.items():\n",
    "    print(f\"Element {element} appears {count} times\")\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    #'original_text': act_txts,\n",
    "    'year': years,\n",
    "    'state': \"NH\",\n",
    "    'link':act_urls,\n",
    "    'original_act_num':original_act_num\n",
    "})\n",
    "\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    #'original_text': act_txts,\n",
    "    'name': names,\n",
    "})\n",
    "\n",
    "names = []\n",
    "\n",
    "datasource = pd.read_excel('./extra_urls.xlsx')\n",
    "\n",
    "for index, act_url in enumerate(datasource['link']):\n",
    "    print(index)\n",
    "    name = str(datasource['year'][index]) + str(datasource['original_act_num'][index]).split('-FN')[0].replace(' ','') + \".html\"\n",
    "    path = download_dir + name\n",
    "    names.append(name)\n",
    "    print(os.path.abspath(path))\n",
    "\n",
    "    driver.get(act_url)\n",
    "\n",
    "    time.sleep(randint(1, 3)*0.01)\n",
    "\n",
    "    response = requests.get(act_url)\n",
    "    # Ensure the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Write the content of the response to a file\n",
    "        with open(path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "    else:\n",
    "        print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "        bad_act_urls.append(act_url)\n",
    "\n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, 'body'))\n",
    "    )\n",
    "\n",
    "\n",
    "def read_html_files(folder_path):\n",
    "    html_files = [f for f in os.listdir(folder_path) if f.endswith('.html')]\n",
    "    extracted_info = []\n",
    "\n",
    "    for file in html_files:\n",
    "        with open(os.path.join(folder_path, file), 'r', encoding='ISO-8859-1') as f:\n",
    "            soup = BeautifulSoup(f, 'lxml')\n",
    "            # Example: Extracting all paragraphs\n",
    "            paragraphs = soup.find_all('p')\n",
    "            extracted_info.append({\n",
    "                'original_text': [p.text for p in paragraphs]\n",
    "            })\n",
    "\n",
    "    return extracted_info\n",
    "\n",
    "# Use the function\n",
    "original_text = read_html_files(download_dir)\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(download_dir):\n",
    "    if filename.endswith(\".html\"):\n",
    "        # Form the path to the HTML file\n",
    "        filepath = os.path.join(download_dir, filename)\n",
    "\n",
    "        # Open and read the HTML file\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "            html_content = file.read()\n",
    "\n",
    "        # Parse HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract content as needed\n",
    "        # Example: Extract all text content\n",
    "        text_content = soup.get_text()\n",
    "\n",
    "        # Example: Extract specific elements like <p>\n",
    "        paragraphs = soup.find_all('p')\n",
    "        for paragraph in paragraphs:\n",
    "            print(paragraph.text)\n",
    "\n",
    "\n",
    "file_name = glob.glob(os.path.join(download_dir, '*.html'))\n",
    "\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    #'original_text': act_txts,\n",
    "    'year': years,\n",
    "    'state': \"NH\",\n",
    "    'link':act_urls,\n",
    "    'original_act_num':original_act_num,\n",
    "    'file_name':names\n",
    "})\n",
    "\n",
    "datasource['file_name'] = names\n",
    "\n",
    "datasource = datasource.drop_duplicates(subset='link', keep='first')\n",
    "\n",
    "datasource.to_csv('NH_Leginfo_url.csv')\n",
    "\n",
    "datasource.to_excel('NH_Leginfo.xlsx')\n",
    "datasource.to_csv('NH_Leginfo.csv')\n",
    "datasource.to_pickle('NH_Leginfo.pkl')\n",
    "datasource.to_json('NH_Leginfo.json')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# End of NH_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of SC_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from word_forms.word_forms import get_word_forms\n",
    "import datefinder\n",
    "from dateutil import parser\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "## others\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import inflect\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import fitz\n",
    "\n",
    "# Set the working directory\n",
    "os.chdir('/Volumes/SSD/AFRI/Data/')\n",
    "\n",
    "# Verify the current working directory\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "# Specify the path to the Firefox binary\n",
    "firefox_binary_path = '/Applications/Firefox.app/Contents/MacOS/firefox'\n",
    "\n",
    "# Create a Firefox profile and configure preferences\n",
    "options = Options()\n",
    "options.binary_location = firefox_binary_path\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path = './geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "# Initialize the WebDriver with the specified path\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# driver_path = 'C:/Program Files (x86)/Google/Chrome/Application/chromedriver.exe' #for windows\n",
    "# driver_path = '/Users/Yanxu/OneDrive/Projects/AgPolicyCoding/chromedriver'\n",
    "# driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir('/Users/long/OneDrive/Projects/AFRI')\n",
    "# os.chdir('/Volumes/GoogleDrive/Shared drives/UCSC-UMN AFRI project/Data/GA')\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# chromeOptions = webdriver.ChromeOptions()\n",
    "dnldpath = {\"download.default_directory\" : \"/Users/long/OneDrive/Projects/AFRI/data/SC/\"}\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "chromeOptions.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": dnldpath, #Change default directory for downloads\n",
    "    \"download.prompt_for_download\": False, #To auto download the file\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True,#It will not show PDF directly in chrome\n",
    "    \"--enable-javascript\":True\n",
    "})\n",
    "driver = webdriver.Chrome(executable_path='./chromedriver', options = chromeOptions)\n",
    "driver.get(\"https://www.scstatehouse.gov/aacts.php\")\n",
    "\n",
    "session_urls = []\n",
    "urls =[]\n",
    "docurls = []\n",
    "acttxts = []\n",
    "titles = []\n",
    "yrs = []\n",
    "billnums= []\n",
    "billnumlist = []\n",
    "specialyears = ['1995','1996']\n",
    "sgurls = []\n",
    "summarys = []\n",
    "\n",
    "\n",
    "driver.get(\"https://www.scstatehouse.gov/aacts.php\")\n",
    "\n",
    "sessions = driver.find_elements_by_css_selector(\"div#contentsection a\")\n",
    "\n",
    "for idx, session in enumerate(sessions):\n",
    "    sessionname = session.text\n",
    "    if \"Excel\" in sessionname:\n",
    "        print(sessionname)\n",
    "    else:\n",
    "        session_url = session.get_attribute('href')\n",
    "        session_urls.append(session_url)\n",
    "for idx, session_url  in enumerate(session_urls):\n",
    "    driver.get(session_url )\n",
    "    sleep(1)\n",
    "    WebDriverWait(driver, 300).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"h2.barheader\"))\n",
    "    )\n",
    "    yr = driver.find_element_by_css_selector(\"h2.barheader\").text.rsplit(\"Acts\")[1].rsplit(\"Session\")[0].rsplit(\" \")[0]\n",
    "    if yr in specialyears:\n",
    "        sessiongroup = driver.find_elements_by_css_selector(\"div#resultsbox dl dd a\")\n",
    "        for element in sessiongroup:\n",
    "            sgurl = element.get_attribute('href')\n",
    "            driver.get(sgurl)\n",
    "            WebDriverWait(driver, 300).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div#contentsection\"))\n",
    "            )\n",
    "            lawlist = driver.find_elements_by_css_selector(\"div#contentsection dl dt a\")\n",
    "            for element in lawlist:\n",
    "                url = element.get_attribute('href')\n",
    "                urls.append(url)\n",
    "    else:\n",
    "        WebDriverWait(driver, 300).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div#resultsbox dl dd\"))\n",
    "        )\n",
    "        lawlist = driver.find_elements_by_xpath('//*[@id=\"resultsbox\"]/ dl / dt / a[1]')\n",
    "        for element in lawlist:\n",
    "            url = element.get_attribute('href')\n",
    "            urls.append(url)\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 300).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div#resultsbox\"))\n",
    "        )\n",
    "        summary = driver.find_element_by_css_selector(\"div#resultsbox\").text\n",
    "    except:\n",
    "        WebDriverWait(driver, 300).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"body\"))\n",
    "        )\n",
    "        summary = driver.find_element_by_css_selector(\"body\").text\n",
    "    # summarys\n",
    "    summarys.append(acttxt)\n",
    "    try:\n",
    "        textloc = driver.find_element_by_css_selector(\"div.bill-list-item a.nodisplay\")\n",
    "        # docurls\n",
    "        docurl = textloc.get_attribute('href')\n",
    "        docurls.append(docurl)\n",
    "        sleep(2)\n",
    "        driver.get(docurl)\n",
    "    except:\n",
    "        docurls.append(url)\n",
    "    body = driver.find_element_by_css_selector(\"body\")\n",
    "    # acttxts\n",
    "    acttxt = body.text\n",
    "    acttxts.append(acttxt)\n",
    "\n",
    "sessions1 = [i.split('Bill')[0] for i in acttxts]\n",
    "sessions2 = [i.split('Download')[0] for i in sessions1]\n",
    "sessions = [i.split('\\n')[1] for i in sessions2]\n",
    "datasource = pd.DataFrame({\n",
    "    #'Year': yrs,\n",
    "    # 'State': sts,\n",
    "    'Session Year': sessions,\n",
    "    #'Bill Number': billnums,\n",
    "    #'Title': titles,\n",
    "    #'Brief Summary': summarys,\n",
    "    # 'Introduced Date': introdts,\n",
    "    #'Date it was signed':sigdts,\n",
    "    # 'Date effective':effdts,\n",
    "    'Full Text':acttxts,\n",
    "    'Link to full text': docurls,\n",
    "    'Link to bill': urls\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "driver.get('https://www.scstatehouse.gov/listofacts.php?O=&Y=2021')\n",
    "\n",
    "url_elements = driver.find_elements(By.LINK_TEXT, 'Word')\n",
    "urls = []\n",
    "for element in url_elements:\n",
    "    element.click()\n",
    "    url = element.get_attribute('href')\n",
    "    urls.append(url)\n",
    "\n",
    "import textract\n",
    "import docx2txt\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = '/Volumes/SSD/AFRI/Data/Raw_Data/SC/2021'\n",
    "\n",
    "# Function to read text from a .docx file using textract\n",
    "# Function to read text from a .docx file using docx2txt\n",
    "def read_docx(file_path):\n",
    "    try:\n",
    "        text = docx2txt.process(file_path)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# List to store the content of all files\n",
    "SC2021 = []\n",
    "\n",
    "# Iterate over all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.docx') and not filename.startswith('._'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        file_content = read_docx(file_path)\n",
    "        SC2021.append(file_content)\n",
    "\n",
    "new_data = pd.DataFrame({\n",
    "    'Full Text': SC2021,\n",
    "    'Link to full text': urls,\n",
    "    'year': 2021\n",
    "})\n",
    "\n",
    "datasource.to_excel('SC_Leginfo.xlsx')\n",
    "datasource.to_csv('SC_Leginfo.csv')\n",
    "print(\"Web-scrapting finished\")\n",
    "# End of SC_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of MO_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "#driver_path = 'C:/Users/longy/OneDrive/Projects/AFRI/windows/chromedriver.exe' #for windows\n",
    "driver_path = '/Users/long/Library/CloudStorage/OneDrive-Personal/Projects/AFRI/chromedriver'\n",
    "# driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir('/Users/long/OneDrive/Projects/AFRI')\n",
    "#os.chdir('C:/Users/longy/OneDrive/Projects/AFRI')\n",
    "# os.chdir('/Volumes/GoogleDrive/Shared drives/UCSC-UMN AFRI project/Data/GA')\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "dnldpath = {\"download.default_directory\" : \"/Users/long/OneDrive/Projects/AFRI/data/ND/\"}\n",
    "chromeOptions.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": dnldpath, #Change default directory for downloads\n",
    "    \"download.prompt_for_download\": False, #To auto download the file\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True, #It will not show PDF directly in chrome\n",
    "    \"--enable-javascript\":True\n",
    "})\n",
    "driver = webdriver.Chrome(executable_path=driver_path, options=chromeOptions)\n",
    "\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "\n",
    "def unique(elements):\n",
    "    x = np.array(elements)\n",
    "    print(np.unique(x))\n",
    "\n",
    "urls=[]\n",
    "sessionurls = []\n",
    "billurls = []\n",
    "elements = []\n",
    "driver.get(\"https://house.mo.gov/LegislationSP.aspx?year=2022&code=R\")\n",
    "WebDriverWait(driver, 300).until(\n",
    "    EC.presence_of_element_located((By.CSS_SELECTOR, \"div.view-content ul li a\"))\n",
    ")\n",
    "driver.find_element_by_css_selector('#Bills\\ Truly\\ Agreed\\ and\\ Finally\\ Passed\\ by\\ Effective\\ Date').click()\n",
    "\n",
    "#reportgrid > tbody > tr > td\n",
    "#reportgrid > tbody > tr > td\n",
    "\n",
    "bills = driver.find_elements_by_css_selector('#reportgrid > tbody > tr:nth-child(2)')\n",
    "\n",
    "bills = driver.find_elements_by_xpath('//*[@id=\"reportgrid\"]/tbody/tr[2]')\n",
    "bills = driver.find_elements_by\n",
    "\n",
    "\n",
    "\n",
    "bills = driver.find_elements_by_xpath('/html/body/form/div[3]/div[1]/table/tbody/tr/th/table/tbody/tr')\n",
    "\n",
    "bills = driver.find_elements_by_css_selector('tbody > tr:nth-child(2) > td:nth-child(1) > a')\n",
    "\n",
    "bills = driver.find_elements_by_css_selector('table#reportgrid')\n",
    "\n",
    "//*[@id=\"reportgrid\"]/tbody/tr[2]\n",
    "\n",
    "bills = driver.find_elements_by_css_selector('tr.reportbillinfo')\n",
    "\n",
    "bills = driver.find_element_by_css_selector('#reportgrid > tbody > tr:nth-child(12) > td > a')\n",
    "\n",
    "#reportgrid > tbody > tr:nth-child(7) > td:nth-child(1)\n",
    "\n",
    "bills = driver.find_element_by_css_selector('#reportgrid > tbody > tr > td:nth-child(1) > a')\n",
    "\n",
    "\n",
    "digits = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "for digit in digits:\n",
    "    element = driver.find_elements_by_partial_link_text(digit)\n",
    "    elements.extend(element)\n",
    "    laws = list(set(elements))\n",
    "\n",
    "\n",
    "c = driver.find_element_by_css_selector('#GridView1')\n",
    "\n",
    "a = driver.find_elements_by_css_selector('tbody tr td a')\n",
    "\n",
    "b = driver.find_elements_by_css_selector('tbody > tr > td > a')\n",
    "\n",
    "a = driver.find_elements_by_css_selector('tbody > tr > td:nth-child(1) > a')\n",
    "\n",
    "#reportgrid > tbody > tr:nth-child(2) > td:nth-child(1) > a\n",
    "\n",
    "#reportgrid > tbody > tr:nth-child(2) > td:nth-child(1) > a\n",
    "\n",
    "laws.click()\n",
    "\n",
    "sessions = driver.find_elements_by_css_selector('div.view-content ul li a')\n",
    "for index,session in enumerate(sessions):\n",
    "    print(index)\n",
    "    print(session.text)\n",
    "    sessionurl = session.get_attribute('href')\n",
    "    sessionurls.append(sessionurl)\n",
    "\n",
    "for sessionurl in sessionurls[:33]:\n",
    "    driver.get(sessionurl)\n",
    "    sleep(1)\n",
    "    driver.find_element_by_link_text(\"Chapter Categories\").click()\n",
    "    try:\n",
    "        chapters = driver.find_elements_by_css_selector('div.field-items ul li a')\n",
    "        for chapter in chapters:\n",
    "            url = chapter.get_attribute('href')\n",
    "            urls.append(url)\n",
    "    except:\n",
    "        print(sessionurl)\n",
    "print(\"urls are done\")\n",
    "\n",
    "acttxts = []\n",
    "for idx,url in enumerate(urls):\n",
    "    driver.get(url)\n",
    "    sleep(1)\n",
    "    path = \"C:/Users/longy/OneDrive/Projects/AFRI/data/ND/\"\n",
    "    list_of_files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    print(latest_file)\n",
    "    with open(os.path.join(os.getcwd(), latest_file), 'r') as f:  # open in readonly mode\n",
    "        # creating a pdf File object of original pdf\n",
    "        pdfFileObj = open(latest_file, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        pagenumber = pdfReader.numPages\n",
    "        acttxt = []\n",
    "        pgtxts = []\n",
    "        for p in range(pagenumber):\n",
    "            pageObj = pdfReader.getPage(p)\n",
    "            pgtxt = pageObj.extractText()\n",
    "            pgtxts.append(pgtxt)\n",
    "            acttxt = '\\n'.join(pgtxts)\n",
    "    acttxts.append(acttxt)\n",
    "path = \"C:/Users/longy/OneDrive/Projects/AFRI/data/ND/\"\n",
    "list_of_files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "#latest_file = max(list_of_files, key=os.path.getctime)\n",
    "acttxts = []\n",
    "for idx,file in enumerate(list_of_files):\n",
    "    with open(os.path.join(os.getcwd(), file), 'r') as f:  # open in readonly mode\n",
    "        # creating a pdf File object of original pdf\n",
    "        pdfFileObj = open(file, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        pagenumber = pdfReader.numPages\n",
    "        acttxt = []\n",
    "        pgtxts = []\n",
    "        for p in range(pagenumber):\n",
    "            pageObj = pdfReader.getPage(p)\n",
    "            pgtxt = pageObj.extractText()\n",
    "            pgtxts.append(pgtxt)\n",
    "            acttxt = '  '.join(pgtxts)\n",
    "    acttxts.append(acttxt)\n",
    "datasource = pd.DataFrame({\n",
    "    'Link to full text':urls,\n",
    "    'Full text': acttxts\n",
    "})\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'Full text': acttxts\n",
    "})\n",
    "datasource.to_excel('ND_Leginfo.xlsx')\n",
    "datasource.to_csv('ND_Leginfo.csv')\n",
    "datasource.to_pickle('ND_Leginfo.pkl')\n",
    "datasource.to_json('ND_Leginfo.json')\n",
    "\n",
    "\n",
    "with open('ND_Leginfo.pkl', 'rb') as k:\n",
    "    data = pickle.load(k)\n",
    "\n",
    "# End of MO_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of CA_leginfo_v2.py\n",
    "\n",
    "# import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import pytest\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "\n",
    "\n",
    "# driver_path = 'C:/Program Files (x86)/Google/Chrome/Application/chromedriver.exe' #for windows\n",
    "# driver_path = '/Users/Yanxu/OneDrive/Projects/AgPolicyCoding/chromedriver'\n",
    "# driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir('/Users/long/OneDrive/Projects/AFRI')\n",
    "# os.chdir('/Volumes/GoogleDrive/Shared drives/UCSC-UMN AFRI project/Data/GA')\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "dnldpath = {\"download.default_directory\" : \"/Users/long/OneDrive/Projects/AFRI/data/CA/\"}\n",
    "chromeOptions.add_experimental_option(\"prefs\",dnldpath)\n",
    "driver = webdriver.Chrome(executable_path='./chromedriver', options=chromeOptions)\n",
    "\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "\n",
    "# input keyword(s)\n",
    "keywords = ['']\n",
    "\n",
    "# choose sessionyears\n",
    "#input_sessionyears = ['2021 - 2022']\n",
    "#input_sessionyears = ['2015 - 2016', '2013 - 2014', '2011 - 2012', '2009 - 2010', '2007 - 2008']\n",
    "#input_sessionyears = ['2015 - 2016', '2013 - 2014', '2011 - 2012', '2009 - 2010', '2007 - 2008']\n",
    "input_sessionyears = ['2021 - 2022', '2019 - 2020', '2017 - 2018', '2015 - 2016', '2013 - 2014', '2011 - 2012', '2009 - 2010', '2007 - 2008', '2005 - 2006', '2003 - 2004', '2001 - 2002', '1999 - 2000']\n",
    "\n",
    "\n",
    "urls = []\n",
    "# search bills by iterating sessionyears first and then keywords\n",
    "for input_sessionyear in input_sessionyears:\n",
    "    driver.get(\"https://leginfo.legislature.ca.gov/faces/advance/advance.xhtml\")\n",
    "\n",
    "    # select session year\n",
    "    select_sessionyear = Select(driver.find_element_by_css_selector('select#session_year'))\n",
    "    select_sessionyear.select_by_visible_text(input_sessionyear)\n",
    "\n",
    "    # search by keywords\n",
    "    #input_keywords = driver.find_element_by_css_selector(\"input#keyword.keyword_text\")\n",
    "    #input_keywords.send_keys(keyword)\n",
    "\n",
    "    WebDriverWait(driver, 300).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"input#executeSearchBtn\"))\n",
    "    )\n",
    "    search_button = driver.find_element_by_css_selector('input#executeSearchBtn')\n",
    "    search_button.click()\n",
    "\n",
    "    # get urls in the first page\n",
    "    searchresult = driver.find_elements_by_css_selector('tbody a')\n",
    "    basicinfo = driver.find_elements_by_css_selector('div.commdataRow')\n",
    "\n",
    "    for i in range(len(searchresult)):\n",
    "        #for i in range(10):\n",
    "        url = searchresult[i].get_attribute('href')\n",
    "        urls.append(url)\n",
    "\n",
    "searchresultmiddle = driver.find_elements_by_css_selector('tbody a')\n",
    "for i in range(len(searchresultmiddle)):\n",
    "    url = searchresultmiddle[i].get_attribute('href')\n",
    "urls.append(url)\n",
    "\n",
    "# get urls in the middle pages\n",
    "try:\n",
    "    #driver.find_element_by_link_text(\"Next 1000 BIlls\")\n",
    "    # centercolumn > div:nth-child(10) > input[type=submit]\n",
    "    next_page = driver.find_element_by_css_selector('div:nth-child(10) > input[type=submit]')\n",
    "    next_page.click()\n",
    "    searchresultmiddle = driver.find_elements_by_css_selector('tbody a')\n",
    "    for i in range(len(searchresultmiddle)):\n",
    "        url = searchresultmiddle[i].get_attribute('href')\n",
    "    urls.append(url)\n",
    "\n",
    "\n",
    "    WebDriverWait(driver, 300).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"div#searchresult tbody a\"))\n",
    "    )\n",
    "    searchresult = driver.find_elements_by_css_selector('div#searchresult tbody a')\n",
    "    basicinfo = driver.find_elements_by_css_selector('div.commdataRow')\n",
    "    for i in range(len(searchresult)):\n",
    "        url = searchresult[i].get_attribute('href')\n",
    "        urls.append(url)\n",
    "    WebDriverWait(driver, 300).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"div#text_bill_returned\"))\n",
    "    )\n",
    "    text_bill_returned = driver.find_element_by_css_selector('div#text_bill_returned').text\n",
    "    page_index = int(text_bill_returned.rsplit('Page ')[1].rsplit(' of')[0])\n",
    "    sleep(3)\n",
    "\n",
    "#print(\"done with urls\")\n",
    "\n",
    "\n",
    "# set up list titles\n",
    "years = []\n",
    "states = []\n",
    "sessionyears = []\n",
    "billnumbers = []\n",
    "titles = []\n",
    "briefsummarys = []\n",
    "introduceddates = []\n",
    "signingdates = []\n",
    "effectivedates = []\n",
    "expireddates = []\n",
    "leadauthors = []\n",
    "textlinks = []\n",
    "\n",
    "\n",
    "# extract bill information on each bill\n",
    "\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "    version = driver.find_element_by_css_selector(\"select#version.bill_version_select\")\n",
    "\n",
    "    if \"Chaptered\" in version.text:\n",
    "        introduceddate = version.text.split(\"- Introduced\")[0]\n",
    "        introduceddate = introduceddate.split(\" \")[-2]\n",
    "        introduceddates.append(introduceddate)\n",
    "\n",
    "        state = \"CA\"\n",
    "        states.append(state)\n",
    "\n",
    "        billnumber = driver.find_element_by_css_selector('div#bill_title')\n",
    "        billnumbers.append(billnumber.text.rsplit(\" \")[0])\n",
    "\n",
    "        title = driver.find_element_by_css_selector('div#bill_title h2')\n",
    "        title = title.text.split(\" \",1)[1]\n",
    "        titles.append((title.rsplit(\"(\",1)[0]))\n",
    "\n",
    "        try:\n",
    "            briefsummaryresolution = driver.find_element_by_css_selector('span.Resolution').text\n",
    "        except:\n",
    "            briefsummaryresolution = \"N/A\"\n",
    "\n",
    "        briefsummarydigest = driver.find_element_by_css_selector('span#digesttext').text\n",
    "\n",
    "        if not briefsummarydigest:\n",
    "            briefsummary = briefsummaryresolution\n",
    "        else:\n",
    "            briefsummary = briefsummarydigest\n",
    "\n",
    "        briefsummarys.append(briefsummary)\n",
    "\n",
    "        if not version.text.find(\"Chaptered\"):\n",
    "            signingdate = 'N/A'\n",
    "        else:\n",
    "            signingdate = version.text.split(\"- Chaptered\")[0].split(\" \")[-2]\n",
    "\n",
    "        signingdates.append(signingdate)\n",
    "\n",
    "        signingdate_str = str(signingdate)\n",
    "        signingdate_length = len(signingdate_str)\n",
    "        shortyear = int(str(signingdate)[signingdate_length-2: signingdate_length])\n",
    "        if shortyear > 22:\n",
    "            f\"{shortyear:02}\"\n",
    "        else:\n",
    "            if shortyear < 10:\n",
    "                year = \"200\" + str(shortyear)\n",
    "            else:\n",
    "                year = \"20\" + str(shortyear)\n",
    "        years.append(year)\n",
    "\n",
    "        if (int(year) % 2) == 0:\n",
    "            sessionyear = str(int(year)-1)+\"-\"+str(year)\n",
    "        else:\n",
    "            sessionyear = str(year)+str(\"-\")+str(int(year)+1)\n",
    "        sessionyears.append(sessionyear)\n",
    "\n",
    "        if \"take effect immediately\" in briefsummary:\n",
    "            effectivedate = signingdate\n",
    "        else:\n",
    "            if \"take effect on or before\" in briefsummary:\n",
    "                effectivedate = briefsummary.split(\"take effect on or before\")[1].split(\".\")[0]\n",
    "            elif \"take effect on\" in briefsummary:\n",
    "                effectivedate = briefsummary.split(\"take effect on\")[1].split(\".\")[0]\n",
    "            else:\n",
    "                effectivedate = \"Need to check\"\n",
    "\n",
    "        effectivedates.append(effectivedate)\n",
    "\n",
    "        if datefinder.find_dates(briefsummary):\n",
    "            dates = datefinder.find_dates(briefsummary)\n",
    "            datelist = []\n",
    "            try:\n",
    "                for date in dates:\n",
    "                    datelist.append(date.date())\n",
    "                exdtkeys = ['expired', 'repeal', 'extend', 'repealed', 'repeals']\n",
    "                if any(key in briefsummary for key in exdtkeys):\n",
    "                    expireddate = max(datelist)\n",
    "                else:\n",
    "                    expireddate = 'N/A'\n",
    "            except:\n",
    "                expireddate = 'Need to check'\n",
    "        else:\n",
    "            expireddate = 'N/A'\n",
    "\n",
    "        expireddates.append(expireddate)\n",
    "\n",
    "        driver.find_element_by_css_selector(\"#nav_bar_top_status > span\").click()\n",
    "        leadauthor = driver.find_element_by_css_selector('#leadAuthors').text\n",
    "        #try:\n",
    "        #    leadauthor = driver.find_element_by_css_selector('#leadAuthors').text.replace(\" (A)\",\"\")\n",
    "        #except:\n",
    "        #    leadauthor = driver.find_element_by_css_selector('#leadAuthors').text.replace(\" (S)\",\"\")\n",
    "        leadauthors.append(leadauthor)\n",
    "\n",
    "        textlinks.append(url)\n",
    "        sleep(5)\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'Year': years,\n",
    "    'State': states,\n",
    "    'Session Year': sessionyears,\n",
    "    'Bill Number': billnumbers,\n",
    "    'Name': titles,\n",
    "    'Brief Summary': briefsummarys,\n",
    "    'Introduced Date': introduceddates,\n",
    "    'Date it was signed':signingdates,\n",
    "    'Date effective':effectivedates,\n",
    "    'Expiration date':expireddates,\n",
    "    'Introducer':leadauthors,\n",
    "    'Link to full text':textlinks\n",
    "})\n",
    "\n",
    "# drop duplicates by \"Official ID\"\n",
    "datasource.drop_duplicates(subset = ['Bill Number', 'Year'],\n",
    "                           keep = 'first', inplace = True, ignore_index= True)\n",
    "\n",
    "# save bill info into excel file\n",
    "datasource.to_excel('CA_Leginfo.xlsx')\n",
    "print(\"download finished\")\n",
    "# End of CA_leginfo_v2.py\n",
    "\n",
    "\n",
    "# Start of CO_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "## selenium\n",
    "import selenium\n",
    "print(selenium.__version__)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.support.ui import Select\n",
    "## others\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "import requests\n",
    "import fitz\n",
    "import re\n",
    "\n",
    "# set up working directory\n",
    "working_directory = '/Volumes/SSD/AFRI/Data/Raw_Data/CO'\n",
    "print(os.path.abspath(working_directory))\n",
    "\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"./PDFs/\"\n",
    "print(os.path.abspath(download_dir))\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path = '../../geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service = service)\n",
    "\n",
    "# empty list to store info\n",
    "act_pdf_urls = []\n",
    "bill_nums = []\n",
    "chapter_nums = []\n",
    "years = []\n",
    "sessions = []\n",
    "broken_act_urls = []\n",
    "act_txts = []\n",
    "act_names = []\n",
    "names = []\n",
    "file_names = []\n",
    "effective_dates = []\n",
    "\n",
    "\n",
    "# Colorado Legislature Website: https://leg.colorado.gov/session-laws?field_sessions_target_id=75371&sort_bef_combine=field_page_value%20ASC\n",
    "\n",
    "driver.get('https://leg.colorado.gov/session-laws?field_sessions_target_id=92641&sort_bef_combine=field_page_value%20ASC')\n",
    "\n",
    "WebDriverWait(driver, 60).until(\n",
    "    EC.element_to_be_clickable((By.CSS_SELECTOR, '#edit-field-sessions-target-id'))\n",
    ")\n",
    "\n",
    "select_session = Select(driver.find_element(By.CSS_SELECTOR,'#edit-field-sessions-target-id'))\n",
    "\n",
    "all_sessions = driver.find_elements(By.CSS_SELECTOR,'#edit-field-sessions-target-id  > option')\n",
    "\n",
    "session_pages = []\n",
    "\n",
    "session_elements = []\n",
    "\n",
    "for session in all_sessions:\n",
    "    if int(session.text[:4]) < 2022:\n",
    "        if session.text != '2017 Extraordinary Session':\n",
    "            print(session.text)\n",
    "            select_session.select_by_visible_text(session.text)\n",
    "            time.sleep(5)\n",
    "            try:\n",
    "                driver.find_element(By.CSS_SELECTOR, '.pager-last > a:nth-child(1)').click()\n",
    "                time.sleep(5)\n",
    "                session_pages.append(int(driver.find_element(By.CSS_SELECTOR, '.pager-current').text))\n",
    "                session_elements.append(session)\n",
    "            except:\n",
    "                session_pages.append(1)\n",
    "                session_elements.append(session)\n",
    "\n",
    "for index,session in enumerate(session_elements):\n",
    "    if int(session.text[:4]) < 2022:\n",
    "        if session.text != '2017 Extraordinary Session':\n",
    "            print(session.text)\n",
    "            select_session.select_by_visible_text(session.text)\n",
    "            time.sleep(5)\n",
    "\n",
    "            WebDriverWait(driver, 300).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".views-table > tbody:nth-child(2) > tr \"))\n",
    "            )\n",
    "\n",
    "            rows = driver.find_elements(By.CSS_SELECTOR, '.views-table > tbody:nth-child(2) > tr')\n",
    "            for row in rows:\n",
    "\n",
    "                years.append(int(session.text[:4]))\n",
    "\n",
    "                bill_num = row.find_element(By.CSS_SELECTOR, ':nth-child(1)').text\n",
    "                bill_nums.append(bill_num)\n",
    "\n",
    "                name = row.find_element(By.CSS_SELECTOR, ':nth-child(2)').text\n",
    "                names.append(name)\n",
    "\n",
    "                effective_date = row.find_element(By.CSS_SELECTOR, ':nth-child(3)').text\n",
    "                effective_dates.append(effective_date)\n",
    "\n",
    "                chapter_num = row.find_element(By.CSS_SELECTOR, ':nth-child(5)').text\n",
    "                chapter_nums.append(chapter_num)\n",
    "\n",
    "                act_pdf_url = row.find_element(By.CSS_SELECTOR, ':nth-child(6) > a:nth-child(1)').get_attribute('href')\n",
    "                act_pdf_urls.append(act_pdf_url)\n",
    "\n",
    "\n",
    "\n",
    "            if session.text != '2020 Extraordinary Session':\n",
    "                for i in range(1, session_pages[index]):\n",
    "\n",
    "                    driver.find_element(By.CSS_SELECTOR, '.pager-next > a:nth-child(1)').click()\n",
    "                    time.sleep(10)\n",
    "                    rows = driver.find_elements(By.CSS_SELECTOR, '.views-table > tbody:nth-child(2) > tr')\n",
    "                    for row in rows:\n",
    "\n",
    "                        years.append(int(session.text[:4]))\n",
    "\n",
    "                        bill_num = row.find_element(By.CSS_SELECTOR, ':nth-child(1)').text\n",
    "                        bill_nums.append(bill_num)\n",
    "\n",
    "                        name = row.find_element(By.CSS_SELECTOR, ':nth-child(2)').text\n",
    "                        names.append(name)\n",
    "\n",
    "                        effective_date = row.find_element(By.CSS_SELECTOR, ':nth-child(3)').text\n",
    "                        effective_dates.append(effective_date)\n",
    "\n",
    "                        chapter_num = row.find_element(By.CSS_SELECTOR, ':nth-child(5)').text\n",
    "                        chapter_nums.append(chapter_num)\n",
    "\n",
    "                        act_pdf_url = row.find_element(By.CSS_SELECTOR, ':nth-child(6) > a:nth-child(1)').get_attribute('href')\n",
    "                        act_pdf_urls.append(act_pdf_url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for year in range(1993, 2016):\n",
    "    print(year)\n",
    "    session_url = 'https://leg.colorado.gov/agencies/office-legislative-legal-services/session-laws-' + str(year)\n",
    "    driver.get(session_url)\n",
    "\n",
    "    WebDriverWait(driver, 300).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"div > table > tbody > tr\"))\n",
    "    )\n",
    "\n",
    "    rows = driver.find_elements(By.CSS_SELECTOR, 'div > table > tbody > tr')\n",
    "    for row in rows[1:]:\n",
    "\n",
    "        bill_num = row.find_element(By.CSS_SELECTOR, 'td:nth-child(1)').text\n",
    "        print(bill_num)\n",
    "        if \"Extraordinary\" in bill_num:\n",
    "            continue\n",
    "        bill_nums.append(bill_num)\n",
    "\n",
    "        years.append(year)\n",
    "\n",
    "        name = row.find_element(By.CSS_SELECTOR, 'td:nth-child(2)').text\n",
    "        names.append(name)\n",
    "\n",
    "        effective_date = row.find_element(By.CSS_SELECTOR, 'td:nth-child(3)').text\n",
    "        effective_dates.append(effective_date)\n",
    "\n",
    "        chapter_num = row.find_element(By.CSS_SELECTOR, 'td:nth-child(5)').text\n",
    "        chapter_nums.append(chapter_num)\n",
    "\n",
    "        try:\n",
    "            act_pdf_url = row.find_element(By.CSS_SELECTOR, 'td:nth-child(6) > a:nth-child(1)').get_attribute('href')\n",
    "            act_pdf_urls.append(act_pdf_url)\n",
    "        except:\n",
    "            act_pdf_urls.append('NA')\n",
    "\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'bill_num': bill_nums,\n",
    "    'chapter_num': chapter_nums,\n",
    "    'link': act_pdf_urls,\n",
    "    'state': \"CO\",\n",
    "    'name': names,\n",
    "    'effective_date':effective_dates,\n",
    "})\n",
    "\n",
    "#for these 'HB11-1130''SB11-024''HB11-1169''SB11-020' bills, I added links by hand.\n",
    "datasource['link'][7136] = 'https://leg.colorado.gov/sites/default/files/images/olls/2011a_sl_106.pdf'\n",
    "datasource['link'][7109] = 'https://leg.colorado.gov/sites/default/files/images/olls/2011a_sl_79.pdf'\n",
    "datasource['link'][7149] = 'https://leg.colorado.gov/sites/default/files/images/olls/2011a_sl_119.pdf'\n",
    "datasource['link'][7069] = 'https://leg.colorado.gov/sites/default/files/images/olls/2011a_sl_39.pdf'\n",
    "\n",
    "#delete one bill since there is no pdf file.https://leg.colorado.gov/agencies/office-legislative-legal-services/session-laws-1994\n",
    "# SB94-157 \tImplementation Of Federal Mandates \t6/1/1994 \t1829 \t305\n",
    "datasource = datasource[datasource['link'] != 'NA']\n",
    "\n",
    "def convert_to_four_digit_year(two_digit_year):\n",
    "    cutoff = 50  # Define a cutoff for deciding the century\n",
    "    if two_digit_year < cutoff:\n",
    "        return 2000 + two_digit_year\n",
    "    else:\n",
    "        return 1900 + two_digit_year\n",
    "\n",
    "datasource.loc[:, 'year'] = datasource['bill_num'].apply(lambda x: convert_to_four_digit_year(int(x[2:4])))\n",
    "\n",
    "\n",
    "# download corresponding pdf file\n",
    "file_names = []\n",
    "for index,row in datasource.iterrows():\n",
    "    if int(row['year']) > 2015:\n",
    "        act_url = row['link']\n",
    "        print(act_url)\n",
    "        file_name = act_url.split('sl/')[1]\n",
    "        file_names.append(file_name )\n",
    "        print(file_name)\n",
    "\n",
    "        path = download_dir + file_name\n",
    "        print(os.path.abspath(path))\n",
    "\n",
    "        time.sleep(randint(1, 100) * 0.005)\n",
    "\n",
    "        response = requests.get(act_url)\n",
    "        # Ensure the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Write the content of the response to a file\n",
    "            with open(path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "            broken_act_urls.append(act_url)\n",
    "    else:\n",
    "        act_url = row['link']\n",
    "        print(act_url)\n",
    "        file_name = act_url.split('olls/')[1]\n",
    "        file_names.append(file_name )\n",
    "        print(file_name)\n",
    "\n",
    "        path = download_dir + file_name\n",
    "        print(os.path.abspath(path))\n",
    "\n",
    "        time.sleep(randint(1, 100) * 0.005)\n",
    "\n",
    "        response = requests.get(act_url)\n",
    "        # Ensure the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Write the content of the response to a file\n",
    "            with open(path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "            broken_act_urls.append(act_url)\n",
    "\n",
    "datasource['file_name'] = file_names\n",
    "\n",
    "datasource = datasource.drop_duplicates()\n",
    "\n",
    "#read pdfs\n",
    "folder_path = \"./PDFs\"\n",
    "print(os.path.abspath(folder_path))\n",
    "\n",
    "files = glob.glob(os.path.join(folder_path, '*.pdf'))\n",
    "\n",
    "broken_files = []\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        doc = fitz.open(file)\n",
    "        pgtxts = []\n",
    "        for page in doc:\n",
    "            pgtxt = page.get_text()\n",
    "            pgtxts.append(pgtxt)\n",
    "            acttxt = ' '.join(pgtxts)\n",
    "        act_txts.append(acttxt)\n",
    "        doc.close()\n",
    "    except:\n",
    "        act_txts.append('NA')\n",
    "        broken_files.append(file)\n",
    "\n",
    "file_names = []\n",
    "\n",
    "for file in files:\n",
    "    file_names.append(file.split(\"./PDFs/\")[1])\n",
    "\n",
    "datasource_pdf = pd.DataFrame({\n",
    "    'original_text': act_txts,\n",
    "    'file_name': file_names\n",
    "})\n",
    "\n",
    "\n",
    "datasource = pd.merge(datasource,datasource_pdf,on='file_name', how='left')\n",
    "\n",
    "datasource.to_excel('CO_Leginfo.xlsx')\n",
    "datasource.to_csv('CO_Leginfo.csv')\n",
    "datasource.to_pickle('CO_Leginfo.pkl')\n",
    "datasource.to_json('CO_Leginfo.json')\n",
    "\n",
    "# End of CO_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of TN_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "#import PyPDF2\n",
    "#import glob\n",
    "\n",
    "driver_path = 'C:/Users/longy/OneDrive/Projects/AFRI/windows/chromedriver.exe' #for windows\n",
    "# driver_path = '/Users/Yanxu/OneDrive/Projects/AgPolicyCoding/chromedriver'\n",
    "# driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "\n",
    "# Change the working directory\n",
    "#os.chdir('/Users/long/OneDrive/Projects/AFRI')\n",
    "os.chdir('C:/Users/longy/OneDrive/Projects/AFRI')\n",
    "# os.chdir('/Volumes/GoogleDrive/Shared drives/UCSC-UMN AFRI project/Data/GA')\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "# for mac\n",
    "#dnldpath = {\"download.default_directory\" : \"/Users/long/OneDrive/Projects/AFRI/data/SD/\"}\n",
    "# for windows\n",
    "dnldpath = {\"download.default_directory\" :\"C:/Users/longy/OneDrive/Projects/AFRI/data/LA/\"}\n",
    "chromeOptions.add_experimental_option(\"prefs\",dnldpath)\n",
    "driver = webdriver.Chrome(executable_path=driver_path, options=chromeOptions)\n",
    "\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "\n",
    "driver.get('https://sos.tn.gov/publications/services/acts-and-resolutions')\n",
    "\n",
    "# End of TN_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of WI_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import time\n",
    "\n",
    "#driver_path = 'C:/Users/longy/OneDrive/Projects/AFRI/windows/chromedriver.exe' #for windows\n",
    "driver_path = '/Users/long/Library/CloudStorage/OneDrive-Personal/Projects/AFRI/chromedriver'\n",
    "# driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir('/Users/long/Library/CloudStorage/OneDrive-Personal/Projects/AFRI')\n",
    "#os.chdir('C:/Users/longy/OneDrive/Projects/AFRI')\n",
    "# os.chdir('/Volumes/GoogleDrive/Shared drives/UCSC-UMN AFRI project/Data/GA')\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "# for mac\n",
    "#dnldpath = {\"download.default_directory\" : \"/Users/long/OneDrive/Projects/AFRI/data/SD/\"}\n",
    "# for windows\n",
    "chromeOptions.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": \"/Users/long/Library/CloudStorage/OneDrive-Personal/Projects/AFRI/data/WI/\", #Change default directory for downloads\n",
    "    \"download.prompt_for_download\": False, #To auto download the file\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True, #It will not show PDF directly in chrome\n",
    "    \"--enable-javascript\":True})\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=driver_path, options=chromeOptions)\n",
    "#river = webdriver.Firefox()\n",
    "\n",
    "sessions_first = ['1975','1977','1979','1981','1983','1985','1987','1989','1991','1993']\n",
    "sessions_first_act_urls = []\n",
    "acttxts = []\n",
    "sts = []\n",
    "\n",
    "for session in sessions_first:\n",
    "    session_url = \"https://docs.legis.wisconsin.gov/\" + str(session) + \"/related/acts\"\n",
    "    print(session_url)\n",
    "    driver.get(session_url)\n",
    "    acts = driver.find_elements_by_partial_link_text(\"Wisconsin\")\n",
    "    for act in acts:\n",
    "        act_url = act.get_attribute('href')\n",
    "        act_urls.append(act_url)\n",
    "\n",
    "for act in sessions_first_act_urls:\n",
    "    driver.get(act)\n",
    "    sleep(randint(1, 2))\n",
    "\n",
    "path = \"/Users/long/OneDrive/Projects/AFRI/data/WI/\"\n",
    "list_of_files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "\n",
    "for file in list_of_files:\n",
    "    with open(os.path.join(os.getcwd(), file), 'r') as f:  # open in readonly mode\n",
    "        # creating a pdf File object of original pdf\n",
    "        pdfFileObj = open(latest_file, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        pagenumber =  pdfReader.numPages\n",
    "        acttxt =[]\n",
    "        pgtxts = []\n",
    "        for p in range(pagenumber):\n",
    "            pageObj = pdfReader.getPage(p)\n",
    "            pgtxt = pageObj.extractText()\n",
    "            pgtxts.append(pgtxt)\n",
    "            acttxt = '\\n'.join(pgtxts)\n",
    "    acttxts.append(acttxt)\n",
    "    st =\"WI\"\n",
    "    sts.append(st)\n",
    "    sleep(1)\n",
    "\n",
    "act2_urls = []\n",
    "\n",
    "sessions_second = ['1995','1997','1999','2001','2003','2005','2007','2009','2011','2013','2015','2017','2019','2021']\n",
    "\n",
    "for session in sessions_second:\n",
    "    session_url = \"https://docs.legis.wisconsin.gov/\" + str(session) + \"/related/acts\"\n",
    "    print(session_url)\n",
    "    driver.get(session_url)\n",
    "    sleep(randint(1,2))\n",
    "    acts = driver.find_elements_by_partial_link_text(\"Wisconsin Act\")\n",
    "    for act in acts:\n",
    "        act2_url = act.get_attribute('href')\n",
    "        act2_urls.append(act2_url)\n",
    "        print(act2_url)\n",
    "        t = 0.1 * randint(1, 9)\n",
    "        time.sleep(t)\n",
    "\n",
    "for index,url in enumerate(act2_urls):\n",
    "    print(index)\n",
    "    driver.get(url)\n",
    "    t = 0.3* randint(1, 9)\n",
    "    time.sleep(t)\n",
    "    WebDriverWait(driver, 300).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"div#document.acts\"))\n",
    "    )\n",
    "    acttxt = driver.find_element_by_css_selector('div#document.acts').text\n",
    "    acttxts.append(acttxt)\n",
    "    st = \"WI\"\n",
    "    sts.append(st)\n",
    "    t = 0.1* randint(1, 9)\n",
    "    time.sleep(t)\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    #'Link to full text':session_urls,\n",
    "    'Full text': acttxts\n",
    "})\n",
    "\n",
    "# drop duplicates by \"Official ID\"\n",
    "datasource.drop_duplicates(subset = ['Full text'],\n",
    "                           keep = 'first', inplace = True, ignore_index= True)\n",
    "\n",
    "\n",
    "datasource.to_excel('WI_Leginfo.xlsx')\n",
    "datasource.to_csv('WI_Leginfo.csv')\n",
    "datasource.to_pickle('WI_Leginfo.pkl')\n",
    "datasource.to_json('WI_Leginfo.json')\n",
    "# End of WI_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of MI_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# driver_path = 'C:/Program Files (x86)/Google/Chrome/Application/chromedriver.exe' #for windows\n",
    "# driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "# set up webdriver by short path\n",
    "driver = webdriver.Chrome('./chromedriver')\n",
    "\n",
    "\n",
    "# choose sessionyears\n",
    "input_sessionyears = ['2021-2022', '2019-2020', '2017-2018', '2015-2016', '2013-2014', '2011-2012', '2009-2010',\n",
    "                      '2007-2008', '2005-2006', '2003-2004', '2001-2002', '1999-2000', '1997-1998']\n",
    "\n",
    "\n",
    "urls = []\n",
    "\n",
    "for input_sessionyear in input_sessionyears:\n",
    "    driver.get(\"https://www.legislature.mi.gov/(S(uycj0anii1cfudf2mj0uyllp))/mileg.aspx?page=LegAdvancedSearch\")\n",
    "\n",
    "    select_public = driver.find_element_by_css_selector('#frg_legadvancedsearch_PublicActs')\n",
    "    select_public.click()\n",
    "\n",
    "    # select session year\n",
    "    select_sessionyear = Select(\n",
    "        driver.find_element_by_css_selector('#frg_legadvancedsearch_LegislativeSession_LegislativeSession'))\n",
    "    select_sessionyear.select_by_visible_text(input_sessionyear)\n",
    "\n",
    "    # search by keywords\n",
    "    #input_keywords = driver.find_element_by_css_selector('#frg_legadvancedsearch_LegFullText')\n",
    "    #input_keywords.send_keys(keyword)\n",
    "\n",
    "    # click search button\n",
    "    search_button = driver.find_element_by_css_selector('#frg_legadvancedsearch_SearchButton')\n",
    "    search_button.click()\n",
    "\n",
    "    WebDriverWait(driver, 300).until(\n",
    "        EC.presence_of_element_located(\n",
    "            (By.CSS_SELECTOR, \"#maincontent > table:nth-child(2) > tbody > tr:nth-child(1) > td > span\"))\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        searchresult = driver.find_elements_by_css_selector(\n",
    "            'table#frg_executesearch_SearchResults_Results  tbody  tr td a')\n",
    "    except:\n",
    "        continue\n",
    "    sleep(3)\n",
    "\n",
    "    for i in range(len(searchresult)):\n",
    "        url = searchresult[i].get_attribute('href')\n",
    "        urls.append(url)\n",
    "\n",
    "print(\"done with urls\")\n",
    "\n",
    "# set up list titles\n",
    "years = []\n",
    "states = []\n",
    "sessionyears = []\n",
    "billnumbers = []\n",
    "names = []\n",
    "briefsummarys = []\n",
    "introduceddates = []\n",
    "signingdates = []\n",
    "effectivedates = []\n",
    "expirationdates = []\n",
    "leadauthors = []\n",
    "textlinks = []\n",
    "acttxts = []\n",
    "\n",
    "for url in urls:\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    textlink = driver.find_element_by_css_selector(\"#frg_print_HtmlDocIcon_DocumentImage\").get_attribute('href')\n",
    "    textlinks.append(textlink)\n",
    "\n",
    "    state = \"MI\"\n",
    "    states.append(state)\n",
    "\n",
    "    year = driver.find_element_by_css_selector('#frg_print_DocumentName').text.split(\" of \")[1]\n",
    "    years.append(year)\n",
    "\n",
    "    if (int(year) % 2) == 0:\n",
    "        sessionyear = str(int(year) - 1) + \"-\" + str(year)\n",
    "    else:\n",
    "        sessionyear = str(year) + str(\"-\") + str(int(year) + 1)\n",
    "    sessionyears.append(sessionyear)\n",
    "\n",
    "    briefsummary = driver.find_element_by_css_selector('#frg_print_DocumentDescription').text\n",
    "    briefsummarys.append(briefsummary)\n",
    "\n",
    "    text = driver.find_element_by_css_selector('#frg_print_HtmlDocIcon_DocumentImage > img')\n",
    "    text.click()\n",
    "\n",
    "    #try:\n",
    "    #    billnumber = driver.find_element_by_xpath(\"//font[contains(text(),'SENATE BILL No.')]\").text.split(\"No. \")[1]\n",
    "    #    billnumber = \"SB-\" + str(billnumber)\n",
    "    #except:\n",
    "    #    billnumber = driver.find_element_by_xpath(\"//font[contains(text(),'HOUSE BILL No.')]\").text.split(\"No. \")[1]\n",
    "    #    billnumber = \"HB-\" + str(billnumber)\n",
    "    #billnumbers.append(billnumber)\n",
    "\n",
    "    #name = driver.find_element_by_xpath(\"//font[contains(text(),'AN ACT to')]\").text\n",
    "    #names.append(name)\n",
    "\n",
    "    #leadauthor = driver.find_element_by_xpath(\"//font[contains(text(),'Introduced by')]\").text.split(\"Introduced by \")[1]\n",
    "    #leadauthors.append(leadauthor)\n",
    "\n",
    "    #signingdate = driver.find_element_by_css_selector('body').text.split('Approved by the Governor')[1].split('Filed with the Secretary of State')[0]\n",
    "    #dates = datefinder.find_dates(signingdate)\n",
    "    #datelist = []\n",
    "    #for date in dates:\n",
    "    #    signingdate = date.date()\n",
    "    #signingdates.append(signingdate)\n",
    "\n",
    "    acttxt = driver.find_element_by_css_selector(\"body\").text\n",
    "    acttxts.append(acttxt)\n",
    "    sleep(randint(1,2))\n",
    "datasource = pd.DataFrame({\n",
    "    'Year': years,\n",
    "    'State': states,\n",
    "    'Session Year': sessionyears,\n",
    "    'Brief Summary': briefsummarys,\n",
    "    'Link to full text':textlinks,\n",
    "    'Full text': acttxts\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# save bill info into excel file\n",
    "datasource.to_excel('MI_Leginfo.xlsx')\n",
    "datasource.to_csv('MI_Leginfo.csv')\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite://', echo=False)\n",
    "datasource.to_sql('MI_Leginfo.sql',con=engine)\n",
    "datasource.to_pickle('MI_Leginfo.pkl')\n",
    "datasource.to_json('MI_Leginfo.json')\n",
    "\n",
    "\n",
    "datasource.to_stata('MI_Leginfo.dta')\n",
    "\n",
    "print(\"download finished\")\n",
    "# End of MI_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of TX_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import fitz\n",
    "import time\n",
    "# driver_path = r'C:\\Users\\longy\\OneDrive\\Projects\\AFRI\\windows\\chromedriver.exe' #for windows\n",
    "driver_path = '/Users/long/OneDrive/Projects/AFRI/chromedriver'\n",
    "# driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir('/Users/long/OneDrive/Projects/AFRI')\n",
    "# os.chdir('C:/Users/longy/OneDrive/Projects/AFRI/')\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "chromeOptions.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": \"C:/Users/longy/OneDrive/Projects/AFRI/data/TX/\", #Change default directory for downloads\n",
    "    \"download.default_directory\": \"/Users/long/OneDrive/Projects/AFRI/data/TX/\", #Change default directory for downloads\n",
    "    \"download.prompt_for_download\": False, #To auto download the file\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True, #It will not show PDF directly in chrome\n",
    "    \"--enable-javascript\":True})\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=driver_path, options=chromeOptions)\n",
    "\n",
    "#64-86th session 2021\n",
    "urls = []\n",
    "\n",
    "for session in range(64,87):\n",
    "    for sub in range(0,4):\n",
    "        url = \"https://lrl.texas.gov/legis/billsearch/searchchapter.cfm?legSession=\" + str(session) +\"-\" + str(sub) + \"&chapter=&submitbutton=Search+by+chapter\"\n",
    "        driver.get(url)\n",
    "        sleep(1)\n",
    "        act_urls = driver.find_elements_by_css_selector('td.results a')\n",
    "        for i in act_urls:\n",
    "            act_url = i.get_attribute('href')\n",
    "            if 'pdf' in act_url:\n",
    "                print(act_url)\n",
    "                urls.append(act_url)\n",
    "\n",
    "\n",
    "for act in urls:\n",
    "    driver.get(act)\n",
    "    t = 0.1* randint(1, 9)\n",
    "    time.sleep(t)\n",
    "\n",
    "\n",
    "\n",
    "path = \"/Users/long/Downloads/TX2\"\n",
    "files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "\n",
    "acttxts = []\n",
    "pgtxts = []\n",
    "for file in files:\n",
    "    with fitz.open(file) as doc:\n",
    "        for page in doc:\n",
    "            pgtxt = page.get_text()\n",
    "            pgtxts.append(pgtxt)\n",
    "            acttxt = '  '.join(pgtxts)\n",
    "        t = 0.03 * randint(1, 9)\n",
    "        time.sleep(t)\n",
    "    acttxts.append(acttxt)\n",
    "\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    #'Link to full text':session_urls,\n",
    "    'Full text': acttxts\n",
    "})\n",
    "\n",
    "\n",
    "datasource.to_string(\"TX_Leginfo\")\n",
    "datasource.to_excel('TX_Leginfo.xlsx')\n",
    "datasource.to_csv('TX_Leginfo.csv')\n",
    "datasource.to_pickle('TX_Leginfo.pkl')\n",
    "datasource.to_json('TX_Leginfo.json')\n",
    "\n",
    "\n",
    "# End of TX_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of NC_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "## selenium\n",
    "import selenium\n",
    "print(selenium.__version__)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.support.ui import Select\n",
    "## others\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "import requests\n",
    "import fitz\n",
    "import re\n",
    "\n",
    "# set up working directory\n",
    "working_directory = '/Volumes/SSD/AFRI/Data/Raw_Data/NC'\n",
    "print(os.path.abspath(working_directory))\n",
    "\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"./PDFs/\"\n",
    "print(os.path.abspath(download_dir))\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path = '../../geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service = service)\n",
    "\n",
    "# empty list to store info\n",
    "\n",
    "act_pdf_urls = []\n",
    "act_html_urls = []\n",
    "pdf_file_names = []\n",
    "html_file_names = []\n",
    "bill_nums = []\n",
    "chapter_nums = []\n",
    "years = []\n",
    "sessions = []\n",
    "broken_act_urls = []\n",
    "act_txts = []\n",
    "act_names = []\n",
    "names = []\n",
    "file_names = []\n",
    "summaries = []\n",
    "\n",
    "\n",
    "# North Caroline Legislature Website: https://www.ncleg.gov/Laws/SessionLaws/\n",
    "\n",
    "driver.get('https://www.ncleg.gov/Laws/SessionLaws/')\n",
    "\n",
    "WebDriverWait(driver, 60).until(\n",
    "    EC.element_to_be_clickable((By.CSS_SELECTOR, 'body'))\n",
    ")\n",
    "\n",
    "driver.find_element(By.CSS_SELECTOR,'#sessionDropdown_chosen > a:nth-child(1) > span:nth-child(1)').click()\n",
    "\n",
    "session_elements = driver.find_elements(By.CSS_SELECTOR,'li.active-result')\n",
    "session_list = []\n",
    "for session in session_elements:\n",
    "    session_list.append(session.text)\n",
    "    print(session.text)\n",
    "\n",
    "session_urls = []\n",
    "\n",
    "for text in session_list:\n",
    "    if int(text.split('-')[0]) <= 2021 and int(text.split('-')[0]) >= 1975:\n",
    "        if 'Extra' in text:\n",
    "            session_urls.append('https://www.ncleg.gov/Laws/SessionLaws/' + text.split('-')[0] + 'e' + text.split('Session ')[1])\n",
    "        else:\n",
    "            session_urls.append('https://www.ncleg.gov/Laws/SessionLaws/' + text.split('-')[0])\n",
    "\n",
    "for url in session_urls:\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, 'body'))\n",
    "    )\n",
    "\n",
    "    rows = driver.find_elements(By.CSS_SELECTOR,'div.py-2')\n",
    "\n",
    "    for row in rows:\n",
    "\n",
    "        act_html_url = row.find_element(By.CSS_SELECTOR,\n",
    "                                        'div:nth-child(1) > a:nth-child(1)').get_attribute('href')\n",
    "        act_html_urls.append(act_html_url)\n",
    "\n",
    "        act_pdf_url = row.find_element(By.CSS_SELECTOR,\n",
    "                                       'div:nth-child(1) > a:nth-child(2)').get_attribute('href')\n",
    "        act_pdf_urls.append(act_html_url)\n",
    "\n",
    "        chapter_num = row.find_element(By.CSS_SELECTOR,\n",
    "                                       'div:nth-child(1) > a:nth-child(3)').text\n",
    "        chapter_nums.append(chapter_num)\n",
    "\n",
    "        bill_num = row.find_element(By.CSS_SELECTOR,\n",
    "                                    'div:nth-child(2)').text\n",
    "        bill_nums.append(bill_num)\n",
    "\n",
    "        summary = row.find_element(By.CSS_SELECTOR,\n",
    "                                   'div:nth-child(3)').text\n",
    "        summaries.append(summary)\n",
    "\n",
    "datasource= pd.DataFrame({\n",
    "    'bill_num': bill_nums,\n",
    "    'chapter_num': chapter_nums,\n",
    "    'act_pdf_url': act_pdf_urls,\n",
    "    'act_html_url': act_html_urls,\n",
    "    'state': \"NC\",\n",
    "    'summary':summaries\n",
    "})\n",
    "\n",
    "\n",
    "# download corresponding pdf file\n",
    "\n",
    "for act_url in act_html_urls:\n",
    "    print(act_url)\n",
    "\n",
    "    # Open the desired webpage\n",
    "    driver.get(act_url)\n",
    "\n",
    "    act_txts.append(driver.find_element(By.CSS_SELECTOR,'body').text)\n",
    "\n",
    "    # Get the HTML content of the page\n",
    "    html_content = driver.page_source\n",
    "\n",
    "    name = act_url.split('SL')[1]\n",
    "    html_file_names.append(name)\n",
    "    print(name)\n",
    "\n",
    "    path = download_dir + name\n",
    "    print(os.path.abspath(path))\n",
    "\n",
    "    # Write the HTML content to a file\n",
    "    with open(path, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "datasource['original_text'] = act_txts\n",
    "datasource = datasource.dropna(subset=['original_text'])\n",
    "# delete duplicates\n",
    "datasource = datasource.drop_duplicates(subset=['original_text'])\n",
    "\n",
    "#datasource.to_excel('NC_Leginfo.xlsx') IllegalCharacterError\n",
    "datasource.to_csv('NC_Leginfo.csv')\n",
    "datasource.to_pickle('NC_Leginfo.pkl')\n",
    "datasource.to_json('NC_Leginfo.json')\n",
    "\n",
    "# End of NC_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of WA_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "## selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "## others\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import fitz\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# set up working directory\n",
    "working_directory = '/Volumes/SSD/AFRI/Data/Raw_Data/WA'\n",
    "print(os.path.abspath(working_directory))\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"../PDFs\"\n",
    "print(os.path.abspath(download_dir))\n",
    "\n",
    "# Create a Firefox profile and configure preferences\n",
    "options = Options()\n",
    "\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path='../../geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service = service)\n",
    "\n",
    "\n",
    "# The page you're looking for doesn't exist.\n",
    "''' \n",
    "['https://leg.wa.gov/CodeReviser/documents/sessionlaw/1991c54.pdf',\n",
    " 'https://leg.wa.gov/CodeReviser/documents/sessionlaw/1991ex1c1.pdf',\n",
    " 'http://lawfilesext.leg.wa.gov/biennium/1991-92/Pdf/Bills/Session%20Laws/House/120.sl.pdf',\n",
    " 'http://lawfilesext.leg.wa.gov/biennium/1991-92/Pdf/Bills/Session%20Laws/Senate/6347-S2.sl.pdf',\n",
    " 'http://lawfilesext.leg.wa.gov/biennium/1995-96/Pdf/Bills/Session%20Laws/House/164.sl.pdf']\n",
    "'''\n",
    "\n",
    "\n",
    "driver.get('https://leg.wa.gov/CodeReviser/pages/session_laws.aspx')\n",
    "\n",
    "# Create a list from range\n",
    "years_list = list(range(1975, 2022))  # Note: 2022 is exclusive\n",
    "\n",
    "# Remove the year 1978\n",
    "years_list.remove(1978)\n",
    "\n",
    "years = []\n",
    "links = []\n",
    "\n",
    "badfiles = []\n",
    "\n",
    "for year in years_list:\n",
    "    driver.get('https://leg.wa.gov/CodeReviser/pages/session_laws.aspx')\n",
    "    print(year)\n",
    "    driver.find_element(By.PARTIAL_LINK_TEXT,str(year)).click()\n",
    "    chapters = driver.find_elements(By.PARTIAL_LINK_TEXT, \"Ch.\")\n",
    "    for chapter in chapters:\n",
    "        try:\n",
    "            years.append(year)\n",
    "            link = chapter.get_attribute(\"href\")\n",
    "            links.append(link)\n",
    "            time.sleep(0.01*randint(1,2))\n",
    "        except:\n",
    "            badfiles.append(chapter)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'year':years,\n",
    "    'link': links\n",
    "})\n",
    "\n",
    "# Dropping duplicates based on 'links' column\n",
    "df = df.drop_duplicates(subset='links')\n",
    "\n",
    "\n",
    "\n",
    "# store some broken pages\n",
    "bad_act_urls = []\n",
    "\n",
    "file_names = []\n",
    "# download pdfs from the above links scraped\n",
    "for index, link in enumerate(df['links']):\n",
    "    try:\n",
    "        name = str(link).split(\"sessionlaw/\")[1].split(\".pdf\")[0]\n",
    "        path = \"/Volumes/SSD/AFRI/Data/Raw_Data/WA/PDFs/\" + name + \".pdf\"\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(link)\n",
    "        # Ensure the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Write the content of the response to a file\n",
    "            with open(path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "            bad_act_urls.append(link)\n",
    "    except:\n",
    "        # Find the position of the last '/'\n",
    "        last_slash_pos = link.rfind('/')\n",
    "\n",
    "        # Split the URL into two parts\n",
    "        base_url = link[:last_slash_pos]\n",
    "        name = str(link.split(\"biennium/\")[1].split(\"/Pdf\")[0]) + link[last_slash_pos + 1:].split(\".pdf\")[0]\n",
    "        path = \"/Volumes/SSD/AFRI/Data/Raw_Data/WA/PDFs/\" + name + \".pdf\"\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(link)\n",
    "        # Ensure the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Write the content of the response to a file\n",
    "            with open(path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "            bad_act_urls.append(link)\n",
    "    time.sleep(0.01)\n",
    "    file_names.append(name)\n",
    "\n",
    "df['file_name'] = file_names\n",
    "\n",
    "\n",
    "act_txts = []\n",
    "pgtxts = []\n",
    "file_names_from_folder = []\n",
    "\n",
    "path = \"./PDFs\"\n",
    "files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "print(os.path.abspath(path))\n",
    "\n",
    "for file in files:\n",
    "    doc = fitz.open(file)\n",
    "    pgtxts = []\n",
    "    for page in doc:\n",
    "        pgtxt = page.get_text()\n",
    "        pgtxts.append(pgtxt)\n",
    "        acttxt = ' '.join(pgtxts)\n",
    "    act_txts.append(acttxt)\n",
    "    doc.close()\n",
    "    file_names_from_folder.append(file.split(\"PDFs/\")[1].replace(\".pdf\",\"\"))\n",
    "\n",
    "df_pdfs = pd.DataFrame({\n",
    "    'original_text':act_txts,\n",
    "    'file_name': file_names_from_folder\n",
    "})\n",
    "\n",
    "\n",
    "datasource = pd.merge(df, df_pdfs, on='file_name', how='inner')\n",
    "\n",
    "datasource.to_excel('WA_Leginfo.xlsx')\n",
    "datasource.to_csv('WA_Leginfo.csv')\n",
    "datasource.to_pickle('WA_Leginfo.pkl')\n",
    "datasource.to_json('WA_Leginfo.json')\n",
    "# End of WA_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of AR_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "## selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "## others\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import fitz\n",
    "\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# set up working directory\n",
    "working_directory = '/Volumes/SSD/AFRI/Data/Raw_Data/AR'\n",
    "print(os.path.abspath(working_directory))\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"./PDFs/\"\n",
    "print(os.path.abspath(download_dir))\n",
    "\n",
    "# Create a Firefox profile and configure preferences\n",
    "options = Options()\n",
    "\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path='../../geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "# Initialize the WebDriver with the specified path\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "years = []\n",
    "chapter_nums = []\n",
    "subtitles = []\n",
    "bill_nums = []\n",
    "filenames = []\n",
    "act_urls = []\n",
    "sessions = []\n",
    "broken_act_urls = []\n",
    "\n",
    "driver.get('https://www.arkleg.state.ar.us/Acts/SearchByRange?ddBienniumSession')\n",
    "# accept cookies button\n",
    "try:\n",
    "    driver.find_element(By.CSS_SELECTOR, 'a.btn').click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "session_elements = driver.find_elements(By.CSS_SELECTOR, '#ddBienniumSessionActsSearchByRange option')\n",
    "session_texts = []\n",
    "for session_text in session_elements:\n",
    "    session_texts.append(session_text.text)\n",
    "\n",
    "for session_text in session_texts[1:]:\n",
    "    print(session_text)\n",
    "    try:\n",
    "        # Attempt to convert the value to an integer\n",
    "        year = int(session_text.replace(' ','').split('-')[0])\n",
    "        if year > 2021:\n",
    "            pass\n",
    "        else:\n",
    "            select = Select(driver.find_element(By.CSS_SELECTOR, '#ddBienniumSessionActsSearchByRange'))\n",
    "            select.select_by_visible_text(session_text)\n",
    "            # clear starting act number and ending act number\n",
    "            starting_num = driver.find_element(By.CSS_SELECTOR, '#startAct')\n",
    "            starting_num.clear()\n",
    "            starting_num.send_keys(1)\n",
    "            ending_num = driver.find_element(By.CSS_SELECTOR, '#endAct')\n",
    "            ending_num.clear()\n",
    "            ending_num.send_keys(9999)\n",
    "            # startAct\n",
    "            search_button = driver.find_element(By.CSS_SELECTOR, 'div.form-row:nth-child(5) > button:nth-child(1)')\n",
    "            search_button.click()\n",
    "\n",
    "            time.sleep(3)\n",
    "\n",
    "            # for page 1\n",
    "            rows = driver.find_elements(By.CSS_SELECTOR, 'div.row.tableRow, div.row.tableRowAlt')\n",
    "            for row in rows:\n",
    "                #years.append(year)\n",
    "                #sessions.append(session_text)\n",
    "                chapter_num = row.find_element(By.CSS_SELECTOR, ':nth-child(1)').text\n",
    "                chapter_nums.append(chapter_num)\n",
    "                subtitles.append(row.text)\n",
    "                bill_nums.append(row.find_element(By.CSS_SELECTOR, ':nth-child(3)').text)\n",
    "                act_url = row.find_element(By.LINK_TEXT, 'PDF').get_attribute('href')\n",
    "                act_urls.append(act_url)\n",
    "\n",
    "                name = str(year) + str(session_text.split(',')[0].replace(' ', '').split('-')[1]) + \"chap\" + str(\n",
    "                    chapter_num) + \".pdf\"\n",
    "                filenames.append(name)\n",
    "\n",
    "                path = download_dir + name\n",
    "                print(os.path.abspath(path))\n",
    "\n",
    "                time.sleep(randint(1, 100) * 0.005)\n",
    "\n",
    "                response = requests.get(act_url)\n",
    "                # Ensure the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Write the content of the response to a file\n",
    "                    with open(path, 'wb') as file:\n",
    "                        file.write(response.content)\n",
    "                else:\n",
    "                    print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "                    broken_act_urls.append(act_url)\n",
    "            time.sleep(1)\n",
    "            # for page 2-n\n",
    "            page_num = int(driver.find_element(By.CSS_SELECTOR,'div.row:nth-child(25) > div:nth-child(1)').text.split(':')[0].split('of ')[1])\n",
    "            for num in range(2,page_num+1):\n",
    "                driver.find_element(By.LINK_TEXT,str(num)).click()\n",
    "                time.sleep(1)\n",
    "                rows = driver.find_elements(By.CSS_SELECTOR, 'div.row.tableRow, div.row.tableRowAlt')\n",
    "                for row in rows:\n",
    "                    years.append(year)\n",
    "                    sessions.append(session_text)\n",
    "                    chapter_num = row.find_element(By.CSS_SELECTOR, ':nth-child(1)').text\n",
    "                    chapter_nums.append(chapter_num)\n",
    "                    subtitles.append(row.find_element(By.CSS_SELECTOR, ':nth-child(2)').text)\n",
    "                    bill_nums.append(row.find_element(By.CSS_SELECTOR, ':nth-child(3)').text)\n",
    "                    act_url = row.find_element(By.LINK_TEXT, 'PDF').get_attribute('href')\n",
    "                    act_urls.append(act_url)\n",
    "\n",
    "                    name = str(year) + str(session_text.split(',')[0].replace(' ','').split('-')[1]) + \"chap\" + str(chapter_num) + \".pdf\"\n",
    "                    filenames.append(name)\n",
    "\n",
    "                    path = download_dir + name\n",
    "                    print(os.path.abspath(path))\n",
    "\n",
    "                    time.sleep(randint(1, 100) * 0.005)\n",
    "\n",
    "                    response = requests.get(act_url)\n",
    "                    # Ensure the request was successful\n",
    "                    if response.status_code == 200:\n",
    "                        # Write the content of the response to a file\n",
    "                        with open(path, 'wb') as file:\n",
    "                            file.write(response.content)\n",
    "                    else:\n",
    "                        print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "                        broken_act_urls.append(act_url)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "acttxts = []\n",
    "pgtxts = []\n",
    "path = \"/Users/long/Downloads/AR/\"\n",
    "files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "for file in files[:20]:\n",
    "    pdfFileObj = open(file, 'rb')\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "    numpages = pdfReader.numPages\n",
    "    for i in range(numpages):\n",
    "        pgtxt = []\n",
    "        pgtxt = pdfReader.getPage(i)\n",
    "        acttxt = ' '.join(pgtxts)\n",
    "    print(acttxt)\n",
    "    acttxts.append(acttxt)\n",
    "    pdfFileObj.close()\n",
    "\n",
    "\n",
    "act_txts = []\n",
    "path = \"./PDFs\"\n",
    "files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "for file in files:\n",
    "    doc = fitz.open(file)\n",
    "    pgtxts = []\n",
    "    for page in doc:\n",
    "        pgtxt = page.get_text()\n",
    "        pgtxts.append(pgtxt)\n",
    "        act_txt = ' '.join(pgtxts)\n",
    "    act_txts.append(act_txt)\n",
    "    doc.close()\n",
    "\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'original_text': act_txts,\n",
    "    'year':years,\n",
    "    'chapter_num':chapter_nums,\n",
    "    'subtitle':subtitles,\n",
    "    'bill_num':bill_nums,\n",
    "    'filename':filenames,\n",
    "    'act_url':act_urls,\n",
    "    'session':sessions\n",
    "})\n",
    "\n",
    "# save bill info into files\n",
    "datasource.to_excel('AR_Leginfo.xlsx')\n",
    "datasource.to_csv('AR_Leginfo.csv')\n",
    "datasource.to_pickle('AR_Leginfo.pkl')\n",
    "datasource.to_json('AR_Leginfo.json')\n",
    "print(\"done\")\n",
    "# End of AR_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of OH_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "## selenium\n",
    "import selenium\n",
    "print(selenium.__version__)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.support.ui import Select\n",
    "## others\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "import requests\n",
    "import fitz\n",
    "import re\n",
    "import datefinder\n",
    "\n",
    "# set up working directory\n",
    "working_directory = '/Volumes/SSD/AFRI/Data/Raw_Data/OH'\n",
    "print(os.path.abspath(working_directory))\n",
    "\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"./PDFs/\"\n",
    "print(os.path.abspath(download_dir))\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path = '../../geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "\n",
    "# empty list to store info\n",
    "\n",
    "act_urls_1997_2014 = []\n",
    "act_pdf_urls = []\n",
    "act_html_urls = []\n",
    "pdf_file_names = []\n",
    "html_file_names = []\n",
    "bill_nums = []\n",
    "chapter_nums = []\n",
    "years = []\n",
    "sessions = []\n",
    "broken_act_urls = []\n",
    "act_txts = []\n",
    "act_names = []\n",
    "names = []\n",
    "file_names = []\n",
    "summaries = []\n",
    "\n",
    "\n",
    "# Ohio lature Website: http://archives.legislature.state.oh.us/acts.cfm?GenAssem=122 1997-2014\n",
    "# https://www.legislature.ohio.gov/legislation/acts/131 after 2016\n",
    "\n",
    "# for 1997-2014\n",
    "\n",
    "for i in range(122,131):\n",
    "    print(i)\n",
    "\n",
    "    driver.get('http://archives.legislature.state.oh.us/acts.cfm?GenAssem=' + str(i))\n",
    "\n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, 'body'))\n",
    "    )\n",
    "\n",
    "    bill_elements = driver.find_elements(By.PARTIAL_LINK_TEXT,'Bill')\n",
    "\n",
    "    for element in bill_elements:\n",
    "        sessions.append(i)\n",
    "        bill_nums.append(element.text)\n",
    "        act_urls_1997_2014.append(element.get_attribute('href'))\n",
    "\n",
    "act_pdf_urls = []\n",
    "act_status_urls = []\n",
    "\n",
    "for url in act_urls_1997_2014:\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, 'body'))\n",
    "    )\n",
    "    #act_txts.append(driver.find_element(By.CSS_SELECTOR,'.bigPanel').text)\n",
    "\n",
    "    try:\n",
    "        act_pdf_url = driver.find_element(By.PARTIAL_LINK_TEXT, 'PDF').get_attribute('href')\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            act_pdf_url = driver.find_element(By.PARTIAL_LINK_TEXT, 'pdf').get_attribute('href')\n",
    "        except Exception as e2:\n",
    "            try:\n",
    "                act_pdf_url = driver.find_element(By.PARTIAL_LINK_TEXT, 'As Enrolled').get_attribute('href')\n",
    "            except Exception as e3:\n",
    "                act_pdf_url = url\n",
    "\n",
    "    act_pdf_urls.append(act_pdf_url)\n",
    "\n",
    "    act_status_url = driver.find_element(By.PARTIAL_LINK_TEXT,'Status ').get_attribute('href')\n",
    "\n",
    "    act_status_urls.append(act_status_url)\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'bill_num': bill_nums,\n",
    "    'session': sessions,\n",
    "    'act_html_url': act_urls_1997_2014,\n",
    "    'state': \"OH\",\n",
    "    'act_pdf_url': act_pdf_urls,\n",
    "    'act_status_url': act_status_urls\n",
    "})\n",
    "\n",
    "datasource.to_csv('OH_Leginfo_1997_2014.csv')\n",
    "\n",
    "datasource = pd.read_csv('OH_Leginfo_1997_2014.csv', index_col=0)\n",
    "\n",
    "for url in datasource['act_html_url']:\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, '.bigPanel'))\n",
    "    )\n",
    "    act_txts.append(driver.find_element(By.CSS_SELECTOR,'.bigPanel').text)\n",
    "    time.sleep(randint(1,100)*0.005)\n",
    "\n",
    "datasource['original_text'] =  act_txts\n",
    "\n",
    "datasource.to_csv('OH_Leginfo_1997_2014.csv')\n",
    "\n",
    "datasource = pd.read_csv('OH_Leginfo_1997_2014.csv', index_col=0)\n",
    "\n",
    "\n",
    "act_urls_2016 = []\n",
    "bill_nums_2016 = []\n",
    "names_2016 = []\n",
    "act_pdf_urls_2016 = []\n",
    "sessions_2016 = []\n",
    "\n",
    "for i in range(131,136):\n",
    "\n",
    "    print(i)\n",
    "\n",
    "    driver.get('https://www.legislature.ohio.gov/legislation/acts/' + str(i))\n",
    "\n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, 'body'))\n",
    "    )\n",
    "\n",
    "    bill_elements = driver.find_elements(By.PARTIAL_LINK_TEXT,'No.')\n",
    "\n",
    "    for element in bill_elements:\n",
    "        sessions_2016.append(i)\n",
    "        bill_nums_2016.append(element.text)\n",
    "        act_urls_2016.append(element.get_attribute('href'))\n",
    "\n",
    "        # Find the next sibling of the element\n",
    "        names_2016.append(element.find_element(By.XPATH, \"following-sibling::*[2]\").text)\n",
    "\n",
    "        grandparent_element = element.find_element(By.XPATH, \"../..\")\n",
    "\n",
    "        act_pdf_urls_2016.append(grandparent_element.find_element(By.PARTIAL_LINK_TEXT,'Download').get_attribute('href'))\n",
    "\n",
    "datasource_2016 = pd.DataFrame({\n",
    "    'session':sessions_2016,\n",
    "    'bill_num': bill_nums_2016,\n",
    "    'chapter_num': 'NA',\n",
    "    'act_pdf_url': act_pdf_urls_2016,\n",
    "    'act_html_url': act_urls_2016,\n",
    "    'state': \"OH\"\n",
    "})\n",
    "\n",
    "datasource_2016.to_csv('OH_Leginfo_2016.csv')\n",
    "\n",
    "\n",
    "\n",
    "datasource_2016 = pd.read_csv('OH_Leginfo_2016.csv', index_col= 0)\n",
    "\n",
    "file_names_20162 = []\n",
    "acttxts_20162 = []\n",
    "\n",
    "for index,row in datasource_2016.iterrows():\n",
    "    act_url = row['act_html_url']\n",
    "    driver.get(act_url)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        driver.find_element(By.LINK_TEXT,'As Enrolled').click()\n",
    "    except:\n",
    "        driver.find_element(By.LINK_TEXT, 'As Adopted by Senate').click()\n",
    "    time.sleep(5)\n",
    "    path = \"/Volumes/SSD/AFRI/Data/Raw_Data/OH/PDFs_2016new/\"\n",
    "    list_of_files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    print(latest_file)\n",
    "    # New file name\n",
    "    new_pdf_file_path = path + act_url.replace('/','').split('legislation')[1] + '.pdf'\n",
    "    file_names_20162.append(act_url.replace('/','').split('legislation')[1] + '.pdf')\n",
    "    # Rename the file\n",
    "    os.rename(latest_file, new_pdf_file_path)\n",
    "\n",
    "file_names_2016 = []\n",
    "for index,row in datasource_2016.iterrows():\n",
    "    act_url = row['act_html_url']\n",
    "    file_names_2016.append(act_url.replace('/','').split('legislation')[1] + '.pdf')\n",
    "\n",
    "datasource_2016['file_name'] = file_names_2016\n",
    "\n",
    "\n",
    "folder_path = \"./PDFs_2016new\"\n",
    "print(os.path.abspath(folder_path))\n",
    "\n",
    "files = glob.glob(os.path.join(folder_path, '*.pdf'))\n",
    "\n",
    "act_txts_2016 = []\n",
    "\n",
    "broken_files = []\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        doc = fitz.open(file)\n",
    "        pgtxts = []\n",
    "        for page in doc:\n",
    "            pgtxt = page.get_text()\n",
    "            pgtxts.append(pgtxt)\n",
    "            acttxt = ' '.join(pgtxts)\n",
    "        act_txts_2016.append(acttxt)\n",
    "        doc.close()\n",
    "    except:\n",
    "        act_txts_2016.append('NA')\n",
    "        broken_files.append(file)\n",
    "\n",
    "file_names_2016 = []\n",
    "\n",
    "for file in files:\n",
    "    file_names_2016.append(file.split(\"PDFs_2016new/\")[1])\n",
    "\n",
    "datasource_2016_pdf = pd.DataFrame({\n",
    "    'original_text':act_txts_2016,\n",
    "    'file_name': file_names_2016\n",
    "})\n",
    "\n",
    "datasource_2016 =  pd.merge(datasource_2016, datasource_2016_pdf, on='file_name', how='left')\n",
    "datasource_2016.to_csv('OH_Leginfo_2016.csv')\n",
    "datasource_2016 =  pd.read_csv('OH_Leginfo_2016.csv',index_col=0)\n",
    "\n",
    "datasource = pd.read_csv('OH_Leginfo_1997_2014.csv', index_col=0)\n",
    "datasource = datasource._append(datasource_2016, ignore_index=True)\n",
    "\n",
    "# cleaning\n",
    "bill_nums = []\n",
    "\n",
    "for num in datasource['bill_num']:\n",
    "    print(num)\n",
    "    if 'Bill' in num:\n",
    "        if 'Senate' in num:\n",
    "            bill_nums.append('SB' + num.split(' Bill ')[1])\n",
    "        else:\n",
    "            bill_nums.append('HB' + num.split(' Bill ')[1])\n",
    "    else:\n",
    "        bill_nums.append(num.replace('No.','').replace('.','').replace(' ',''))\n",
    "datasource['bill_num'] = bill_nums\n",
    "\n",
    "sessions = []\n",
    "for index, row in datasource.iterrows():\n",
    "    if pd.isna(row['session']):\n",
    "        session = row['act_html_url'].split('legislation/')[1][:3]\n",
    "        sessions.append(session)\n",
    "    else:\n",
    "        sessions.append(row['session'])\n",
    "\n",
    "datasource['session'] = sessions\n",
    "\n",
    "act_status_urls = []\n",
    "for index, row in datasource.iterrows():\n",
    "    if pd.isna(row['act_status_url']):\n",
    "        act_status_urls.append(row['act_html_url'] + '/status')\n",
    "    else:\n",
    "        act_status_urls.append(row['act_status_url'])\n",
    "\n",
    "datasource['act_status_url'] = act_status_urls\n",
    "\n",
    "years = []\n",
    "for session in datasource['session']:\n",
    "    if 122 <= int(session) <= 135:\n",
    "        # Calculate the year range based on the session number\n",
    "        start_year = 1997 + (int(session) - 122) * 2\n",
    "        end_year = start_year + 1\n",
    "        year_range = f\"{start_year}-{end_year}\"\n",
    "        years.append(year_range)\n",
    "    else:\n",
    "        years.append(\"Unknown Session\")\n",
    "datasource['year'] =  years\n",
    "\n",
    "\n",
    "state = 'OH'\n",
    "# Read the CSV file into a DataFrame\n",
    "datasource = pd.read_csv(f'/Volumes/SSD/AFRI/Data/Cleaned/{state}_leginfo_clean.csv', index_col=0)\n",
    "\n",
    "status_file_urls = []\n",
    "\n",
    "for link in datasource['link']:\n",
    "    print(link)\n",
    "    if 'archives' in link:\n",
    "        status_url = 'http://archives.legislature.state.oh.us/lsc/statusreport' + link.split('ID=')[1][:3] + '/' +  link.split('ID=')[1] + '_statusreport.pdf'\n",
    "    else:\n",
    "        status_url = link + '/votes'\n",
    "    status_file_urls.append(status_url)\n",
    "year_info = []\n",
    "for url in status_file_urls:\n",
    "    if 'archives' in url:\n",
    "        print(url)\n",
    "        driver.execute_script(f\"window.location.href = '{url}'\")\n",
    "        time.sleep(1)\n",
    "        year_info.append('')\n",
    "    else:\n",
    "        driver.get('https://www.legislature.ohio.gov/legislation/134/hb575/votes')\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'body'))\n",
    "        )\n",
    "        year_info.append(driver.find_element(By.CSS_SELECTOR, 'body').text)\n",
    "\n",
    "link = 'http://archives.legislature.state.oh.us/bills.cfm?ID=130_SB_378'\n",
    "'https://www.legislature.ohio.gov/legislation/134/hb687/status'\n",
    "\n",
    "#http://archives.legislature.state.oh.us/lsc/statusreport125/125_HB_1_statusreport.pdf\n",
    "#http://archives.legislature.state.oh.us/lsc/statusreport122/122_HB_1_statusreport.pdf\n",
    "#http://archives.legislature.state.oh.us/lsc/statusreport\n",
    "#http://archives.legislature.state.oh.us/bills.cfm?ID=122_HB_1\n",
    "#https://www.legislature.ohio.gov/legislation/134/hb687statusreport.pdf\n",
    "#https://www.legislature.ohio.gov/legislation/134/hb575/votes\n",
    "\n",
    "files = glob.glob(os.path.join('/Users/long/Downloads', '*.pdf'))\n",
    "\n",
    "status_info = []\n",
    "\n",
    "for file in files:\n",
    "    doc = fitz.open(file)\n",
    "    pgtxts = []\n",
    "    for page in doc:\n",
    "        pgtxt = page.get_text()\n",
    "        pgtxts.append(pgtxt)\n",
    "        acttxt = ' '.join(pgtxts)\n",
    "    status_info.append(acttxt)\n",
    "    doc.close()\n",
    "\n",
    "df_status_info = pd.DataFrame({\n",
    "    'file_name': files,\n",
    "    'status': status_info\n",
    "})\n",
    "\n",
    "df_status_info['file_name'] = df_status_info['file_name'].str.replace('/Users/long/Downloads/', '')\n",
    "\n",
    "\n",
    "file_names = []\n",
    "for url in status_file_urls:\n",
    "    try:\n",
    "        file_names.append(url.split('/statusreport')[1][4:])\n",
    "    except:\n",
    "        file_names.append(url)\n",
    "\n",
    "datasource['file_name'] = file_names\n",
    "\n",
    "datasource = datasource.merge(df_status_info, on='file_name', how='left')\n",
    "\n",
    "datasource.drop(['status_x'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "datasource.to_excel('OH_Leginfo.xlsx')\n",
    "datasource.to_csv('OH_Leginfo.csv')\n",
    "datasource.to_pickle('OH_Leginfo.pkl')\n",
    "datasource.to_json('OH_Leginfo.json')\n",
    "\n",
    "datasource.to_csv(\"OH_leginfo_clean.csv\")\n",
    "\n",
    "datasource = pd.read_csv('OH_Leginfo.csv')\n",
    "\n",
    "datasource.drop()\n",
    "\n",
    "datasource.drop('Unnamed: 0.2', axis=1, inplace=True)\n",
    "\n",
    "for index, row in datasource.iterrows():\n",
    "    if 'pdf' not in row['file_name']:\n",
    "        print(row['file_name'])\n",
    "        driver.get(row['file_name'])\n",
    "\n",
    "\n",
    "status_df = pd.read_excel('status-report-of-legislation.xlsx')\n",
    "\n",
    "\n",
    "datasource['key'] = datasource['link'].str.replace('https://www.legislature.ohio.gov/legislation/', '').str.replace('/','')\n",
    "\n",
    "datasource = datasource.merge(status_df, on='key', how='left')\n",
    "\n",
    "\n",
    "years = []\n",
    "\n",
    "for status in datasource['status_y']:\n",
    "    #print(status)\n",
    "    try:\n",
    "        year = status.split('Consideration')[1].replace('A','').replace('*','').replace(' ','')[:10].replace('\\n','')[-2:]\n",
    "        if int(year) > 30:\n",
    "            years.append(int('19' + year))\n",
    "        else:\n",
    "            years.append(int('20' + year))\n",
    "        print(year)\n",
    "    except:\n",
    "        years.append(status)\n",
    "\n",
    "datasource['year'] = datasource['year'].astype('int')\n",
    "\n",
    "\n",
    "datasource['year'] = years\n",
    "\n",
    "datasource.loc[datasource['year'].isna(), 'year'] = datasource['introduced']\n",
    "\n",
    "\n",
    "nan_df = datasource[datasource['year'].isna()]\n",
    "\n",
    "datasource['year'][492] = '1999'\n",
    "datasource['year'][493] = '1999'\n",
    "datasource['year'][566] = '2001'\n",
    "\n",
    "datasource = datasource.iloc[:, :-3]\n",
    "\n",
    "for row in datasource[:10].iterrows():\n",
    "    #print(row[1]['key'])\n",
    "    try:\n",
    "        row[1]('introduced') == row[1]('status_y').split('Consideration')[1].replace('A','').replace('*','').replace(' ','')[:10].replace(' ','')[-3:].replace('/','')\n",
    "    except:\n",
    "        print(row[1]('status_y')\n",
    "\n",
    "\n",
    "        status_df['key'] = status_df['session'].astype(str) + status_df['session'].str.replace('.','').lower().astype(str) + status_df['Number'].astype(str)\n",
    "        # End of OH_Leginfo.py\n",
    "\n",
    "\n",
    "        # Start of ND_Leginfo.py\n",
    "\n",
    "        # import libraries\n",
    "        import pandas as pd\n",
    "        import os\n",
    "        import re\n",
    "        from nltk.corpus import stopwords\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        from word_forms.word_forms import get_word_forms\n",
    "        import datefinder\n",
    "        from dateutil import parser\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.firefox.service import Service\n",
    "        from selenium.webdriver.firefox.options import Options\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from selenium.webdriver.common.keys import Keys\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.common.exceptions import NoSuchElementException\n",
    "        from selenium.common.exceptions import TimeoutException\n",
    "        from selenium.webdriver.support.ui import Select\n",
    "        ## others\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        import datefinder\n",
    "        import calendar\n",
    "        import os\n",
    "        import unittest\n",
    "        from random import randint\n",
    "        import PyPDF2\n",
    "        import glob\n",
    "        import pickle\n",
    "        import inflect\n",
    "        import time\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        import fitz\n",
    "\n",
    "\n",
    "        # Set the working directory\n",
    "        os.chdir('/Volumes/SSD/AFRI/Data/')\n",
    "\n",
    "        # Verify the current working directory\n",
    "        print(os.getcwd())\n",
    "\n",
    "\n",
    "        urls=[]\n",
    "        sessionurls = []\n",
    "        driver.get(\"https://ndlegis.gov/session-laws/\")\n",
    "        WebDriverWait(driver, 300).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.view-content ul li a\"))\n",
    "        )\n",
    "        sessions = driver.find_elements_by_css_selector('div.view-content ul li a')\n",
    "        for index,session in enumerate(sessions):\n",
    "            print(index)\n",
    "        print(session.text)\n",
    "        sessionurl = session.get_attribute('href')\n",
    "        sessionurls.append(sessionurl)\n",
    "\n",
    "        for sessionurl in sessionurls[:33]:\n",
    "            driver.get(sessionurl)\n",
    "        sleep(1)\n",
    "        driver.find_element_by_link_text(\"Chapter Categories\").click()\n",
    "        try:\n",
    "            chapters = driver.find_elements_by_css_selector('div.field-items ul li a')\n",
    "            for chapter in chapters:\n",
    "                url = chapter.get_attribute('href')\n",
    "                urls.append(url)\n",
    "        except:\n",
    "            print(sessionurl)\n",
    "print(\"urls are done\")\n",
    "\n",
    "acttxts = []\n",
    "for idx,url in enumerate(urls):\n",
    "    driver.get(url)\n",
    "    sleep(1)\n",
    "    path = \"C:/Users/longy/OneDrive/Projects/AFRI/data/ND/\"\n",
    "    list_of_files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    print(latest_file)\n",
    "    with open(os.path.join(os.getcwd(), latest_file), 'r') as f:  # open in readonly mode\n",
    "        # creating a pdf File object of original pdf\n",
    "        pdfFileObj = open(latest_file, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        pagenumber = pdfReader.numPages\n",
    "        acttxt = []\n",
    "        pgtxts = []\n",
    "        for p in range(pagenumber):\n",
    "            pageObj = pdfReader.getPage(p)\n",
    "            pgtxt = pageObj.extractText()\n",
    "            pgtxts.append(pgtxt)\n",
    "            acttxt = '\\n'.join(pgtxts)\n",
    "    acttxts.append(acttxt)\n",
    "path = \"C:/Users/longy/OneDrive/Projects/AFRI/data/ND/\"\n",
    "list_of_files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "#latest_file = max(list_of_files, key=os.path.getctime)\n",
    "acttxts = []\n",
    "for idx,file in enumerate(list_of_files):\n",
    "    with open(os.path.join(os.getcwd(), file), 'r') as f:  # open in readonly mode\n",
    "        # creating a pdf File object of original pdf\n",
    "        pdfFileObj = open(file, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        pagenumber = pdfReader.numPages\n",
    "        acttxt = []\n",
    "        pgtxts = []\n",
    "        for p in range(pagenumber):\n",
    "            pageObj = pdfReader.getPage(p)\n",
    "            pgtxt = pageObj.extractText()\n",
    "            pgtxts.append(pgtxt)\n",
    "            acttxt = '  '.join(pgtxts)\n",
    "    acttxts.append(acttxt)\n",
    "datasource = pd.DataFrame({\n",
    "    'Link to full text':urls,\n",
    "    'Full text': acttxts\n",
    "})\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'Full text': acttxts\n",
    "})\n",
    "datasource.to_excel('ND_Leginfo.xlsx')\n",
    "datasource.to_csv('ND_Leginfo.csv')\n",
    "datasource.to_pickle('ND_Leginfo.pkl')\n",
    "datasource.to_json('ND_Leginfo.json')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open('ND_Leginfo.pkl', 'rb') as k:\n",
    "    data = pickle.load(k)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def get_mdls_where_from(file_path):\n",
    "    try:\n",
    "        result = subprocess.run(['mdls', '-name', 'kMDItemWhereFroms', '-raw', file_path],\n",
    "                                stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        if result.returncode == 0:\n",
    "            where_from_info = result.stdout.strip()\n",
    "            if where_from_info:\n",
    "                # The output might be a JSON-like list, so we print it directly\n",
    "                return where_from_info\n",
    "            else:\n",
    "                print(\"No 'Where from' information available.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"Error running mdls command: {result.stderr}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "acttxts = []\n",
    "urls = []\n",
    "path = \"Raw_Data/ND/PDFs\"\n",
    "files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "for file in files:\n",
    "    print(file)\n",
    "    doc = fitz.open(file)\n",
    "    pgtxts = []\n",
    "    for page in doc:\n",
    "        pgtxt = page.get_text()\n",
    "        pgtxts.append(pgtxt)\n",
    "        acttxt = ' '.join(pgtxts)\n",
    "    acttxts.append(acttxt)\n",
    "    doc.close()\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'original_text': acttxts,\n",
    "})\n",
    "\n",
    "datasource.to_csv('ND_Leginfo_AAEA.csv')\n",
    "\n",
    "datasource = pd.read_csv('ND_Leginfo_AAEA.csv')\n",
    "\n",
    "\n",
    "# New list to hold the split strings\n",
    "acttxts = []\n",
    "\n",
    "# Loop through each string in the example list\n",
    "for file in datasource['original_text']:\n",
    "    # Split the string by 'Filed'\n",
    "    split_parts = file.split('\\nFiled')\n",
    "    # Extend the new list with the resulting parts\n",
    "    acttxts.extend(split_parts)\n",
    "\n",
    "# Print the new list\n",
    "print(new_list)\n",
    "\n",
    "\n",
    "acttxts\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'original_text': acttxts,\n",
    "})\n",
    "\n",
    "df['original_text'][0]\n",
    "\n",
    "df['year'] = df['original_text'].apply(lambda x: x[-10:].replace('\\n','').replace(' ','')[-4:])\n",
    "\n",
    "df = df[df['original_text'].str.len() > 100]\n",
    "\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "\n",
    "df = df.dropna(subset=['year'])\n",
    "\n",
    "df['year'] = df['year'].astype(int)\n",
    "\n",
    "df = df[(df['year'] >= 1975) & (df['year'] <= 2021)]\n",
    "\n",
    "df.to_csv(\"Raw_Data/Cleaned/ND_leginfo_clean.csv\")\n",
    "\n",
    "df.to_csv(\"Raw_Data/ND/ND_leginfo_clean.csv\")\n",
    "\n",
    "\n",
    "# End of ND_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of AK_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "## selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "## others\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "#import fitz\n",
    "\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# set up working directory\n",
    "working_directory = '/Volumes/SSD/AFRI/Data/'\n",
    "print(os.path.abspath(working_directory))\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"./PDFs/\"\n",
    "print(os.path.abspath(download_dir))\n",
    "\n",
    "# Create a Firefox profile and configure preferences\n",
    "options = Options()\n",
    "\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path='./geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "# Initialize the WebDriver with the specified path\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "\n",
    "years = []\n",
    "chapter_nums = []\n",
    "subtitles = []\n",
    "bill_nums = []\n",
    "filenames = []\n",
    "act_urls = []\n",
    "sessions = []\n",
    "broken_act_urls = []\n",
    "\n",
    "driver.get('https://www.arkleg.state.ar.us/Acts/SearchByRange?ddBienniumSession')\n",
    "# accept cookies button\n",
    "try:\n",
    "    driver.find_element(By.CSS_SELECTOR, 'a.btn').click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "session_elements = driver.find_elements(By.CSS_SELECTOR, '#ddBienniumSessionActsSearchByRange option')\n",
    "session_texts = []\n",
    "for session_text in session_elements:\n",
    "    session_texts.append(session_text.text)\n",
    "\n",
    "for session_text in session_texts[1:]:\n",
    "    print(session_text)\n",
    "    try:\n",
    "        # Attempt to convert the value to an integer\n",
    "        year = int(session_text.replace(' ','').split('-')[0])\n",
    "        if year > 2021:\n",
    "            pass\n",
    "        else:\n",
    "            select = Select(driver.find_element(By.CSS_SELECTOR, '#ddBienniumSessionActsSearchByRange'))\n",
    "            select.select_by_visible_text(session_text)\n",
    "            # clear starting act number and ending act number\n",
    "            starting_num = driver.find_element(By.CSS_SELECTOR, '#startAct')\n",
    "            starting_num.clear()\n",
    "            starting_num.send_keys(1)\n",
    "            ending_num = driver.find_element(By.CSS_SELECTOR, '#endAct')\n",
    "            ending_num.clear()\n",
    "            ending_num.send_keys(9999)\n",
    "            # startAct\n",
    "            search_button = driver.find_element(By.CSS_SELECTOR, 'div.form-row:nth-child(5) > button:nth-child(1)')\n",
    "            search_button.click()\n",
    "\n",
    "            time.sleep(3)\n",
    "\n",
    "            # for page 1\n",
    "            rows = driver.find_elements(By.CSS_SELECTOR, 'div.row.tableRow, div.row.tableRowAlt')\n",
    "            for row in rows:\n",
    "                #years.append(year)\n",
    "                #sessions.append(session_text)\n",
    "                chapter_num = row.find_element(By.CSS_SELECTOR, ':nth-child(1)').text\n",
    "                chapter_nums.append(chapter_num)\n",
    "                subtitles.append(row.text)\n",
    "                bill_nums.append(row.find_element(By.CSS_SELECTOR, ':nth-child(3)').text)\n",
    "                act_url = row.find_element(By.LINK_TEXT, 'PDF').get_attribute('href')\n",
    "                act_urls.append(act_url)\n",
    "\n",
    "                name = str(year) + str(session_text.split(',')[0].replace(' ', '').split('-')[1]) + \"chap\" + str(\n",
    "                    chapter_num) + \".pdf\"\n",
    "                filenames.append(name)\n",
    "\n",
    "                path = download_dir + name\n",
    "                print(os.path.abspath(path))\n",
    "\n",
    "                time.sleep(randint(1, 100) * 0.005)\n",
    "\n",
    "                response = requests.get(act_url)\n",
    "                # Ensure the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Write the content of the response to a file\n",
    "                    with open(path, 'wb') as file:\n",
    "                        file.write(response.content)\n",
    "                else:\n",
    "                    print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "                    broken_act_urls.append(act_url)\n",
    "            time.sleep(1)\n",
    "            # for page 2-n\n",
    "            page_num = int(driver.find_element(By.CSS_SELECTOR,'div.row:nth-child(25) > div:nth-child(1)').text.split(':')[0].split('of ')[1])\n",
    "            for num in range(2,page_num+1):\n",
    "                driver.find_element(By.LINK_TEXT,str(num)).click()\n",
    "                time.sleep(1)\n",
    "                rows = driver.find_elements(By.CSS_SELECTOR, 'div.row.tableRow, div.row.tableRowAlt')\n",
    "                for row in rows:\n",
    "                    years.append(year)\n",
    "                    sessions.append(session_text)\n",
    "                    chapter_num = row.find_element(By.CSS_SELECTOR, ':nth-child(1)').text\n",
    "                    chapter_nums.append(chapter_num)\n",
    "                    subtitles.append(row.find_element(By.CSS_SELECTOR, ':nth-child(2)').text)\n",
    "                    bill_nums.append(row.find_element(By.CSS_SELECTOR, ':nth-child(3)').text)\n",
    "                    act_url = row.find_element(By.LINK_TEXT, 'PDF').get_attribute('href')\n",
    "                    act_urls.append(act_url)\n",
    "\n",
    "                    name = str(year) + str(session_text.split(',')[0].replace(' ','').split('-')[1]) + \"chap\" + str(chapter_num) + \".pdf\"\n",
    "                    filenames.append(name)\n",
    "\n",
    "                    path = download_dir + name\n",
    "                    print(os.path.abspath(path))\n",
    "\n",
    "                    time.sleep(randint(1, 100) * 0.005)\n",
    "\n",
    "                    response = requests.get(act_url)\n",
    "                    # Ensure the request was successful\n",
    "                    if response.status_code == 200:\n",
    "                        # Write the content of the response to a file\n",
    "                        with open(path, 'wb') as file:\n",
    "                            file.write(response.content)\n",
    "                    else:\n",
    "                        print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "                        broken_act_urls.append(act_url)\n",
    "    except:\n",
    "        pass\n",
    "acttxts = []\n",
    "pgtxts = []\n",
    "path = \"/Users/long/Downloads/AR/\"\n",
    "files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "for file in files[:20]:\n",
    "    pdfFileObj = open(file, 'rb')\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "    numpages = pdfReader.numPages\n",
    "    for i in range(numpages):\n",
    "        pgtxt = []\n",
    "        pgtxt = pdfReader.getPage(i)\n",
    "        acttxt = ' '.join(pgtxts)\n",
    "    print(acttxt)\n",
    "    acttxts.append(acttxt)\n",
    "    pdfFileObj.close()\n",
    "\n",
    "\n",
    "act_txts = []\n",
    "path = \"./PDFs\"\n",
    "files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "for file in files:\n",
    "    doc = fitz.open(file)\n",
    "    pgtxts = []\n",
    "    for page in doc:\n",
    "        pgtxt = page.get_text()\n",
    "        pgtxts.append(pgtxt)\n",
    "        act_txt = ' '.join(pgtxts)\n",
    "    act_txts.append(act_txt)\n",
    "    doc.close()\n",
    "\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'original_text': act_txts,\n",
    "    'year':years,\n",
    "    'chapter_num':chapter_nums,\n",
    "    'subtitle':subtitles,\n",
    "    'bill_num':bill_nums,\n",
    "    'filename':filenames,\n",
    "    'act_url':act_urls,\n",
    "    'session':sessions\n",
    "})\n",
    "\n",
    "# save bill info into files\n",
    "datasource.to_excel('AR_Leginfo.xlsx')\n",
    "datasource.to_csv('AR_Leginfo.csv')\n",
    "datasource.to_pickle('AR_Leginfo.pkl')\n",
    "datasource.to_json('AR_Leginfo.json')\n",
    "print(\"done\")\n",
    "# End of AK_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of DC_Leginfo.py\n",
    "\n",
    "# %%\n",
    "# import libraries\n",
    "## selenium\n",
    "import selenium\n",
    "\n",
    "print(selenium.__version__)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.support.ui import Select\n",
    "## others\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "import requests\n",
    "import fitz\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# set up working directory\n",
    "working_directory = '/Volumes/SSD/AFRI/Data/Raw_Data/DC'\n",
    "print(os.path.abspath(working_directory))\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"./PDFs/\"\n",
    "print(os.path.abspath(download_dir))\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path = '../../geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "# %%\n",
    "\n",
    "years = []\n",
    "chapter_nums = []\n",
    "subtitles = []\n",
    "bill_nums = []\n",
    "filenames = []\n",
    "act_urls = []\n",
    "sessions = []\n",
    "broken_act_urls = []\n",
    "act_txts = []\n",
    "expired_dates = []\n",
    "\n",
    "# go the website and select Bill under the Legislation Category; enated, expired, Official law under the status, click download\n",
    "# https://lims.dccouncil.gov/searchresult/currentPage=1&documentSearch=false&refinBy[0]=statusId%3A%20Expired&refinBy[1]=statusId%3A%20Enacted&refinBy[2]=statusId%3A%20Official%20Law&refinBy[3]=legislationCategoryId%3A%20Bill&searchString=\n",
    "# download '⁦/Volumes/SSD/AFRI/Data/Raw_Data/DC/PDFs⁩'\n",
    "\n",
    "\n",
    "driver.get('https://lims.dccouncil.gov/searchresult/documentSearch=false&searchString=')\n",
    "\n",
    "#More under Legislation Category\n",
    "driver.find_element(By.CSS_SELECTOR, 'div.widget:nth-child(2) > div:nth-child(3) > a:nth-child(1) > span:nth-child(1)').click()\n",
    "# select bill\n",
    "driver.find_element(By.CSS_SELECTOR, 'div.widget:nth-child(2) > div:nth-child(2) > ul:nth-child(1) > li:nth-child(2) > div:nth-child(1) > label:nth-child(2)').click()\n",
    "\n",
    "#More under Status\n",
    "driver.find_element(By.CSS_SELECTOR, 'div.widget:nth-child(4) > div:nth-child(3) > a:nth-child(1) > span:nth-child(1)').click()\n",
    "# select Enacted\n",
    "driver.find_element(By.CSS_SELECTOR, 'div.widget:nth-child(4) > div:nth-child(2) > ul:nth-child(1) > li:nth-child(1) > div:nth-child(1) > label:nth-child(2)').click()\n",
    "# select Expired\n",
    "driver.find_element(By.CSS_SELECTOR, 'div.widget:nth-child(4) > div:nth-child(2) > ul:nth-child(1) > li:nth-child(2) > div:nth-child(1) > label:nth-child(2)').click()\n",
    "# select Official Law\n",
    "driver.find_element(By.CSS_SELECTOR, 'div.widget:nth-child(4) > div:nth-child(2) > ul:nth-child(1) > li:nth-child(4) > div:nth-child(1) > label:nth-child(2)').click()\n",
    "\n",
    "# more period\n",
    "driver.find_element(By.CSS_SELECTOR, 'div.widget:nth-child(1) > div:nth-child(3) > a:nth-child(1) > span:nth-child(1)').click()\n",
    "\n",
    "# select period and download\n",
    "\n",
    "period_elements = driver.find_elements(By.CSS_SELECTOR,'div.widget:nth-child(1) > div:nth-child(2) > ul:nth-child(1) > li > div:nth-child(1) > label:nth-child(2)')\n",
    "for element in period_elements:\n",
    "\n",
    "    element.click()\n",
    "\n",
    "    driver.find_element(By.LINK_TEXT, 'Download').click()\n",
    "\n",
    "    time.sleep(30)\n",
    "\n",
    "    element.click()\n",
    "\n",
    "# Initialize an empty DataFrame to hold all appended data\n",
    "datasource = pd.DataFrame()\n",
    "\n",
    "# Iterate over all files in the specified directory\n",
    "for file in os.listdir(download_dir):\n",
    "\n",
    "    print(file)\n",
    "\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(download_dir, file)\n",
    "\n",
    "    # Check if the file is an Excel file\n",
    "    if file.endswith('.xlsx'):\n",
    "        try:\n",
    "            # Read the Excel file into a DataFrame\n",
    "            file_df = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "            # Append the DataFrame to the master DataFrame\n",
    "            datasource = pd.concat([df, file_df], ignore_index=True)\n",
    "        except:\n",
    "            print('not working')\n",
    "\n",
    "datasource = datasource.drop_duplicates()\n",
    "\n",
    "datasource['year'] = datasource['Introduction Date'].str[-4:]\n",
    "\n",
    "datasource.sort_values(by='year', inplace=True)\n",
    "\n",
    "#datasource = datasource[pd.notna(datasource['Act/Res Number'])]\n",
    "\n",
    "# Reset the index\n",
    "datasource.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for index, row in datasource.iterrows():\n",
    "    print(index)\n",
    "    url = 'https://lims.dccouncil.gov/Legislation/' + row['Legislation Number']\n",
    "    act_urls.append(url)\n",
    "    try:\n",
    "        broken_act_urls.append('NA')\n",
    "        driver.get(url)\n",
    "        time.sleep(randint(1,5)*0.01)\n",
    "\n",
    "        session_elements = WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'body'))\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            driver.find_element(By.LINK_TEXT, 'Enrollment').click()\n",
    "        except NoSuchElementException:\n",
    "            print(\"Enrollment link not found, trying Introduction link...\")\n",
    "            try:\n",
    "                driver.find_element(By.PARTIAL_LINK_TEXT, 'View Introduction').click()\n",
    "            except NoSuchElementException:\n",
    "                print(\"Introduction link not found, trying download icon...\")\n",
    "                try:\n",
    "                    # Assuming 'i.download.icon' is a CSS class, not link text\n",
    "                    driver.find_element(By.CSS_SELECTOR, 'i.download.icon').click()\n",
    "                except NoSuchElementException:\n",
    "                    print(\"Download icon not found. No action could be taken.\")\n",
    "                    try:\n",
    "                        # Assuming 'i.download.icon' is a CSS class, not link text\n",
    "                        driver.find_element(By.CSS_SELECTOR, '.download').click()\n",
    "                    except NoSuchElementException:\n",
    "                        print(\"Download icon not found. No action could be taken.\")\n",
    "    except:\n",
    "        broken_act_urls.append(url)\n",
    "\n",
    "\n",
    "df = pd.read_csv('/Volumes/SSD/AFRI/Data/Raw_Data/DC/DC_Leginfo_broken_urls.csv')\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "broken_act_urls = []\n",
    "\n",
    "for url in df['urls']:\n",
    "    print(url)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        broken_act_urls.append('NA')\n",
    "\n",
    "        time.sleep(randint(1,5)*0.01)\n",
    "\n",
    "        session_elements = WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'body'))\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            driver.find_element(By.LINK_TEXT, 'Enrollment').click()\n",
    "        except NoSuchElementException:\n",
    "            print(\"Enrollment link not found, trying Introduction link...\")\n",
    "            try:\n",
    "                driver.find_element(By.PARTIAL_LINK_TEXT, 'View Introduction').click()\n",
    "            except NoSuchElementException:\n",
    "                print(\"Introduction link not found, trying download icon...\")\n",
    "                try:\n",
    "                    # Assuming 'i.download.icon' is a CSS class, not link text\n",
    "                    driver.find_element(By.CSS_SELECTOR, 'i.download.icon').click()\n",
    "                except NoSuchElementException:\n",
    "                    print(\"Download icon not found. No action could be taken.\")\n",
    "                    try:\n",
    "                        # Assuming 'i.download.icon' is a CSS class, not link text\n",
    "                        driver.find_element(By.CSS_SELECTOR, '.download').click()\n",
    "                    except NoSuchElementException:\n",
    "                        print(\"Download icon not found. No action could be taken.\")\n",
    "    except:\n",
    "        broken_act_urls.append(url)\n",
    "\n",
    "\n",
    "\n",
    "act_txts = []\n",
    "\n",
    "path = \"/Volumes/SSD/AFRI/Data/Raw_Data/DC/PDFs\"\n",
    "files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "\n",
    "for file in files:\n",
    "    doc = fitz.open(file)\n",
    "    pgtxts = []\n",
    "    for page in doc:\n",
    "        pgtxt = page.get_text()\n",
    "        pgtxts.append(pgtxt)\n",
    "        act_txt = ' '.join(pgtxts)\n",
    "    act_txts.append(act_txt)\n",
    "    doc.close()\n",
    "\n",
    "file_df = pd.DataFrame({\n",
    "    'file_name': files,\n",
    "    'original_text': act_txts\n",
    "})\n",
    "\n",
    "\n",
    "bill_nums = []\n",
    "\n",
    "for name in file_df['file_name']:\n",
    "    result = re.search(r'B\\d+-\\d+', name)\n",
    "    if result:\n",
    "        extracted_text = result.group(0)\n",
    "        print(extracted_text)\n",
    "        bill_nums.append(extracted_text)\n",
    "    else:\n",
    "        bill_nums.append('NA')\n",
    "\n",
    "file_df['bill_name'] = bill_nums\n",
    "\n",
    "datasource = pd.read_csv('/Volumes/SSD/AFRI/Data/Raw_Data/DC/DC_Leginfo.csv', index_col=0)\n",
    "\n",
    "datasource = pd.merge(datasource, file_df, left_on='Legislation Number', right_on='bill_name', how='left')\n",
    "\n",
    "# Drop rows where 'file_name' is NaN\n",
    "datasource.dropna(subset=['file_name'], inplace=True)\n",
    "\n",
    "#  reset the index after dropping rows\n",
    "datasource.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.drop(columns=['bill_name','Status', 'Legislation Number'], inplace=True)\n",
    "\n",
    "df.rename(columns={'Act/Res Number': 'act_num'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "datasource.to_excel('DC_Leginfo.xlsx')\n",
    "datasource.to_csv('DC_Leginfo.csv')\n",
    "datasource.to_pickle('DC_Leginfo.pkl')\n",
    "datasource.to_json('DC_Leginfo.json')\n",
    "\n",
    "\n",
    "# End of DC_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of MN_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "## selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "## others\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import fitz\n",
    "\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# set up working directory\n",
    "working_directory = '/Volumes/SSD/AFRI/Data/Raw_Data/MN'\n",
    "print(os.path.abspath(working_directory))\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(working_directory)\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Specify the desired download directory\n",
    "download_dir = \"./PDFs/\"\n",
    "print(os.path.abspath(download_dir))\n",
    "\n",
    "# Create a Firefox profile and configure preferences\n",
    "options = Options()\n",
    "\n",
    "\n",
    "# Initialize the Firefox driver with the specified options\n",
    "executable_path='../../geckodriver'\n",
    "print(os.path.abspath(executable_path))\n",
    "\n",
    "service = Service(executable_path)\n",
    "driver = webdriver.Firefox(service = service)\n",
    "\n",
    "# session years: 1975-2021\n",
    "\n",
    "file_names = []\n",
    "act_urls = []\n",
    "bill_nums = []\n",
    "chapter_nums = []\n",
    "years = []\n",
    "sessions = []\n",
    "broken_act_urls = []\n",
    "act_txts = []\n",
    "\n",
    "for index, year in enumerate(range(1975, 2022)):\n",
    "\n",
    "    print(year)\n",
    "\n",
    "    for session_num in range(0,3):\n",
    "        # there might be extra two sessions in one year.\n",
    "        url = \"https://www.revisor.mn.gov/laws/\" + str(year) + \"/\"+str(session_num)+\"/\"\n",
    "\n",
    "        print(url)\n",
    "\n",
    "        if session_num == 0:\n",
    "            session = \"rs\"\n",
    "            #\"regular_session\"\n",
    "        elif session_num == 1:\n",
    "            session = \"fs\"\n",
    "            #\"1st special session\"\n",
    "        else:\n",
    "            session = \"ss\"\n",
    "            #\"2nd special session\"\n",
    "        print(session)\n",
    "\n",
    "        try:\n",
    "            # Open the website in Chrome\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            try:\n",
    "                # Wait for the page to load and find all the chapter links\n",
    "                chapter_elements = WebDriverWait(driver, 60).until(\n",
    "                    EC.presence_of_all_elements_located((By.PARTIAL_LINK_TEXT, \"Chapter\"))\n",
    "                )\n",
    "\n",
    "                chapter_urls = []\n",
    "\n",
    "                for element in chapter_elements:\n",
    "                    bill_nums.append(element.find_element(By.XPATH,'../following-sibling::*').text)\n",
    "                    chapter_nums.append(element.find_element(By.XPATH,'..').text.replace(\" \",\"\"))\n",
    "                    chapter_urls.append(element.get_attribute(\"href\"))\n",
    "\n",
    "                for index,chapter_url in enumerate(chapter_urls, start=1):\n",
    "\n",
    "                    print(index)\n",
    "\n",
    "                    print(chapter_url)\n",
    "\n",
    "                    years.append(year)\n",
    "\n",
    "                    sessions.append(session)\n",
    "\n",
    "                    if year < 1983:\n",
    "                        act_url = chapter_url\n",
    "                        act_txts.append(\"\")\n",
    "                    else:\n",
    "                        driver.get(chapter_url)\n",
    "\n",
    "                        WebDriverWait(driver, 60).until(\n",
    "                            EC.presence_of_element_located(\n",
    "                                (By.LINK_TEXT,\n",
    "                                 \"PDF\"))\n",
    "                        )\n",
    "                        # If accessing the URL directly fails, get the URL from the link with text \"PDF\" and navigate to it\n",
    "                        pdf_link = driver.find_element(By.LINK_TEXT, \"PDF\")\n",
    "                        pdf_url = pdf_link.get_attribute(\"href\")\n",
    "                        act_url = pdf_url\n",
    "                        #act_txts.append(driver.find_element(By.CSS_SELECTOR,\".xtend > pre:nth-child(1)\").text)\n",
    "\n",
    "                    act_urls.append(act_url)\n",
    "\n",
    "                    name = str(year) + str(session) + \"chap\" + str(chapter_url.split(\"Chapter/\")[1].replace(\"/\",\"\"))+ \".pdf\"\n",
    "                    file_names.append(name)\n",
    "\n",
    "                    path = download_dir + name\n",
    "                    print(os.path.abspath(path))\n",
    "\n",
    "                    time.sleep(randint(1, 100) * 0.005)\n",
    "\n",
    "                    response = requests.get(act_url)\n",
    "                    # Ensure the request was successful\n",
    "                    if response.status_code == 200:\n",
    "                        # Write the content of the response to a file\n",
    "                        with open(path, 'wb') as file:\n",
    "                            file.write(response.content)\n",
    "                    else:\n",
    "                        print(\"Failed to fetch PDF. Status code:\", response.status_code)\n",
    "                        broken_act_urls.append(act_url)\n",
    "            except TimeoutException:\n",
    "                print(\"Failed to retrieve website content.\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "act_txts = []\n",
    "\n",
    "folder_path = \"./PDFs\"\n",
    "print(os.path.abspath(folder_path))\n",
    "\n",
    "\n",
    "files = glob.glob(os.path.join(folder_path, '*'))\n",
    "\n",
    "broken_files = []\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        doc = fitz.open(file)\n",
    "        pgtxts = []\n",
    "        for page in doc:\n",
    "            pgtxt = page.get_text()\n",
    "            pgtxts.append(pgtxt)\n",
    "            acttxt = ' '.join(pgtxts)\n",
    "        act_txts.append(acttxt)\n",
    "        doc.close()\n",
    "    except:\n",
    "        broken_files.append(file)\n",
    "\n",
    "file_names = []\n",
    "\n",
    "for file in files:\n",
    "    file_names.append(file.split(\"./PDFs/\")[1])\n",
    "\n",
    "\n",
    "datasource =  pd.DataFrame({\n",
    "    'original_text': act_txts,\n",
    "    'bill_num': bill_nums,\n",
    "    'chapter_num': chapter_nums,\n",
    "    'year':years,\n",
    "    'session':sessions,\n",
    "    'link': act_urls,\n",
    "    'state': \"MN\",\n",
    "    'filename': file_names\n",
    "})\n",
    "\n",
    "datasource = datasource.drop_duplicates(subset='link', keep='first')\n",
    "\n",
    "datasource.to_excel('MN_Leginfo.xlsx')\n",
    "datasource.to_csv('MN_Leginfo.csv')\n",
    "datasource.to_pickle('MN_Leginfo.pkl')\n",
    "datasource.to_json('MN_Leginfo.json')\n",
    "\n",
    "# End of MN_Leginfo.py\n",
    "\n",
    "\n",
    "# Start of GA_Leginfo.py\n",
    "\n",
    "# import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "import calendar\n",
    "import os\n",
    "import pytest\n",
    "import unittest\n",
    "from random import randint\n",
    "import PyPDF2\n",
    "import glob\n",
    "\n",
    "\n",
    "# driver_path = 'C:/Program Files (x86)/Google/Chrome/Application/chromedriver.exe' #for windows\n",
    "# driver_path = '/Users/Yanxu/OneDrive/Projects/AgPolicyCoding/chromedriver'\n",
    "# driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir('/Users/long/OneDrive/Projects/AFRI')\n",
    "# os.chdir('/Volumes/GoogleDrive/Shared drives/UCSC-UMN AFRI project/Data/GA')\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "dnldpath = {\"download.default_directory\" : \"/Users/long/OneDrive/Projects/AFRI/data/GA/\"}\n",
    "chromeOptions.add_experimental_option(\"prefs\",dnldpath)\n",
    "driver = webdriver.Chrome(executable_path='./chromedriver', options=chromeOptions)\n",
    "driver.get(\"https://www.legis.ga.gov/legislation/signed-by-governor\")\n",
    "yrs = []\n",
    "sts = []\n",
    "seyrs = []\n",
    "titles = []\n",
    "bfsums = []\n",
    "introdts = []\n",
    "sigdts = []\n",
    "effdts = []\n",
    "epddtes = []\n",
    "introducers = []\n",
    "txtlks = []\n",
    "acttxts = []\n",
    "stas = []\n",
    "hoscoms = []  # house committee\n",
    "sponsors = []\n",
    "\n",
    "sessions =[]\n",
    "urls = []\n",
    "billnums = []\n",
    "\n",
    "WebDriverWait(driver, 300).until(\n",
    "    EC.presence_of_element_located((By.CSS_SELECTOR, \"#session > option\"))\n",
    ")\n",
    "session_list = driver.find_elements_by_css_selector('#session > option')\n",
    "\n",
    "for index, i in enumerate(session_list):\n",
    "    WebDriverWait(driver, 300).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"#session > option\"))\n",
    "    )\n",
    "    session_list = driver.find_elements_by_css_selector('#session > option')\n",
    "    session_list[index].click()\n",
    "    session = session_list[index].text\n",
    "    print(session)\n",
    "\n",
    "    search_btn = driver.find_element_by_css_selector('button.btn.btn-primary')\n",
    "    search_btn.click()\n",
    "\n",
    "    sleep(2)\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(driver, 3).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'div.noResults'))\n",
    "        )\n",
    "        driver.find_element_by_css_selector('div.noResults')\n",
    "        print(\"No results found\")\n",
    "    except:\n",
    "        WebDriverWait(driver, 300).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.resultCount b:nth-child(2)\"))\n",
    "        )\n",
    "        total_bills = driver.find_element_by_css_selector('div.resultCount b:nth-child(2)').text\n",
    "\n",
    "        # total_bills = 40\n",
    "        if int(total_bills) % 20 == 0:\n",
    "            total_pages = int(int(total_bills)/20)\n",
    "        else:\n",
    "            total_pages = int(int(total_bills)/20)+1\n",
    "        print(total_pages)\n",
    "\n",
    "        # temporary setup for developing\n",
    "        #total_pages = 6\n",
    "\n",
    "        for page in range(total_pages):\n",
    "            WebDriverWait(driver, 300).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".text-nowrap\"))\n",
    "            )\n",
    "            bill_list = driver.find_elements_by_css_selector('.text-nowrap')\n",
    "            for bill in bill_list:\n",
    "                url = bill.get_attribute('href')\n",
    "                urls.append(url)\n",
    "                billnum = bill.text\n",
    "                billnums.append(billnum)\n",
    "                sessions.append(session)\n",
    "            print(\"page\",page+1)\n",
    "\n",
    "            if total_pages > 0:\n",
    "                WebDriverWait(driver, 300).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"ul > li:last-child > a > span\"))\n",
    "                )\n",
    "                next_page = driver.find_element_by_css_selector('ul > li:last-child > a > span')\n",
    "                try:\n",
    "                    next_page.click()\n",
    "                    sleep(1)\n",
    "                except:\n",
    "                    print(\"this session is done for urls\")\n",
    "            else:\n",
    "                print(\"this session is done for urls\")\n",
    "\n",
    "for idx,url in enumerate(urls):\n",
    "    driver.get(url)\n",
    "    sleep(1)\n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"div.card div.card-body\"))\n",
    "    )\n",
    "    try:\n",
    "        bfsum= driver.find_elements_by_css_selector('h2.card-title + div.card-text-sm')[0].text\n",
    "        #Briefsummary = driver.find_elements_by_css_selector('div.card div.card-body')[1].text\n",
    "    except:\n",
    "        bfsum = ''\n",
    "    bfsums.append(bfsum)\n",
    "\n",
    "    title = driver.find_element_by_css_selector(\"div.h1.subHead\").text\n",
    "    titles.append(title)\n",
    "\n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"app-status-history-list div  table tbody\"))\n",
    "    )\n",
    "    sleep(1)\n",
    "    ## Status History Dates\n",
    "    try:\n",
    "        dates = driver.find_element_by_css_selector('app-status-history-list div  table tbody').text\n",
    "\n",
    "        if \"Senate Date Signed by Governor\" in dates:\n",
    "            sigdt = dates.split(\"Senate Date Signed by Governor\")[0].split(\"\\n\")[-1].replace(\" \", \"\")\n",
    "        else:\n",
    "            sigdt = dates.split(\"House Date Signed by Governor\")[0].split(\"\\n\")[-1].replace(\" \", \"\")\n",
    "            sigdts.append(sigdt)\n",
    "\n",
    "        effdt = dates.split(\" Effective Date\")[0]\n",
    "        effdts.append(effdt)\n",
    "\n",
    "        introdt = dates.split(\"\\n\")[-1].split(\" \")[0]\n",
    "        introdts.append(introdt)\n",
    "\n",
    "        yr = effdt.split(\"/\")[2]\n",
    "        yrs.append(yr)\n",
    "\n",
    "        '''\n",
    "        # if (int(yr) % 2) == 0:\n",
    "            seyr = str(int(yr) - 1) + \"-\" + str(yr)\n",
    "        else:\n",
    "            seyr = str(yr) + str(\"-\") + str(int(yr) + 1)\n",
    "        '''\n",
    "    except:\n",
    "        effdt =''\n",
    "        effdts.append(effdt)\n",
    "        sigdt =''\n",
    "        sigdts.append(sigdt)\n",
    "        introdt =''\n",
    "        introdts.append(introdt)\n",
    "        yr =''\n",
    "        yrs.append(yr)\n",
    "\n",
    "        '''\n",
    "        seyr =''\n",
    "        seyrs.append(seyr)\n",
    "        '''\n",
    "\n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"li.list-inline-item\"))\n",
    "    )\n",
    "    version_btn =driver.find_elements_by_css_selector('li.list-inline-item')\n",
    "    version_btn[0].click()\n",
    "\n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"div.col-6.justify-content-end.text-right > button\"))\n",
    "    )\n",
    "    dnldbtn = driver.find_element_by_class_name('btn.btn-outline-primary.btn-sm.mr-2.mb-1')\n",
    "    dnldbtn.click()\n",
    "\n",
    "    sleep(randint(1, 2))\n",
    "    path = \"/Users/long/OneDrive/Projects/AFRI/data/GA/\"\n",
    "    list_of_files = glob.glob(os.path.join(path, '*.pdf'))\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    print(latest_file)\n",
    "    with open(os.path.join(os.getcwd(), latest_file), 'r') as f:  # open in readonly mode\n",
    "        # creating a pdf File object of original pdf\n",
    "        pdfFileObj = open(latest_file, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        pagenumber =  pdfReader.numPages\n",
    "        acttxt =[]\n",
    "        pgtxts = []\n",
    "        for p in range(pagenumber):\n",
    "            pageObj = pdfReader.getPage(p)\n",
    "            pgtxt = pageObj.extractText()\n",
    "            pgtxts.append(pgtxt)\n",
    "            acttxt = '\\n'.join(pgtxts)\n",
    "    acttxts.append(acttxt)\n",
    "    st =\"GA\"\n",
    "    sts.append(st)\n",
    "\n",
    "\n",
    "    print(idx+1)\n",
    "    # sleep(randint(1, 2))\n",
    "\n",
    "print(\"Downloading is finished\")\n",
    "\n",
    "\n",
    "#'sponsors': sponsors\n",
    "#'HouseCommittees': HouseCommittees,\n",
    "datasource = pd.DataFrame({\n",
    "    'Session Year': sessions,\n",
    "    'Bill Number': billnums,\n",
    "    'Link to full text': urls\n",
    "})\n",
    "\n",
    "\n",
    "datasource = pd.DataFrame({\n",
    "    'Year': yrs,\n",
    "    # 'State': sts,\n",
    "    'Session Year': sessions,\n",
    "    'Bill Number': billnums,\n",
    "    'Title': titles,\n",
    "    'Brief Summary': bfsums,\n",
    "    # 'Introduced Date': introdts,\n",
    "    #'Date it was signed':sigdts,\n",
    "    # 'Date effective':effdts,\n",
    "    # 'Full Text':acttxts,\n",
    "    'Link to full text': urls\n",
    "})\n",
    "\n",
    "#'House Committee': hoscoms,\n",
    "# drop duplicates\n",
    "#datasource.drop_duplicates(subset = ['Bill Number'],\n",
    "# keep = 'first', inplace = True, ignore_index= True)\n",
    "\n",
    "# save bill info into excel file\n",
    "datasource.to_excel('GA_Leginfo.xlsx')\n",
    "print(\"Web-scrapting finished\")\n",
    "\n",
    "# End of GA_Leginfo.py\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "72db359963b886546324fdab9aa5857888ab40de550209afc182f1efe35e5205"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
